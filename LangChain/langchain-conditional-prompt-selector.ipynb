{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df24099e-ed26-4b12-83fe-06e83adaafe1",
   "metadata": {},
   "source": [
    "# Conditional Prompt Selector\n",
    "\n",
    "LLM will not be invoked. They are referenced to demonstrate the working of the conditional prompting.\n",
    "\n",
    "https://api.python.langchain.com/en/stable/chains/langchain.chains.prompt_selector.ConditionalPromptSelector.html#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7f2c0-a69a-4930-8913-f5bb3b4db064",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2124e-a351-43fb-a3c1-7e0a0ea04863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff0897-ed57-4aca-a689-170bcbda3639",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db24268-0959-4b8e-b4ec-92324db6c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c0141-0b93-4a39-9883-16fb6a77c3c3",
   "metadata": {},
   "source": [
    "## 1. Define the default template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c2e157-cfa1-49ee-be89-37beb14cd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = \"\"\"summarize the text below.\n",
    "\n",
    "Text:\n",
    "{text}\"\"\"\n",
    "\n",
    "default_prompt_template = PromptTemplate(template=default_prompt, input_variables=[\"text\"])\n",
    "\n",
    "# default_prompt.format(text=\"DUMMY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d69a9ed5-2623-401c-9277-0c6176d19f3b",
   "metadata": {},
   "source": [
    "## 2. AI21 prompt template &LLM\n",
    "\n",
    "**ASSUME** that following prompt produced a better result compared to the default prompt.\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec108cc2-166e-4a9a-a776-853d10b4348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai21_prompt = \"\"\"\n",
    "<<SYS>>\n",
    "You are an assistant tasked with summarizing technical research papers.\n",
    "<</SYS>> \n",
    "\n",
    "[INST] \n",
    "Generate summary: \n",
    "\n",
    "{text} \n",
    "[/INST]\"\"\"\n",
    "\n",
    "ai21_prompt_template = PromptTemplate(template=ai21_prompt, input_variables=[\"text\"])\n",
    "\n",
    "# ai21_prompt_template.format(text=\"DUMMY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d051637c-93a6-4b93-b564-f9b1320f84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ai21 import AI21\n",
    "\n",
    "ai21_llm = AI21(ai21_api_key=\"DUMMY-KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d43595-1aa0-4456-aa16-44d192abfa17",
   "metadata": {},
   "source": [
    "## 3. Cohere Command prompt template & LLM\n",
    "\n",
    "**ASSUME** that following prompt produced a better result compared to the default prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa2c784-9837-4854-852f-32ccd7a29377",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_prompt = \"\"\"\n",
    "You are an expert technical writer. Your task is to summarize the given research paper's content.\n",
    "\n",
    "Content:\n",
    "{text}\"\"\"\n",
    "\n",
    "cohere_prompt_template = PromptTemplate(template=cohere_prompt, input_variables=[\"text\"])\n",
    "\n",
    "# cohere_prompt_template.format(text=\"DUMMY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31f9c4c8-0a8c-475c-bf83-d3f5e157b60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import Cohere`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.cohere import Cohere\n",
    "\n",
    "cohere_llm = Cohere(cohere_api_key=\"DUMMY-KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e1828-b7b0-46af-b040-ad0bdbb345c6",
   "metadata": {},
   "source": [
    "## 4. Create the conditional prompt\n",
    "\n",
    "Provide instance of LLM and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c1a397-cfc0-465e-b6f4-a39cb48988b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conditional_prompt_selector = ConditionalPromptSelector(\n",
    "    default_prompt=  default_prompt_template,\n",
    "    conditionals=[\n",
    "        (lambda llm: isinstance(llm, type(ai21_llm)), ai21_prompt_template),\n",
    "        (lambda llm: isinstance(llm, type(cohere_llm)), cohere_prompt_template)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26f55d-26b8-4805-b55f-8f0a25b0b306",
   "metadata": {},
   "source": [
    "## 5. Test the prompt generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46ba4b3-ebac-4098-82a9-2a523f6e0945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<<SYS>>\n",
      "You are an assistant tasked with summarizing technical research papers.\n",
      "<</SYS>> \n",
      "\n",
      "[INST] \n",
      "Generate summary: \n",
      "\n",
      "DUMMY \n",
      "[/INST]\n"
     ]
    }
   ],
   "source": [
    "# Using AI21 model\n",
    "llm = ai21_llm\n",
    "prompt = conditional_prompt_selector.get_prompt(llm)\n",
    "\n",
    "print(prompt.format(text=\"DUMMY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4facd557-effd-4310-bd65-fa6e18bf7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert technical writer. Your task is to summarize the given research paper's content.\n",
      "\n",
      "Content:\n",
      "DUMMY\n"
     ]
    }
   ],
   "source": [
    "# Using AI21 model\n",
    "llm = cohere_llm\n",
    "prompt = conditional_prompt_selector.get_prompt(llm)\n",
    "\n",
    "print(prompt.format(text=\"DUMMY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a853b-312f-4b28-a665-418cbd6515c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now you can use the llm & prompt to carry out the infrence\n",
    "# llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563af410-a920-47ac-9198-3c00475b8508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
