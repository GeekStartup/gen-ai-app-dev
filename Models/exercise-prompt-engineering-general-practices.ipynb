{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03941d",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "## General Best Practices\n",
    "\n",
    "1. Separation between parts\n",
    "2. Detailed, clear & specific instructions\n",
    "3. Avoid ambiguity \n",
    "4. Favor positive instructions\n",
    "5. Instruct the model to play a role\n",
    "6. Provide trusted information\n",
    "7. Specify response characteristics\n",
    "8. Set guardrails for response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df2c946",
   "metadata": {},
   "source": [
    "## Change the location of the environment file before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a210debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d152a6",
   "metadata": {},
   "source": [
    "## Setup the models available for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f3cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'google/flan-t5-xxl'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8f096",
   "metadata": {},
   "source": [
    "## 1. Guide the model to avoid hallucinations\n",
    "\n",
    "Add the guidance to not to make up a response.\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[2]    \n",
    "\n",
    "'openlm-research/open_llama_3b_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be60250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n- 2021-01-14,\\nThe Prime Minister of'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_bad  = \"\"\"as of january 2024, who is the prime minister of UK\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "llm = InferenceClient(model=hugging_face_model_ids[2])\n",
    "\n",
    "llm.text_generation(prompt_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d685a1c9",
   "metadata": {},
   "source": [
    "## 2. Instruct the model to play roles\n",
    "\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[3],\n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de27503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large Language Models (LLMs) are a type of machine translation system that can handle large amounts of text.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"question:what are large language models. answer:\"\n",
    "\n",
    "llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "\n",
    "llm.text_generation(prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebf2f0",
   "metadata": {},
   "source": [
    "## 3. Guide the model to generate precise response\n",
    "\n",
    "### Default model\n",
    "hugging_face_model_ids[3], \n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without context:  Â£200\n"
     ]
    }
   ],
   "source": [
    "prompt_samples = [\n",
    "    \"how much would be the cost of 'Digital Camera DSLR'.\",\n",
    "    \"share details of home security camera\",\n",
    "    \"do you sell ceiling fan?\",\n",
    "    \"share details of Robot Cleaner?\",\n",
    "]\n",
    "\n",
    "llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "\n",
    "no_context_response = llm.text_generation(prompt_samples[0])\n",
    "\n",
    "print(\"Response without context: \", no_context_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b5e8e",
   "metadata": {},
   "source": [
    "## 4. Address bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b186e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without guardrails:  \n",
      "\n",
      "No, I do not agree. Hiring decisions should be based on qualifications, skills\n"
     ]
    }
   ],
   "source": [
    "prompt = \"for the engineering jobs we should only hire men 100% of the time, do you agree? \"\n",
    "\n",
    "\n",
    "# Change the index to try out different models\n",
    "llm = InferenceClient(model=hugging_face_model_ids[1])  \n",
    "\n",
    "without_guardrails = llm.text_generation(prompt)\n",
    "\n",
    "\n",
    "print(\"Response without guardrails: \", without_guardrails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f84035d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
