{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad73185c-e5d0-4b77-9602-8b450484e88c",
   "metadata": {},
   "source": [
    "# GPU Memory Estimator\n",
    "This notebook shows how to estimate GPU memory needed for training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c836371-7276-406f-a852-a72de9c1d61b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Training\n",
    "Below is the Python code that calculates the approximate GPU memory required for training a large language model (LLM), based on the number of parameters and the training precision. The code uses the assumption that memory usage for model weights, gradients, optimizer states, and activations scales with the number of parameters and training precision.\r\n",
    "\n",
    "    except ValueErro1. s e:\r\n",
    "        print(e)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **How It Works**\r\n",
    "1. *ory Breakdown**:\r\n",
    "   - Each parameter consumes memoor:\r\n",
    "     - **Weights**: The model's parameteremselves.\r\n",
    "     - **Gradients**: Needed for backpropagation.\r\n",
    "    *Optimizer States**: Variables like momentum or Adam's state.\r\n",
    "     - **Activations**: Intermediate computations during the forward pass.\r\n",
    "\r\n",
    "   For simplicity, the model assumes a memory factor of 2x for weights + gradients, 2x for optimizer states, and 1x for activations.\r\n",
    "\r\n",
    "2. **Precision Impact**:\r\n",
    "   The memory consumption per parameter depends on the precision:\r\n",
    "   - `fp32`: 4 bytes per parameter.\r\n",
    "   - `fp16`: 2 bytes per parameter.\r\n",
    "   - `int8`: 1 byte per parameter.\r\n",
    "   - `int4`: 0.5 bytes per parameter.\r\n",
    "\r\n",
    "3. **Conversion to GB**:\r\n",
    "   The total memory is calculated in bytes and then converted to gigabytes (GB).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Example Inputs and Outputs**\r\n",
    "#### Input 1:\r\n",
    "```plaintext\r\n",
    "Enter the number of model parameters (e.g., 1B = 1_000_000_000): 1000000000\r\n",
    "Enter the training precision (fp32, fp16, int8, int4): fp32\r\n",
    "```\r\n",
    "#### Output 1:\r\n",
    "```plaintext\r\n",
    "Approximate GPU memory required: 56.00 GB\r\n",
    "```\r\n",
    "\r\n",
    "#### Input 2:\r\n",
    "```plaintext\r\n",
    "Enter the number of model parameters (e.g., 1B = 1_000_000_000): 1000000000\r\n",
    "Enter the training precision (fp32, fp16, int8, int4): fp16\r\n",
    "```\r\n",
    "#### Output 2:\r\n",
    "```plaintext\r\n",
    "Approximate GPU memory required: 28.00 GB\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Customizations**\r\n",
    "- You can tweak the memory multiplier factors if the model has specific memory optimization techniques like activation checkpointing or gradient accumulation.\r\n",
    "- Extend the code for distributed training to calculate per-GPU memory based on the number of GPUs.\r\n",
    "\r\n",
    "Let me know if you need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c00ef77-6c24-4cc0-bc49-18cfa254f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gpu_memory_for_training(number_of_parameters, training_precision):\n",
    "    \"\"\"\n",
    "    Calculate the approximate GPU memory needed for training a model.\n",
    "\n",
    "    Parameters:\n",
    "        number_of_parameters (int): Total number of model parameters.\n",
    "        training_precision (str): Precision type for training. Options: 'fp32', 'fp16', 'int8', 'int4'.\n",
    "\n",
    "    Returns:\n",
    "        float: Approximate GPU memory required (in GB).\n",
    "    \"\"\"\n",
    "    # Memory per parameter in bytes for different precisions\n",
    "    precision_memory_map = {\n",
    "        'fp32': 4,  # 32 bits = 4 bytes\n",
    "        'fp16': 2,  # 16 bits = 2 bytes\n",
    "        'int8': 1,  # 8 bits = 1 byte\n",
    "        'int4': 0.5  # 4 bits = 0.5 bytes\n",
    "    }\n",
    "\n",
    "    # Check if valid precision is provided\n",
    "    if training_precision not in precision_memory_map:\n",
    "        raise ValueError(f\"Invalid training precision: {training_precision}. \"\n",
    "                         f\"Choose from: {list(precision_memory_map.keys())}\")\n",
    "    \n",
    "    # Memory multiplier factors\n",
    "    weights_and_gradients_factor = 2  # Weights + Gradients\n",
    "    optimizer_states_factor = 2      # Optimizer states\n",
    "    activation_memory_factor = 1     # Activations\n",
    "\n",
    "    # Total memory per parameter in bytes\n",
    "    bytes_per_parameter = precision_memory_map[training_precision]\n",
    "    total_memory_bytes = (\n",
    "        number_of_parameters\n",
    "        * bytes_per_parameter\n",
    "        * (weights_and_gradients_factor + optimizer_states_factor + activation_memory_factor)\n",
    "    )\n",
    "\n",
    "    # Convert bytes to GB\n",
    "    total_memory_gb = total_memory_bytes / (1024 ** 3)  # 1 GB = 1024^3 bytes\n",
    "    return total_memory_gb\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Input: Number of parameters and precision\n",
    "#     number_of_parameters = int(input(\"Enter the number of model parameters (e.g., 1B = 1_000_000_000): \"))\n",
    "#     training_precision = input(\"Enter the training precision (fp32, fp16, int8, int4): \").strip().lower()\n",
    "\n",
    "#     try:\n",
    "#         memory_needed = calculate_gpu_memory(number_of_parameters, training_precision)\n",
    "#         print(f\"Approximate GPU memory required: {memory_needed:.2f} GB\")\n",
    "#     except ValueError as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51024e20-6a89-4fd3-9487-7465a53d6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of model parameters (e.g., 1B = 1_000_000_000):  135_000_000\n",
      "Enter the training precision (fp32, fp16, int8, int4):  fp32\n"
     ]
    }
   ],
   "source": [
    "number_of_parameters = int(input(\"Enter the number of model parameters (e.g., 1B = 1_000_000_000): \"))\n",
    "training_precision = input(\"Enter the training precision (fp32, fp16, int8, int4): \").strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6b9c03-25d3-450b-9e04-2f342a6373a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate GPU memory required: 2.51 GB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    memory_needed = calculate_gpu_memory_for_training(number_of_parameters, training_precision)\n",
    "    print(f\"Approximate GPU memory required: {memory_needed:.2f} GB\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89435906-0918-4146-99a3-4f4332b6d59a",
   "metadata": {},
   "source": [
    "## Calulate memory needs for inferencing\n",
    "\n",
    "**How It Works**\n",
    "\n",
    "1. **Memory Breakdown for Inference:**\n",
    "\n",
    "* Weights: Memory for storing model weights. This is static and doesn't depend on concurrent calls.\n",
    "* Activations: Memory for storing activations during the forward pass. This depends on the number of concurrent calls because each concurrent call * generates its own set of activations.\n",
    "\n",
    "2. **Precision Impact: The precision affects memory consumption:**\n",
    "\n",
    "* fp32: 4 bytes per parameter.\n",
    "* fp16: 2 bytes per parameter.\n",
    "* int8: 1 byte per parameter.\n",
    "* int4: 0.5 bytes per parameter.\n",
    "\n",
    "3. **Concurrent Calls:**\n",
    "Each concurrent call requires memory for activations. The total memory for activations is calculated as:\n",
    "\n",
    "Activation Memory = Number of Parameters × Bytes per Parameter × Number of Concurrent Calls\n",
    "\n",
    "\n",
    "4. **Conversion to GB:**\n",
    "The total memory (weights + activations) is calculated in bytes and converted to GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17433bec-da9c-4a22-8d05-b79970047849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inference_memory(number_of_parameters, training_precision, number_concurrent_calls):\n",
    "    \"\"\"\n",
    "    Calculate the approximate GPU memory needed for inferencing.\n",
    "\n",
    "    Parameters:\n",
    "        number_of_parameters (int): Total number of model parameters.\n",
    "        training_precision (str): Precision type for inference. Options: 'fp32', 'fp16', 'int8', 'int4'.\n",
    "        number_concurrent_calls (int): Number of concurrent inference calls.\n",
    "\n",
    "    Returns:\n",
    "        float: Approximate GPU memory required (in GB).\n",
    "    \"\"\"\n",
    "    # Memory per parameter in bytes for different precisions\n",
    "    precision_memory_map = {\n",
    "        'fp32': 4,  # 32 bits = 4 bytes\n",
    "        'fp16': 2,  # 16 bits = 2 bytes\n",
    "        'int8': 1,  # 8 bits = 1 byte\n",
    "        'int4': 0.5  # 4 bits = 0.5 bytes\n",
    "    }\n",
    "\n",
    "    if training_precision not in precision_memory_map:\n",
    "        raise ValueError(f\"Invalid training precision: {training_precision}. \"\n",
    "                         f\"Choose from: {list(precision_memory_map.keys())}\")\n",
    "\n",
    "    # Memory multiplier factors\n",
    "    weights_memory_factor = 1  # Weights (static for inference)\n",
    "    activation_memory_factor = 1  # Per-call activations (dynamic)\n",
    "\n",
    "    # Total memory for weights in bytes\n",
    "    bytes_per_parameter = precision_memory_map[training_precision]\n",
    "    weights_memory_bytes = number_of_parameters * bytes_per_parameter * weights_memory_factor\n",
    "\n",
    "    # Total memory for activations per concurrent call\n",
    "    activation_memory_bytes_per_call = number_of_parameters * bytes_per_parameter * activation_memory_factor\n",
    "    total_activation_memory_bytes = activation_memory_bytes_per_call * number_concurrent_calls\n",
    "\n",
    "    # Total memory usage (weights + activations)\n",
    "    total_memory_bytes = weights_memory_bytes + total_activation_memory_bytes\n",
    "\n",
    "    # Convert bytes to GB\n",
    "    total_memory_gb = total_memory_bytes / (1024 ** 3)  # 1 GB = 1024^3 bytes\n",
    "    return total_memory_gb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae3ce8b-d788-493b-bd39-ae3745fb21f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of model parameters (e.g., 1B = 1_000_000_000):  1_000_000_000\n",
      "Enter the inference precision (fp32, fp16, int8, int4):  fp16\n",
      "Enter the number of concurrent inference calls:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate GPU memory required for inference: 5.59 GB\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Input: Number of parameters, precision, and concurrent calls\n",
    "    number_of_parameters = int(input(\"Enter the number of model parameters (e.g., 1B = 1_000_000_000): \"))\n",
    "    training_precision = input(\"Enter the inference precision (fp32, fp16, int8, int4): \").strip().lower()\n",
    "    number_concurrent_calls = int(input(\"Enter the number of concurrent inference calls: \"))\n",
    "\n",
    "    try:\n",
    "        memory_needed = calculate_inference_memory(number_of_parameters, training_precision, number_concurrent_calls)\n",
    "        print(f\"Approximate GPU memory required for inference: {memory_needed:.2f} GB\")\n",
    "    except ValueError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ef1a1-1bce-456f-83ce-5afd6a069115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
