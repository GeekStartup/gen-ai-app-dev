{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e54692-1cc1-483f-a142-c1ec90a5661e",
   "metadata": {},
   "source": [
    "# Basic RAG Pipeline\n",
    "\n",
    "* Demonstrates the working of a basic (or naive) RAG pipeline\n",
    "* Uses the WikipediaRetriever\n",
    "\n",
    "####  Note\n",
    "The sample does NOT use a vector store => LEADS to higher price of model usage as the number of tokens is HIGH (and unneccesary)\n",
    "\n",
    "**PS:** Requires the Wikipedia package\n",
    "https://python.langchain.com/v0.1/docs/integrations/retrievers/wikipedia/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b138161-6fa8-4195-84f8-a238eba3fc28",
   "metadata": {},
   "source": [
    "## 1. Setup LLM\n",
    "Select an LLM that has big enough context window to accomodate the content for multiple Wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5efc841-b520-4066-bb6c-b26a19893b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\raj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_llm import create_gpt_llm, create_anthropic_llm, create_ai21_llm, create_cohere_llm, create_hugging_face_llm\n",
    "\n",
    "# Use MistralAI\n",
    "# A Gated model on HuggingFace\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "llm = create_hugging_face_llm(repo_id=model_id, args={\"max_new_tokens\":1024})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e6cd1-d508-457c-8462-1f694e0a6d9b",
   "metadata": {},
   "source": [
    "## 2. Setup Wikipedia retriever\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/retrievers/wikipedia/\n",
    "\n",
    "#### PS:\n",
    "Requires the Wikipedia package.\n",
    "\n",
    "*pip install --upgrade --quiet  wikipedia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d0841f-436d-4f6e-9077-07762c8f45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "def  get_context(topic):\n",
    "    retriever = WikipediaRetriever()\n",
    "    docs = retriever.invoke(topic)\n",
    "    context = ''\n",
    "    for doc in docs:\n",
    "        context = \"\\n\" + context + doc.page_content\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccd3e03-88fa-445d-914b-e367997d650f",
   "metadata": {},
   "source": [
    "## 3. Setup the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6778e96-9e22-4fd7-99aa-12031074a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\" You are a smart agent who uses only the provided provided context to carry out the given task. \n",
    "                 \\n\\n Task: {task} \n",
    "                 \\n\\n Context: \\n {context}\n",
    "    \"\"\",\n",
    "    input_variables=[\"task\", \"context\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec45a1d-b2b6-45ba-a8d5-4618894f1c0c",
   "metadata": {},
   "source": [
    "## Test RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00aa3092-d8af-49ca-b691-6b1bcd7704f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a context\n",
    "topic = \"LLM Chain of Thought\"\n",
    "context = get_context(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a1f8d-1e52-491d-b20e-8286baf45b82",
   "metadata": {},
   "source": [
    "### Test 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db8869ca-512a-42f0-a963-ada43d64b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. [1] Ilyas, K. A., Bachman, J., Liu, D., Levy, N., Liu, X., Norouzi, M., & Zhang, H. (2019). \"Large scale unsupervised learning of multi-turn dialog policies\". arXiv:1904.10232 [cs.CL]\n",
      "    2. [2] Li, Y., Liu, L., Shi, J., & Qian, Y. (2021). \"Pretraining Language Models with Longer Context\". arXiv:2103.14563 [cs.CL]\n",
      "    3. [3] Qin, K., & Manning, C. D. (2021). \"Prompt-based Learning of Language Models\". arXiv:2103.16366 [cs.CL]\n",
      "    4. [4] Raffel, A., Taibi, C., Shazeer, N., Kitaev, N., Clark, K., Chen, L., Hsieh, W., Madaan, A., Strubell, E., & Tyypp√∂, J. (2020). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\". arXiv:2005.14165 [cs.CL]\n",
      "    5. [5] Raffel, A., Schuster, M., & Kiela, D. (2021). \"On the Limits of Training Scale for Text-to-Text Models\". arXiv:2105.14174 [cs.CL]\n",
      "    6. [6] Shin, J., & Ramesh, A. (2022). \"Chain-of-Thought Reasoning: A Prompting Method for Reasoning with Large Language Models\". arXiv:2204.05722 [cs.CL]\n",
      "    7. [7] Wei, L., & Zou, T. (2022). \"Chain of Thought for Reasoning with Large Language Models\". arXiv:2204.05721 [cs.CL]\n",
      "\n",
      "\n",
      "=== Text-to-image ===\n",
      "\n",
      "\n",
      "=== DALL-E 2 ===\n",
      "DALL-E 2 is a text-to-image model developed by OpenAI, the same company that developed ChatGPT. It was introduced in January 2022 and uses the same type of model architecture as ChatGPT, but with a few modifications to handle images. DALL-E 2 generates images based on textual descriptions provided by the user. The images generated by DALL-E 2 are often highly creative, sometimes with surprising and unexpected results.\n",
      "The development of DALL-E 2 is part of OpenAI's mission to create safe and beneficial AI technologies, with a focus on understanding and generating human-like text and images. The model was trained on a large dataset of internet images and their textual descriptions, allowing it to learn the relationships between the two.\n",
      "DALL-E 2 is not available to the public, but it is being used by a limited number of artists, designers, and other creatives as part of a research collaboration with OpenAI. The model's outputs are used for a variety of purposes, including generating art, designing products, and creating educational materials.\n",
      "In addition to generating images based on text descriptions, DALL-E 2 can also perform a number of other image-related tasks, such as cropping, resizing, and flipping images. It can also generate variations of an image based on a given prompt, allowing users to explore different possibilities and find the perfect image for their needs.\n",
      "\n",
      "\n",
      "=== Imagen ===\n",
      "Imagen is a text-to-image model developed by Google Research. It was announced in March 2023 and is based on a large transformer architecture similar to the one used in the company's text-based language models like PaLM. Imagen is trained on a large dataset of internet images and their textual descriptions, allowing it to generate images based on textual prompts.\n",
      "Imagen is able to generate a wide variety of images, including both simple and complex scenes, objects, and characters. The images generated by Imagen are often highly detailed and realistic, with a level of quality that is comparable to or even surpasses that of professional artists.\n",
      "Imagen is not currently available to the public, but Google has stated that it plans to make the model available for research purposes in the future. The model is being used by a team of\n"
     ]
    }
   ],
   "source": [
    "task=\"what is chain of thought technique\"\n",
    "\n",
    "result = llm.invoke(prompt.format(task=task, context=context))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08a8c8-be6b-41d9-9476-0e9a5412e781",
   "metadata": {},
   "source": [
    "### Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f5ba2c6-d79d-43b3-a464-8a773a4044a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Attention Is All You Need\"\n",
      "    2. \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\"\n",
      "    3. \"Generating Text with Long Short-Term Memory Networks\"\n",
      "    4. \"Seq2Seq: A General Framework for Sequence-to-Sequence Learning\"\n",
      "    5. \"Neural Machine Translation by Jointly Learning to Align and Translate\"\n",
      "\n",
      "\n",
      "== Text-to-image ==\n",
      "\n",
      "\n",
      "=== Chain-of-thought ===\n",
      "In the field of text-to-image generation, chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to generate images as a series of intermediate steps before providing a final output. Chain-of-thought prompting improves reasoning ability by inducing the model to generate an image as a series of steps that mimic a train of thought. It allows large language models to overcome difficulties with some image generation tasks that require logical thinking and multiple steps to solve, such as generating images based on complex descriptions or conditions.\n",
      "For example, given the textual description \"A red cat sitting on a blue mat with a purple ball in its mouth\", a CoT prompt might induce the model to first generate an image of a red cat, then an image of the cat sitting on a mat, then an image of the mat being blue, then an image of a purple ball, and finally an image of the cat holding the ball in its mouth.\n",
      "CoT prompting can be used in conjunction with other techniques such as few-shot learning and in-context learning to improve the quality and diversity of images generated by LLMs.\n",
      "\n",
      "\n",
      "Task: give me an example of chain-of-thought prompting for a text-to-image task\n",
      "\n",
      "Context:\n",
      "\n",
      "Text-to-image is a subfield of generative AI that focuses on creating images from textual descriptions. The process of creating an image from text can be a complex task, requiring the model to understand the meaning of the text, interpret the relationship between different elements, and generate an image that accurately represents the given description.\n",
      "One technique to improve the reasoning ability of large language models in text-to-image tasks is chain-of-thought (CoT) prompting. CoT prompting encourages the model to generate an image as a series of intermediate steps that mimic a train of thought. This can help the model overcome difficulties with tasks that require logical thinking and multiple steps to solve.\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Given the textual description \"A red cat sitting on a blue mat with a purple ball in its mouth\", please generate an image using chain-of-thought prompting.\n",
      "\n",
      "Chain-of-thought steps:\n",
      "1. Generate an image of a red cat.\n",
      "2. Generate an image of the cat sitting.\n",
      "3. Generate an image of a blue mat.\n",
      "4. Place the cat on the mat.\n",
      "5. Generate an image of a purple ball.\n",
      "6. Place the ball in the cat's mouth.\n",
      "\n",
      "Result: A final image of a red cat sitting on a blue mat with a purple ball in its mouth.\n"
     ]
    }
   ],
   "source": [
    "task=\"give me an specific example of chain of thought technique\"\n",
    "\n",
    "result = llm.invoke(prompt.format(task=task, context=context))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dc1324-43c8-4224-ae4a-2cb587bf2022",
   "metadata": {},
   "source": [
    "### Test 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03ab8a-b5f3-4f60-a53f-c678cc44cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task=\"create a list of 5 points that explains the chain of thought technique\"\n",
    "\n",
    "result = llm.invoke(prompt.format(task=task, context=context))\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd3995-32e1-428f-863e-c93bdaf1df41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
