{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bff477c",
   "metadata": {},
   "source": [
    "# LLM Challenges\n",
    "\n",
    "* Multiple models in use for demonstrating the behavior of the LLM\n",
    "* You can use either the InferenceClient or the HTTP API invocation.\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation\n",
    "\n",
    "**Note**\n",
    "* YOUR RESULTS MAY BE DIFFERENT THAN THE RESULTS IN VIDEO\n",
    "* If you get a '404 not found', try a different model for the call \n",
    "* A return value of 503 indicates that the model is in cold state and is loading\n",
    "* Wait a few moments and try again\n",
    "* In case of 500, model is in freezed state or may not be available for some time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4515c-7734-474b-bb7f-4332e4ab8313",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the HF API Token\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed22c1f-31ce-4c47-84c2-e4de146f3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The script is downloaded and run to setup the utils folder\n",
    "\n",
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a5e71",
   "metadata": {},
   "source": [
    "## Setup the enviornment varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b45c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c1f7a6-f88d-4b93-a631-f50f9a5f936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694e045",
   "metadata": {},
   "source": [
    "## Create LLM for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abfdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "# SOME OF THESE MODELS ARE NOW REMOVED FROM HUGGINGFACE INFERENCE - \n",
    "# August 10th, 2025\n",
    "# hugging_face_model_ids = [\n",
    "#     'google/gemma-2-2b-it',\n",
    "#     'tiiuae/falcon-7b-instruct',\n",
    "#     'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "#     'openlm-research/open_llama_3b_v2',\n",
    "#     'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "# ]\n",
    "\n",
    "# Feel free to add other models by checking out the availability from following link\n",
    "# https://router.huggingface.co/v1/models\n",
    "hugging_face_model_ids = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    \"deepseek-ai/DeepSeek-V3-0324\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3e7f2",
   "metadata": {},
   "source": [
    "## 1. Hallucination\n",
    "\n",
    "Some models are better than others. Try out a couple of models to figure out the ones that hallucinate more than other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a6b51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the context of biology, LLM stands for Linear Mitochondrial Linker or more specifically, the Linear Mitochondrial Linker protein.\\n\\nHowever, a more relevant definition in the context of biology would be the Linear Mitochondrial Linker is a protein but a more applicable answer is that LLM refers to the Long Linear Mitochondrial DNA in a non-human context. \\n\\nThe more widely applicable definition of LLM, especially in the context of biology, is the Large Linear Mitochondrial'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"define LLM in the context of biology\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(hugging_face_model_ids[0])\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04015520",
   "metadata": {},
   "source": [
    "## 2. Dated knowledge\n",
    "\n",
    "**Note:**\n",
    "You will also observe hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ce47a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As of my cut-off knowledge in December 2022, Rishi Sunak was the Prime Minister of the United Kingdom. However, my knowledge may not be up-to-date, and I do not have real-time information.\\n\\nTo confirm the current Prime Minister of the UK, I recommend checking a reliable news source or the official UK government website.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out the models & your own prompts\n",
    "# text = \"who won the 2022 super bowl?\"  # LA Rams vs Cincinnati Bengals  https://en.wikipedia.org/wiki/Super_Bowl_LVI\n",
    "text = \"as of today, who is the prime minister of UK\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[0])\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080bf8b",
   "metadata": {},
   "source": [
    "## 3. Missing context\n",
    "\n",
    "Model is not aware of your enterprise's business domain or model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b97a64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have a physical product to return, but I can provide information on how to get a refund or cancel a subscription if you're not satisfied with the service. Here are the general guidelines:\\n\\n1. **Free Trial or Subscription**: If you're not satisfied with the service, you can cancel your subscription or trial at any time. Please note that cancellation policies may vary depending on the platform or service you're using.\\n2. **Incorrect or Unhelpful Responses**: If you feel that my\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out the models and your own prompts\n",
    "text = \"what is your return policy?\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  # 1 = mistralai/Mistral-7B-Instruct-v0.2\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d7fdd",
   "metadata": {},
   "source": [
    "## 4. Bias\n",
    "\n",
    "Some models are trained to prevent biases. Try out the model **index= 1 'tiiuae/falcon-7b-instruct'** to see its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d95e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s break down why the statement \"for engineering jobs we should only hire men\" is not only inaccurate but also harmful and illegal.  I will address this point by point, covering the factual, ethical, and legal reasons.  I will also explain the benefits of a diverse engineering workforce.  This will be a comprehensive response.\\n\\n**Why the Statement is Wrong & Harmful**\\n\\nThe idea that engineering jobs should only be filled by men is based on outdated and demonstrably'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"for the engineering jobs we should only hire men because \"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  # 2 = openlm-research/open_llama_3b_v2\n",
    "# llm.text_generation(text, max_new_tokens=75)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[2])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbe0f5-61c7-4f85-951c-283d0085bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
