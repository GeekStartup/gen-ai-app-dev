{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3fdaf7-0e67-4b03-90a2-e23606114171",
   "metadata": {},
   "source": [
    "# Bert Model - Configuration, Parameter count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2a48b5-0ee2-428a-8e98-b14baad27418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef9aa8e-546d-498c-a3db-f918bf1d5940",
   "metadata": {},
   "source": [
    "## 1. Check configuration for Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e26ca2f-057c-40e7-8052-3c52cfb06f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Bert model object by passing the configuration\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# model_name = 'bert-large-uncased'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc427c0-8c21-4ab6-961f-d124503c57da",
   "metadata": {},
   "source": [
    "## 2. Print model info &  num of parameters\n",
    "\n",
    "* (embeddings)   : represents the input embedding. \n",
    "  * All of these NN have weights that are learned during training\n",
    "  * (word_embedding) : a vector of size 728 for each of the word in the vocabulary\n",
    "  * (LayerNorm)  : output from the embedding layer has a dimension of 768\n",
    "  \n",
    "* (encoder) There are 12 encoder blocks or encoders marked with index of 0 to 11\n",
    "    * (attention) : Each encoder block has 12 \"attention heads\"\n",
    "    *               Within each attention heads - weights are learned fo query, key and value vectors\n",
    "    *               Output from the multi-head attention blocks is concatenated\n",
    "    * (intermediate): represents the FFNN. Using GELU as the activation function. Output dimension is 3072\n",
    "* (output): is the last hidden state. Scaled down to dimension of 768.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8246de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Print model info\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2dfd82-c39f-4f88-be12-07dafa1c47a7",
   "metadata": {},
   "source": [
    "## 3. Parameter count, Iterate through the list\n",
    "\n",
    "The PreTrainedModel class in Transformers is a subclass of pytorch.nn.Module\n",
    "\n",
    "The pytorch.nn.Module.parameters returns an iterator over module parameters.\n",
    "\n",
    "Use the dimensionality of the tensor to calculate the number of weights that are learned in each layer or network.\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231052ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_parameters (millions) :  109.48224\n"
     ]
    }
   ],
   "source": [
    "# Check the number of parameters\n",
    "num_parameters = model.num_parameters()\n",
    "\n",
    "# print number of parameters in millions\n",
    "print(\"num_parameters (millions) : \", (num_parameters/(1e6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6830e32-1eb9-4345-904f-329963b31665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  torch.Size([30522, 768])\n",
      "1  :  torch.Size([512, 768])\n",
      "2  :  torch.Size([2, 768])\n",
      "3  :  torch.Size([768])\n",
      "4  :  torch.Size([768])\n",
      "5  :  torch.Size([768, 768])\n",
      "6  :  torch.Size([768])\n",
      "7  :  torch.Size([768, 768])\n",
      "8  :  torch.Size([768])\n",
      "9  :  torch.Size([768, 768])\n",
      "10  :  torch.Size([768])\n",
      "11  :  torch.Size([768, 768])\n",
      "12  :  torch.Size([768])\n",
      "13  :  torch.Size([768])\n",
      "14  :  torch.Size([768])\n",
      "15  :  torch.Size([3072, 768])\n",
      "16  :  torch.Size([3072])\n",
      "17  :  torch.Size([768, 3072])\n",
      "18  :  torch.Size([768])\n",
      "19  :  torch.Size([768])\n",
      "20  :  torch.Size([768])\n",
      "21  :  torch.Size([768, 768])\n",
      "22  :  torch.Size([768])\n",
      "23  :  torch.Size([768, 768])\n",
      "24  :  torch.Size([768])\n",
      "25  :  torch.Size([768, 768])\n",
      "26  :  torch.Size([768])\n",
      "27  :  torch.Size([768, 768])\n",
      "28  :  torch.Size([768])\n",
      "29  :  torch.Size([768])\n",
      "30  :  torch.Size([768])\n",
      "31  :  torch.Size([3072, 768])\n",
      "32  :  torch.Size([3072])\n",
      "33  :  torch.Size([768, 3072])\n",
      "34  :  torch.Size([768])\n",
      "35  :  torch.Size([768])\n",
      "36  :  torch.Size([768])\n",
      "37  :  torch.Size([768, 768])\n",
      "38  :  torch.Size([768])\n",
      "39  :  torch.Size([768, 768])\n",
      "40  :  torch.Size([768])\n",
      "41  :  torch.Size([768, 768])\n",
      "42  :  torch.Size([768])\n",
      "43  :  torch.Size([768, 768])\n",
      "44  :  torch.Size([768])\n",
      "45  :  torch.Size([768])\n",
      "46  :  torch.Size([768])\n",
      "47  :  torch.Size([3072, 768])\n",
      "48  :  torch.Size([3072])\n",
      "49  :  torch.Size([768, 3072])\n",
      "50  :  torch.Size([768])\n",
      "51  :  torch.Size([768])\n",
      "52  :  torch.Size([768])\n",
      "53  :  torch.Size([768, 768])\n",
      "54  :  torch.Size([768])\n",
      "55  :  torch.Size([768, 768])\n",
      "56  :  torch.Size([768])\n",
      "57  :  torch.Size([768, 768])\n",
      "58  :  torch.Size([768])\n",
      "59  :  torch.Size([768, 768])\n",
      "60  :  torch.Size([768])\n",
      "61  :  torch.Size([768])\n",
      "62  :  torch.Size([768])\n",
      "63  :  torch.Size([3072, 768])\n",
      "64  :  torch.Size([3072])\n",
      "65  :  torch.Size([768, 3072])\n",
      "66  :  torch.Size([768])\n",
      "67  :  torch.Size([768])\n",
      "68  :  torch.Size([768])\n",
      "69  :  torch.Size([768, 768])\n",
      "70  :  torch.Size([768])\n",
      "71  :  torch.Size([768, 768])\n",
      "72  :  torch.Size([768])\n",
      "73  :  torch.Size([768, 768])\n",
      "74  :  torch.Size([768])\n",
      "75  :  torch.Size([768, 768])\n",
      "76  :  torch.Size([768])\n",
      "77  :  torch.Size([768])\n",
      "78  :  torch.Size([768])\n",
      "79  :  torch.Size([3072, 768])\n",
      "80  :  torch.Size([3072])\n",
      "81  :  torch.Size([768, 3072])\n",
      "82  :  torch.Size([768])\n",
      "83  :  torch.Size([768])\n",
      "84  :  torch.Size([768])\n",
      "85  :  torch.Size([768, 768])\n",
      "86  :  torch.Size([768])\n",
      "87  :  torch.Size([768, 768])\n",
      "88  :  torch.Size([768])\n",
      "89  :  torch.Size([768, 768])\n",
      "90  :  torch.Size([768])\n",
      "91  :  torch.Size([768, 768])\n",
      "92  :  torch.Size([768])\n",
      "93  :  torch.Size([768])\n",
      "94  :  torch.Size([768])\n",
      "95  :  torch.Size([3072, 768])\n",
      "96  :  torch.Size([3072])\n",
      "97  :  torch.Size([768, 3072])\n",
      "98  :  torch.Size([768])\n",
      "99  :  torch.Size([768])\n",
      "100  :  torch.Size([768])\n",
      "101  :  torch.Size([768, 768])\n",
      "102  :  torch.Size([768])\n",
      "103  :  torch.Size([768, 768])\n",
      "104  :  torch.Size([768])\n",
      "105  :  torch.Size([768, 768])\n",
      "106  :  torch.Size([768])\n",
      "107  :  torch.Size([768, 768])\n",
      "108  :  torch.Size([768])\n",
      "109  :  torch.Size([768])\n",
      "110  :  torch.Size([768])\n",
      "111  :  torch.Size([3072, 768])\n",
      "112  :  torch.Size([3072])\n",
      "113  :  torch.Size([768, 3072])\n",
      "114  :  torch.Size([768])\n",
      "115  :  torch.Size([768])\n",
      "116  :  torch.Size([768])\n",
      "117  :  torch.Size([768, 768])\n",
      "118  :  torch.Size([768])\n",
      "119  :  torch.Size([768, 768])\n",
      "120  :  torch.Size([768])\n",
      "121  :  torch.Size([768, 768])\n",
      "122  :  torch.Size([768])\n",
      "123  :  torch.Size([768, 768])\n",
      "124  :  torch.Size([768])\n",
      "125  :  torch.Size([768])\n",
      "126  :  torch.Size([768])\n",
      "127  :  torch.Size([3072, 768])\n",
      "128  :  torch.Size([3072])\n",
      "129  :  torch.Size([768, 3072])\n",
      "130  :  torch.Size([768])\n",
      "131  :  torch.Size([768])\n",
      "132  :  torch.Size([768])\n",
      "133  :  torch.Size([768, 768])\n",
      "134  :  torch.Size([768])\n",
      "135  :  torch.Size([768, 768])\n",
      "136  :  torch.Size([768])\n",
      "137  :  torch.Size([768, 768])\n",
      "138  :  torch.Size([768])\n",
      "139  :  torch.Size([768, 768])\n",
      "140  :  torch.Size([768])\n",
      "141  :  torch.Size([768])\n",
      "142  :  torch.Size([768])\n",
      "143  :  torch.Size([3072, 768])\n",
      "144  :  torch.Size([3072])\n",
      "145  :  torch.Size([768, 3072])\n",
      "146  :  torch.Size([768])\n",
      "147  :  torch.Size([768])\n",
      "148  :  torch.Size([768])\n",
      "149  :  torch.Size([768, 768])\n",
      "150  :  torch.Size([768])\n",
      "151  :  torch.Size([768, 768])\n",
      "152  :  torch.Size([768])\n",
      "153  :  torch.Size([768, 768])\n",
      "154  :  torch.Size([768])\n",
      "155  :  torch.Size([768, 768])\n",
      "156  :  torch.Size([768])\n",
      "157  :  torch.Size([768])\n",
      "158  :  torch.Size([768])\n",
      "159  :  torch.Size([3072, 768])\n",
      "160  :  torch.Size([3072])\n",
      "161  :  torch.Size([768, 3072])\n",
      "162  :  torch.Size([768])\n",
      "163  :  torch.Size([768])\n",
      "164  :  torch.Size([768])\n",
      "165  :  torch.Size([768, 768])\n",
      "166  :  torch.Size([768])\n",
      "167  :  torch.Size([768, 768])\n",
      "168  :  torch.Size([768])\n",
      "169  :  torch.Size([768, 768])\n",
      "170  :  torch.Size([768])\n",
      "171  :  torch.Size([768, 768])\n",
      "172  :  torch.Size([768])\n",
      "173  :  torch.Size([768])\n",
      "174  :  torch.Size([768])\n",
      "175  :  torch.Size([3072, 768])\n",
      "176  :  torch.Size([3072])\n",
      "177  :  torch.Size([768, 3072])\n",
      "178  :  torch.Size([768])\n",
      "179  :  torch.Size([768])\n",
      "180  :  torch.Size([768])\n",
      "181  :  torch.Size([768, 768])\n",
      "182  :  torch.Size([768])\n",
      "183  :  torch.Size([768, 768])\n",
      "184  :  torch.Size([768])\n",
      "185  :  torch.Size([768, 768])\n",
      "186  :  torch.Size([768])\n",
      "187  :  torch.Size([768, 768])\n",
      "188  :  torch.Size([768])\n",
      "189  :  torch.Size([768])\n",
      "190  :  torch.Size([768])\n",
      "191  :  torch.Size([3072, 768])\n",
      "192  :  torch.Size([3072])\n",
      "193  :  torch.Size([768, 3072])\n",
      "194  :  torch.Size([768])\n",
      "195  :  torch.Size([768])\n",
      "196  :  torch.Size([768])\n",
      "197  :  torch.Size([768, 768])\n",
      "198  :  torch.Size([768])\n",
      "Exact count of parameters :  109482240\n"
     ]
    }
   ],
   "source": [
    "# get the list of layers\n",
    "LP=list(model.parameters())\n",
    "\n",
    "lp_count = len(LP)\n",
    "\n",
    "# iterate through list\n",
    "num_params_calc = 0\n",
    "for p in range(0,lp_count):\n",
    "    print(p, \" : \", LP[p].shape)\n",
    "    dim_1 = LP[p].shape[0]\n",
    "    try:\n",
    "        dim_2 = LP[p].shape[1]\n",
    "    except:\n",
    "        dim_2 = 1\n",
    "\n",
    "    num_params_calc = num_params_calc + (dim_1*dim_2)\n",
    "\n",
    "print(\"Exact count of parameters : \", num_params_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00fe691-df2e-4f39-944d-e582b63aaefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
