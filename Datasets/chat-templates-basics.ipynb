{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1635e455-2069-45ef-9b56-09a87281d2e8",
   "metadata": {},
   "source": [
    "# Chat templates\n",
    "\n",
    "Chat template specifies how the conversation (messages) are converted into a single tokenizable string in format that model expects. Different models expect very different input formats for chat. Chat templates are part of the tokenizer, they are used to encode the conversation into model specific format. \n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "**Pre-requisites**\n",
    "1. You must have an understanding of **Tokenizers** : Covered in lessons under the section **Hugging Face Models : Advanced**\n",
    "2. You must understand  the use of **Datasets** : Covered in lessons under the section **Datasets for Training, and Testing**\n",
    "\n",
    "**Google Colab**\n",
    "You MUST set the HuggingFace token otherwise you will get an authorization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7c15be-588a-4fc1-96f9-f944e5e16807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from IPython.display import JSON\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the file that contains the HuggingFace token\n",
    "# CHANGE THE location\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# Needed as we are using Gated models in this notebook\n",
    "# Make sure to get access to the gated models on HF\n",
    "os.environ['HF_TOKEN']=os.environ['HUGGINGFACEHUB_API_TOKEN']\n",
    "\n",
    "# Try out model for chat template for it\n",
    "model_ids = [\n",
    "                \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "                \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                \"google/gemma-2-2b-it\",\n",
    "                \"google-bert/bert-base-uncased\"\n",
    "]\n",
    "\n",
    "# Sample chat messages\n",
    "sample_chat = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a polite and helpful customer service agent\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e7fda0-4978-4609-a1e2-d7e396445a78",
   "metadata": {},
   "source": [
    "## 1. Checkout special tokens\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Some of the models are **gated** and requires you to get access on HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8ce15e-afd6-4e8c-8b7e-b3c1bda72177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :  google-bert/bert-base-uncased\n",
      "Special tokens : \n",
      " {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "Padding side :  right\n",
      "Tuncation side :  right\n"
     ]
    }
   ],
   "source": [
    "# change this to try out the model of your choice\n",
    "test_model_index = 3\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[test_model_index])\n",
    "\n",
    "print(\"Model : \", model_ids[test_model_index])\n",
    "print(\"Special tokens : \\n\", tokenizer.special_tokens_map)\n",
    "# print(\"Additional special tokens : \", tokenizer.additional_special_tokens)\n",
    "print(\"Padding side : \", tokenizer.padding_side)\n",
    "print(\"Tuncation side : \", tokenizer.truncation_side)\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e2f4d-ba9c-4190-add8-7c7004041ad6",
   "metadata": {},
   "source": [
    "## 2. Checkout chat template\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.47.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.apply_chat_template\n",
    "\n",
    "\n",
    "**Note**\n",
    "\n",
    "* Some models may not be compatible with ChatML\n",
    "* Some parameters have no effect in case of *tokenize=False*\n",
    "\n",
    "**Mistral:**  Tokenizer adds the control tokens [INST] and [/INST] to indicate the start and end of user messages (but not assistant messages!), and the entire chat is condensed into a single string. \n",
    "\n",
    "**Gemma:** Does not support  message with the *role=system*\n",
    "\n",
    "**BERT:**  Does not support chat_template as it is NOT a chat model (but a base model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10f91e9-7fa3-4e00-b8bf-5e7ee18048dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model :  meta-llama/Llama-3.3-70B-Instruct\n",
      "======================================\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a polite and helpful customer service agent<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm doing great. How can I help you today?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'd like to show off how chat templating works!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# change this to try out the model of your choice\n",
    "test_model_index = 0\n",
    "\n",
    "# Whether to tokenize or not\n",
    "tokenize_or_not = False\n",
    "\n",
    "# By default tokenize returns an array of tokens\n",
    "# By setting the return_dicts you can get a dictionary\n",
    "return_dict_or_not = False\n",
    "\n",
    "# Whether to pad or not - no effect if tokenize=False\n",
    "pad_or_not = True\n",
    "\n",
    "truncation_or_not = True\n",
    "\n",
    "# max_length controls the length - if not specified model max_length is used\n",
    "max_length = 1024\n",
    "\n",
    "# Whether to add assistant marker at end or not\n",
    "# Generation Prompt\n",
    "add_generation_prompt_or_not = False\n",
    "\n",
    "# If true, doesn't add the end marker - prompts the model to continue with generation of last message\n",
    "# Cannot be true if add_generation_prompt=True\n",
    "continue_final_message_or_not = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[test_model_index])\n",
    "\n",
    "print(\"Model : \", model_ids[test_model_index])\n",
    "print(\"======================================\")\n",
    "\n",
    "# Some models may NOT support the ChatML format - for example gemma \n",
    "try:\n",
    "    formatted_chat = tokenizer.apply_chat_template(sample_chat, \n",
    "                                                   tokenize = tokenize_or_not, \n",
    "                                                   max_length = max_length,\n",
    "                                                   padding = pad_or_not,\n",
    "                                                   truncation = truncation_or_not,\n",
    "                                                   add_generation_prompt=add_generation_prompt_or_not,\n",
    "                                                   continue_final_message=continue_final_message_or_not,\n",
    "                                                   return_dict = return_dict_or_not)\n",
    "    print(formatted_chat)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982776f-5c1c-482e-aa05-15f4524e7b90",
   "metadata": {},
   "source": [
    "## 3. Prepare the training dataset for Chat\n",
    "\n",
    "* Create a test dataset from a dictionary\n",
    "* Add a new column to dataset for the *model specific formatted chat text*\n",
    "* Fine-tuning is carried out using the chats in *formatted chat column* in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e76f9-5ebc-4d41-99f9-e5430d567c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]\n",
    "\n",
    "test_model_index = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[test_model_index])\n",
    "\n",
    "# Create a dataset from dictionary\n",
    "chat_dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "\n",
    "JSON(chat_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ee67e-5aef-44d7-b6fb-61656e0cd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use map to create a formattet chat column in the dataset\n",
    "chat_dataset_formatted = chat_dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "\n",
    "# Print the row\n",
    "JSON(chat_dataset_formatted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3605e8-e7fd-4019-b3d8-00da06de1d9d",
   "metadata": {},
   "source": [
    "## 4. Custom templates\n",
    "\n",
    "**Jinja** is a template engine for Python that allows developers to create dynamic content.\n",
    "\n",
    "https://jinja.palletsprojects.com/en/stable/templates/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df40cfe-d4ec-449b-8ec8-2da971adde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_chat_template = \"\"\"\n",
    "{%- for message in messages %}\n",
    "    {{- '<|bert_start|>' + message['role'] + '\\n' + message['content'] + '<|bert_end|>' + '\\n' }}\n",
    "{%- endfor %}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddc476-7e10-4f27-b6b3-79afa05ab6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Base Uncased\n",
    "test_model_index = 4\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ids[test_model_index])\n",
    "\n",
    "tokenizer.chat_template = custom_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53feaf8-c1e4-4ed7-84d4-c268828a4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_chat = tokenizer.apply_chat_template(sample_chat, tokenize=False)\n",
    "formatted_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2565bb40-9bc9-42f7-b7d7-188a3e22901d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
