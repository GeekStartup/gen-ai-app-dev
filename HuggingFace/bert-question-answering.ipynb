{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230d891b",
   "metadata": {},
   "source": [
    "# Exercise#2 : Question-Answering Task\n",
    "\n",
    "Using a Bert model fine tuned for Q&A on Squad 2,0 dataset.\n",
    "\n",
    "https://huggingface.co/distilbert-base-cased-distilled-squad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19222f37-618c-4dab-9de6-0577076fc53b",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3a1e57-4252-4089-b3b2-2273c26dbba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788be9aa-078c-4578-9107-cd3f9521e0c9",
   "metadata": {},
   "source": [
    "### Import appropriate model & config classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "579cb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForQuestionAnswering\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import getpass\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e0f3f",
   "metadata": {},
   "source": [
    "## 1. Auto Model Class for task\n",
    "\n",
    "AutoModelForQuestionAnswering\n",
    "\n",
    "https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "\n",
    "Model in use\n",
    "\n",
    "https://huggingface.co/distilbert-base-cased-distilled-squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27b6f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa10d2df7ba4b7abb944d41f1bd0842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e0442cab0d47a7aa20dae29b31182b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8834285db984441bd4cc82ae4382e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60defebbc23d453d9b18e038a361fbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdee657aa74a43aaa70b558d40b44e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model in use\n",
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "# Create tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634557f",
   "metadata": {},
   "source": [
    "## 2. Setup context and questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "069369be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the context and questions\n",
    "\n",
    "context = \"\"\"Beyonce Giselle Knowles-Carter (/biː\\ˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) \n",
    "is an American singer, songwriter, record producer and actress. Born and raised in Houston, \n",
    "Texas, she performed in various singing and dancing competitions as a child, and rose \n",
    "to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her \n",
    "father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of \n",
    "all time. Their hiatus saw the release of Beyonce\\'s debut album, Dangerously in Love (2003), \n",
    "which established her as a solo artist worldwide, earned five Grammy Awards and featured the \n",
    "Billboard Hot 100 number-one singles \\\"Crazy in Love\\\" and \\\"Baby Boy.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "   \"When did Beyonce start becoming popular?\",\n",
    "   \"When did Beyonce leave Destiny's Child and become a solo singer?\",\n",
    "   \"What areas did Beyonce compete in when she was growing up?\",\n",
    "   \"In what city and state did Beyonce grow up?\",\n",
    "   \"In what R&B group was she the lead singer?\"\n",
    "]\n",
    "\n",
    "## Demonstrates that model understands the context and is not just using pattern matching\n",
    "## Uncomment to try these out !!!\n",
    "# context = \"two plus two is four. 6 plus 2 is eight. 1 plus nine is 10.\"\n",
    "# questions = [\"what is 2 + 2\", \"what is 6 + 2\", \"what is two + 1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d68dd7",
   "metadata": {},
   "source": [
    "## 3. Tokenize, and run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647f0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try for any question by changing index\n",
    "index = 1\n",
    "inputs = tokenizer(questions[index], context, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Printhe tokenized string\n",
    "# print(inputs)\n",
    "# print(tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "# print(type(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30a8d3",
   "metadata": {},
   "source": [
    "## 4. Interpret the output\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput\n",
    "    \n",
    "Output has the following tensors:\n",
    "\n",
    "* **start_logits** A score for each token in the context. The token with the maximum score is the start token for the answer\n",
    "\n",
    "* **end_logits** A score for each token in the context. The token with the maximum score is the start token for the answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "585d1cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start index:  tensor(149)   Token at start_index:  2003\n",
      "End index:  tensor(150)   Token at end_index:  2003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2003'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logits in the QuestionAnsweringOutput class are start/end of response\n",
    "# LINK\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# print(start_scores)\n",
    "# print(end_scores)\n",
    "\n",
    "# Get the start index = index for token that has the maximum score\n",
    "start_index = torch.argmax(start_scores)\n",
    "\n",
    "# This is to clarify the idea behind end index\n",
    "print(\"Start index: \", start_index, \"  Token at start_index: \", tokenizer.decode(inputs['input_ids'][0][start_index]))\n",
    "\n",
    "# Get the end index = index for the token that has the maximum score\n",
    "# Add 1 to include the last token\n",
    "end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "# This is to clarify the idea behind end index\n",
    "print(\"End index: \", end_index, \"  Token at end_index: \", tokenizer.decode(inputs['input_ids'][0][end_index-1]))\n",
    "\n",
    "# Decode the tokens between start_index & end_index\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0, start_index:end_index])\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea7274",
   "metadata": {},
   "source": [
    "# Using pipeline\n",
    "\n",
    "An instance of QuestionAnsweringPipeline is used. Refer to documentation for parameters.\n",
    "\n",
    "https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89008fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f680c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: '2003', score: 0.9859, start: 532, end: 536\n"
     ]
    }
   ],
   "source": [
    "question_answerer_pipeline = pipeline(\"question-answering\", model=model_name)\n",
    "\n",
    "result = question_answerer_pipeline(question=questions[1],     context=context)\n",
    "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50cf68a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9425580501556396, 'start': 93, 'end': 95, 'answer': '13'}\n",
      "{'score': 0.8635509014129639, 'start': 10, 'end': 21, 'answer': '176 billion'}\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/distilbert-base-cased-distilled-squad\n",
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
    "\n",
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\"\n",
    "\n",
    "# https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline\n",
    "output = question_answerer(question=question, context=context)\n",
    "\n",
    "print(output)\n",
    "\n",
    "\n",
    "question = \"How many parameters does BLOOM have?\"\n",
    "output = question_answerer(question=question, context=context)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b9807",
   "metadata": {},
   "source": [
    "# Using Inference Client\n",
    "\n",
    "Use the question_answering(...) function in InferenceClient class\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/v0.20.2/en/package_reference/inference_client#huggingface_hub.InferenceClient.question_answering\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad9c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8342d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will prompted for the HuggingFace token\n",
    "print(\"Copy/paste HuggingFace token and hit <enter>\")\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass.getpass()\n",
    "\n",
    "client = InferenceClient(model=model_name, token=HUGGINGFACEHUB_API_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2aaf4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: yuFYn4OP68UAEUaGJptDe)\n\nBad request:\nAuthorization header is correct, but the token seems invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/distilbert-base-cased-distilled-squad",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m client\u001b[38;5;241m.\u001b[39mquestion_answering(question, context)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:1351\u001b[0m, in \u001b[0;36mInferenceClient.question_answering\u001b[1;34m(self, question, context, model)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03mRetrieve the answer to a question from a given text.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m payload: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: context}\n\u001b[1;32m-> 1351\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m   1352\u001b[0m     json\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[0;32m   1353\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1354\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1355\u001b[0m )\n\u001b[0;32m   1356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m QuestionAnsweringOutputElement\u001b[38;5;241m.\u001b[39mparse_obj_as_instance(response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     hf_raise_for_status(response)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:358\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    355\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m     )\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n",
      "\u001b[1;31mBadRequestError\u001b[0m:  (Request ID: yuFYn4OP68UAEUaGJptDe)\n\nBad request:\nAuthorization header is correct, but the token seems invalid"
     ]
    }
   ],
   "source": [
    "client.question_answering(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbb5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
