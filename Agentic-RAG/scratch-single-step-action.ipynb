{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ed7585-a502-4bbf-8483-0df5e6dfbed7",
   "metadata": {},
   "source": [
    "# Single step agent\n",
    "\n",
    "1. Setup prompt that asks an LLM to use provided tools or use tool responses to generate answers\n",
    "2. Setup the tools, descriptions\n",
    "3. **Single Step Agent**:\n",
    "   - Asks LLM to pick the tools to run\n",
    "   - Runs the tools\n",
    "   - Sends the tools responses to LLM to generate final answer\n",
    "4. Test setup\n",
    "\n",
    "Note:\n",
    "* Code uses the OpenAI GPT 3.5 turbo model\n",
    "* Code does not use Langchain Agent & Tool classes (covered later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2b1d8-251a-4b0a-b113-1d9bfca8b72b",
   "metadata": {},
   "source": [
    "## Setup LLM\n",
    "Same LLM will be used for planning & answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcee5a3-230e-4d0f-a61b-bb7c3944acc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\raj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm, create_anthropic_chat_llm, create_hugging_face_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "# llm = create_gpt_chat_llm()\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "llm = create_hugging_face_chat_llm(repo_id=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f932d9-f0a0-4776-b96a-c52188884994",
   "metadata": {},
   "source": [
    "## 1. Setup the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f862de3b-b134-4a0e-a4f9-8a41897fdc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a helpful assistant who can answer questions on a variety of topics. \n",
    "Do not use your own internal knowledge or information to answer the question. \n",
    "Think step by step and use only the following available tools.\n",
    "\n",
    "Tools:\n",
    "{tools}\n",
    "\n",
    "There is no need for preamble, just give the response in ONE of the following valid JSON formats:\n",
    "\n",
    "option 1:\n",
    "use this if you need tools to be run to get the information\n",
    "{{\"actions\": [{{ \"action\" : tool name, \"arguments\" : dictionary of argument values}}}}]\n",
    "\n",
    "option 2:\n",
    "use this if you can use the esponses from the tool to answer the question\n",
    "{{\"answer\": \"your response to the question\", \"explanation\": \"provide your explanation here\"}}\n",
    "\n",
    "Here is the question: {question}\n",
    "\n",
    "tool_responses: {tool_responses}\n",
    "\n",
    "response:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = ['tools', 'question', 'tool_responses']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f44c12-4358-405a-8099-a1f305a25da9",
   "metadata": {},
   "source": [
    "## 2. Setup tools\n",
    "* Dummy tool functions for stock price & city weather\n",
    "* Tool map = dictionary of functions with function name as the key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4af73d-9733-48c4-8084-2512b9b8c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tool 1 for stocks\n",
    "def  company_stock_price(stock_symbol: str) -> float:\n",
    "    if stock_symbol.upper()=='AAPL':\n",
    "        return  {\"price\": 192.32}\n",
    "    elif stock_symbol.upper()=='MSFT':\n",
    "        return  {\"price\": 415.60}\n",
    "    elif stock_symbol.upper()=='AMZN':\n",
    "        return  {\"price\": 183.60}\n",
    "    else:\n",
    "        return {\"price\": \"unknown\"}\n",
    "\n",
    "\n",
    "stock_tool_description = {\n",
    "    \"name\" : \"company_stock_price\",\n",
    "    \"description\": \"This tool returns the last known stock price for a company based on its ticker symbol\",\n",
    "    \"arguments\": [\n",
    "        {\"stock_symbol\" : \"stock ticker symbol for the company\"}\n",
    "    ],\n",
    "    \"response\": \"last known stock price\"\n",
    "}\n",
    "\n",
    "## Tool 2 for city weather\n",
    "def city_weather(city: str) -> int:\n",
    "    if city.lower() == \"new york\":\n",
    "        return {\"temperature\": 68, \"forecast\": \"rain\"}\n",
    "    elif city.lower() == \"paris\":\n",
    "        return {\"temperature\": 73, \"forecast\": \"sunny\"}\n",
    "    elif city.lower() == \"london\":\n",
    "        return {\"temperature\": 82, \"forecast\": \"cloudy\"}\n",
    "    else:\n",
    "        return {\"temperature\": \"unknown\"}\n",
    "        \n",
    "city_weather_tool_description = {\n",
    "    \"name\" : \"city_weather\",\n",
    "    \"description\": \"This tool returns the current temperature and forecast for the given city\",\n",
    "    \"arguments\": [\n",
    "        {\"city\" : \"name of the city\"}\n",
    "    ],\n",
    "    \"response\": \"current temperature & forecast\"\n",
    "}\n",
    "\n",
    "# Maintain the tools in a map for invocation by th eagent\n",
    "tools = [stock_tool_description, city_weather_tool_description]\n",
    "tools_map = {\n",
    "    'company_stock_price': company_stock_price,\n",
    "    'city_weather' : city_weather\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b2d07-7fae-4a69-ae69-f0bc8c8a1460",
   "metadata": {},
   "source": [
    "## 3. Agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff8a1e9-bdae-4767-a884-63cc4d4d3503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to process the actions receieved from the LLM\n",
    "# Responses from the tools are expected to be in JSON format \n",
    "def   invoke_action_functions(response):\n",
    "    action_responses = []\n",
    "    if len(response[\"actions\"]) == 0:\n",
    "        print('question cannot be answered as there is no tool to use !!!')\n",
    "        exit\n",
    "    else:\n",
    "        for action in response[\"actions\"]:\n",
    "\n",
    "            # Get the function pointer from the map\n",
    "            action_function = tools_map[action[\"action\"]]\n",
    "            \n",
    "            # Invoke the tool/function with the arguments as suggested by the LLM\n",
    "            action_invoke_result = action_function(**action[\"arguments\"])\n",
    "            action[\"response\"] = action_invoke_result\n",
    "\n",
    "            # Add the response to the action attribute\n",
    "            action_responses.append(action)\n",
    "\n",
    "    # Return the response      \n",
    "    return action_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d9f564-92b5-4729-a58d-5d06b14d6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for answering the question\n",
    "def  process_query(question):\n",
    "    \n",
    "    # Setup the prompt. Since no tool has been invoked set action_response as blank\n",
    "    query = prompt.format(tools=tools, question=question, tool_responses=\"\")\n",
    "\n",
    "    # STEP-1 Invoke LLM for a plan i.e., tools to execute\n",
    "    # ===================================================\n",
    "    # Invoke LLM to get the tools to be run\n",
    "    # The response consist of tools that LLM requires to be executed\n",
    "    response = llm.invoke(query)\n",
    "\n",
    "    # Convert response to JSON object. The response is of type AIMessage\n",
    "    response_json = json.loads(response.content)\n",
    "\n",
    "    # print the response\n",
    "    print(\"STEP-1:\", response_json)\n",
    "\n",
    "    # STEP-2  Invoke the tool(s) suggested by LLM\n",
    "    # ===========================================\n",
    "    # LLM may respond with an answer \n",
    "    # It may happen if LLM determines that no tool is available for responding to the question\n",
    "    action_responses=[]\n",
    "    if \"answer\" in response_json:\n",
    "        # If the answer is already there\n",
    "        return {\"answer\" : response_json[\"answer\"]}\n",
    "    elif \"actions\" in response_json:\n",
    "        # If the LLM has suggested tools to be executed, execute the tools\n",
    "        action_responses = invoke_action_functions(response_json)\n",
    "\n",
    "    # Print the tool responses\n",
    "    print(\"STEP-2\", \"   Agent tool invocation responses :\", action_responses)\n",
    "\n",
    "    # STEP-3  Invoke LLM to generate final response\n",
    "    # =============================================\n",
    "    # Now send the action responses to LLM for generating the answer\n",
    "    query = prompt.format(tools=tools, question=question,tool_responses=action_responses)\n",
    "    response = llm.invoke(query)\n",
    "\n",
    "    # print the response\n",
    "    print(\"STEP-3:\", response_json)\n",
    "\n",
    "    # Convert response to JSON object\n",
    "    response_json = json.loads(response.content)\n",
    "\n",
    "    # Extract the answer from the response    \n",
    "    if \"answer\" in response_json:\n",
    "        return response_json[\"answer\"]\n",
    "    else:\n",
    "        return (\"Can't generate as there is no response from the tool!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc5ccf-3755-48e7-bbeb-0eeeb1adaa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fff2bfe-bf71-459a-a509-29c2236f4346",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb069cbc-093e-4218-abc4-6f9d470455ad",
   "metadata": {},
   "source": [
    "### 1. Simple test that uses 1 tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ce2fda5-d301-4dca-b597-bc87ff74c1c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": " (Request ID: GzPJqSG6DKxb_1wvWbMH2)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-70B-Instruct",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich of these cities is hotter, Paris or London\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am visting paris, should i carry an umbrella?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m process_query(question)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal response::\u001b[39m\u001b[38;5;124m\"\u001b[39m,response)\n",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m, in \u001b[0;36mprocess_query\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m      5\u001b[0m query \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mformat(tools\u001b[38;5;241m=\u001b[39mtools, question\u001b[38;5;241m=\u001b[39mquestion, tool_responses\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# STEP-1 Invoke LLM for a plan i.e., tools to execute\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ===================================================\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Invoke LLM to get the tools to be run\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# The response consist of tools that LLM requires to be executed\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert response to JSON object. The response is of type AIMessage\u001b[39;00m\n\u001b[0;32m     14\u001b[0m response_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:346\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    343\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    344\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    347\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    348\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    349\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    350\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    351\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    352\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    353\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    355\u001b[0m         )\n\u001b[0;32m    356\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    357\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    358\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:703\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    697\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    701\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    702\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:882\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    868\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    869\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    870\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    880\u001b[0m         )\n\u001b[0;32m    881\u001b[0m     ]\n\u001b[1;32m--> 882\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    883\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    884\u001b[0m     )\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:740\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    739\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    741\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:727\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    718\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    719\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    724\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 727\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    728\u001b[0m                 prompts,\n\u001b[0;32m    729\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    730\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    731\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    732\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    733\u001b[0m             )\n\u001b[0;32m    734\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    735\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    736\u001b[0m         )\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1431\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1430\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1431\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1433\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1434\u001b[0m     )\n\u001b[0;32m   1435\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\langchain_community\\llms\\huggingface_endpoint.py:266\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m    267\u001b[0m         json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: invocation_params},\n\u001b[0;32m    268\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    269\u001b[0m         task\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask,\n\u001b[0;32m    270\u001b[0m     )\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:240\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     hf_raise_for_status(response)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:329\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    326\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n\u001b[1;32m--> 329\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mBadRequestError\u001b[0m:  (Request ID: GzPJqSG6DKxb_1wvWbMH2)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."
     ]
    }
   ],
   "source": [
    "question = \"Which of these cities is hotter, Paris or London\"\n",
    "question = \"I am visting paris, should i carry an umbrella?\"\n",
    "\n",
    "response = process_query(question)\n",
    "\n",
    "print(\"Final response::\",response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e859be7-f0a9-44f7-acdd-158bcb73c46c",
   "metadata": {},
   "source": [
    "### 2. Test for a scenario when no appropriate tool is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c315ae8-22f1-4272-a28c-b48a3e510aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP-1: {'actions': [{'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AAPL'}}]}\n",
      "STEP-2    Agent tool invocation responses : [{'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AAPL'}, 'response': {'price': 192.32}}]\n",
      "STEP-3: {'actions': [{'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AAPL'}, 'response': {'price': 192.32}}]}\n",
      "Final response: You can invest in AAPL stock as the last known stock price is $192.32\n"
     ]
    }
   ],
   "source": [
    "question = \"I have only $200, which stock can I invest in?\"\n",
    "\n",
    "response = process_query(question)\n",
    "\n",
    "print(\"Final response:\",response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac578414-77f7-4b9e-8556-23b3776deb20",
   "metadata": {},
   "source": [
    "### 3. A complex scenario requiring use of more than one tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db8c90b-76b2-421c-83a4-fc2fcce651b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP-1: {'actions': [{'action': 'city_weather', 'arguments': {'city': 'New York'}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AAPL'}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'MSFT'}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AMZN'}}]}\n",
      "STEP-2    Agent tool invocation responses : [{'action': 'city_weather', 'arguments': {'city': 'New York'}, 'response': {'temperature': 68, 'forecast': 'rain'}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AAPL'}, 'response': {'price': 192.32}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'MSFT'}, 'response': {'price': 415.6}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AMZN'}, 'response': {'price': 183.6}}]\n",
      "STEP-3: {'actions': [{'action': 'city_weather', 'arguments': {'city': 'New York'}, 'response': {'temperature': 68, 'forecast': 'rain'}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AAPL'}, 'response': {'price': 192.32}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'MSFT'}, 'response': {'price': 415.6}}, {'action': 'company_stock_price', 'arguments': {'stock_symbol': 'AMZN'}, 'response': {'price': 183.6}}]}\n",
      "Final response: You should invest in Microsoft (MSFT) stock.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" \n",
    "I am interested in investing in one of these stocks [AAPL, MSFT, AMZN].\n",
    "I will make my decision based on the weather forecast in the city I am in and the price of the stock.\n",
    "If the weather is sunny then I will chose a stock that has the lowest price.\n",
    "If the weather is raining then I will chose a stock that has the highest price.\n",
    "If the wwather is cloudy then I will not buy any stock.\n",
    "\n",
    "I am in new york, which stock should I invest in?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = process_query(question)\n",
    "\n",
    "print(\"Final response:\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25aa896-f8f3-458c-95da-eeb987e1156b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f659b7168a2a48899360112355770392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06299726d4ff4ad2a3be8111dc2949cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/59.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e55f0cc5964d88a25803f6e1794de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6c8e53bdbb4faf9d3c8c83a601f90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00030.safetensors:   0%|          | 0.00/4.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561cd706be9949de806d463e0311e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eff792f2c794b959a970cec71a1d7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4db45a4ec8d44f98b6414283172a6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7d870ea495413291834a901b33d365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c277c7ce005841b8a04fd03bd72ee540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7b84bc3e77420ca8b072a53a2f0fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1887070d30d4a45a0cee3db51dce5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb0b68668bc4f9988b0f5874e4b3b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00030.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ae24d414574a6489d400f26d78702d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00010-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becf0be2c07646c4abe9161078e44388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00011-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6e13a437224754a6484e16990d5685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00012-of-00030.safetensors:   0%|          | 0.00/4.66G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce936715380f4724a743af24612f9ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00013-of-00030.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01f399-c34b-424a-ae14-392ec26c8d83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
