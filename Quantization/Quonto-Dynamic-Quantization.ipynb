{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94f15c8-f54b-4d65-a5fe-40fc5faa5479",
   "metadata": {},
   "source": [
    "# HuggingFace Quanto\n",
    "\n",
    "Notebook demonstrates the use of HuggingFace Quanto library for **dynamic** quantization.\n",
    "\n",
    "#### Note:\n",
    "Quanto does not create serializable quantized models.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/quantization/quanto\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065752ea-8cdf-4f26-8599-e7491cd6608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q quanto "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b4fa8-33e1-4c73-af86-9ae70ea3390e",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "Taken from the HuggingFace blog that explains the quantization.\n",
    "\n",
    "https://blog.gopenai.com/linear-quantization-with-hugging-face-quanto-222a1d29721f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e08a027-ef14-4e4f-8512-2733b4d63dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def named_module_tensors(module, recurse=False):\n",
    "    \"\"\"\n",
    "    Generator function that yields named tensors (parameters and buffers) from a PyTorch module.\n",
    "\n",
    "    This function iterates over all the named parameters and buffers in a given module and yields\n",
    "    the name of each tensor and its corresponding value. For parameters with `_data` or `_scale` \n",
    "    attributes, it yields those as separate entries.\n",
    "\n",
    "    Args:\n",
    "        module (torch.nn.Module): The PyTorch module from which to extract named tensors.\n",
    "        recurse (bool, optional): If True, recurses through the module's submodules. Defaults to False.\n",
    "\n",
    "    Yields:\n",
    "        Tuple[str, torch.Tensor]: A tuple where the first element is the name of the tensor, and the \n",
    "                                  second element is the tensor itself.\n",
    "\n",
    "    Example:\n",
    "        >>> model = torch.nn.Linear(10, 5)\n",
    "        >>> for name, tensor in named_module_tensors(model):\n",
    "        >>>     print(f\"{name}: {tensor.shape}\")\n",
    "    \"\"\"\n",
    "    for named_parameter in module.named_parameters(recurse=recurse):\n",
    "      name, val = named_parameter\n",
    "      flag = True\n",
    "      if hasattr(val,\"_data\") or hasattr(val,\"_scale\"):\n",
    "        if hasattr(val,\"_data\"):\n",
    "          yield name + \"._data\", val._data\n",
    "        if hasattr(val,\"_scale\"):\n",
    "          yield name + \"._scale\", val._scale\n",
    "      else:\n",
    "        yield named_parameter\n",
    "\n",
    "    for named_buffer in module.named_buffers(recurse=recurse):\n",
    "      yield named_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5982c85b-d247-4af0-a853-74299e4eb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_byte_size(dtype):\n",
    "    \"\"\"\n",
    "    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if dtype == torch.bool:\n",
    "        return 1 / 8\n",
    "    bit_search = re.search(r\"[^\\d](\\d+)$\", str(dtype))\n",
    "    if bit_search is None:\n",
    "        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n",
    "    bit_size = int(bit_search.groups()[0])\n",
    "    return bit_size // 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f71e7d-2395-4ac2-843b-2e190caf6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_module_sizes(model):\n",
    "    \"\"\"\n",
    "    Compute the size of each submodule of a given model.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    module_sizes = defaultdict(int)\n",
    "    for name, tensor in named_module_tensors(model, recurse=True):\n",
    "      size = tensor.numel() * dtype_byte_size(tensor.dtype)\n",
    "      name_parts = name.split(\".\")\n",
    "      for idx in range(len(name_parts) + 1):\n",
    "        module_sizes[\".\".join(name_parts[:idx])] += size\n",
    "\n",
    "    return module_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3297541-27b5-4b50-9246-58f416d3f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_output(model, text):\n",
    "    \"\"\"\n",
    "    Generates and prints the output from a language model based on the provided text input.\n",
    "\n",
    "    This function tokenizes the input text, feeds it to the given model, and generates text \n",
    "    based on the model's output. The generated output is decoded and returned.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): The language model used to generate the output.\n",
    "        text (str): The input text to be tokenized and provided to the model.\n",
    "\n",
    "    Example:\n",
    "        >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "        >>> test_model_output(model, \"Once upon a time\")\n",
    "        Once upon a time there was a little girl who...\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dc868fb-3e41-4b54-b941-bb5598f7a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def   get_weights_dtype(model_eval, num_layers=3):\n",
    "    for i, tup in enumerate(model_eval.named_parameters()):\n",
    "        if i == num_layers:\n",
    "            break;\n",
    "        name = tup[0]\n",
    "        param = tup[1]\n",
    "        print(f\"{i}. Parameter: {name}, Data Type: {param.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11cff38-eb3f-4ebe-a302-632770405a49",
   "metadata": {},
   "source": [
    "## Original model output & size\n",
    "Create an instance of the non-quantized model. \n",
    "\n",
    "1. Check the dtype used for parameters\n",
    "2. Evaluate the model size\n",
    "3. Invoke model to test its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a916f7d-a2d6-44b1-b764-ca458d05c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "\n",
    "# Doesn't work well with this\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "\n",
    "# Works well with this\n",
    "model_id = \"openai-community/gpt2\"\n",
    "model_id = \"facebook/opt-125m\"\n",
    "\n",
    "# Create the tokenizer and model\n",
    "# clean_up_tokenization_spaces=True added to prevent future warnings\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76fab5f-cbea-46fe-a108-c32703285be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size :  0.500957184 GB\n",
      "Test input :  Once upon a time, there was a\n",
      "Test response :  Once upon a time, there was a time when the world was a different place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data type used for model weights\n",
    "# get_weights_dtype(model)\n",
    "\n",
    "# Compute the in memory size of the model\n",
    "module_sizes = compute_module_sizes(model) \n",
    "original_model_size = module_sizes['']\n",
    "print(f\"Model size :  {module_sizes[''] * 1e-9} GB\")\n",
    "\n",
    "# Invoke the model\n",
    "text = \"Once upon a time, there was a\"\n",
    "response = test_model_output(model, text)\n",
    "\n",
    "# Print the input text & response\n",
    "print(\"Test input : \", text)\n",
    "print(\"Test response : \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d32ec-b157-450b-9e9b-a8e870107f3c",
   "metadata": {},
   "source": [
    "## Quantized model\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24aee822-8878-49f3-971c-e2d15d4691ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup configuration for quantization\n",
    "# Supported values : (“float8”,“int8”,“int4”,“int2”)\n",
    "quantization_config = QuantoConfig(weights=\"int8\")\n",
    "\n",
    "# Added to suppress warning\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config= quantization_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b869d55-cf6b-48bc-a6a2-d8ee0075df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size :  0.24648556800000002 GB\n",
      "Test input :  Once upon a time, there was a\n",
      "Test response :  Once upon a time, there was a time when the world was a better place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data type used for model weights\n",
    "# They are still float32 as quantization is applied dynamically\n",
    "# get_weights_dtype(quantized_model)\n",
    "\n",
    "# Compute the in memory size of the model\n",
    "module_sizes = compute_module_sizes(quantized_model) \n",
    "quantized_model_size = module_sizes['']\n",
    "print(f\"Model size :  {module_sizes[''] * 1e-9} GB\")\n",
    "\n",
    "# Invoke the model\n",
    "text = \"Once upon a time, there was a\"\n",
    "response = test_model_output(quantized_model, text)\n",
    "\n",
    "# Print the input text & response\n",
    "print(\"Test input : \", text)\n",
    "print(\"Test response : \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b6cb829-6add-4d88-b3a2-babe5dfe873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage memory saving : 51%\n"
     ]
    }
   ],
   "source": [
    "# Memory saving\n",
    "pct_saving = round((original_model_size - quantized_model_size)*100/original_model_size)\n",
    "print(f\"Percentage memory saving : {pct_saving}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61c64a5-625e-4c90-ab2a-febaa4c5d9db",
   "metadata": {},
   "source": [
    "## Quanto models are not serializable\n",
    "\n",
    "The following code will fail.\n",
    "\n",
    "```The model is quantized with QuantizationMethod.QUANTO and is not serializable```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d18f9da-b617-4e0d-8bd4-a83c87b8228d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3bf15e5966403fb2ca2b8233a0d907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b79e0e1-08a5-4e60-9ef3-6b8dca82cfe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model is quantized with QuantizationMethod.QUANTO and is not serializable - check out the warnings from the logger on the traceback to understand the reason why the quantized model is not serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quantized_model\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt-125m-quanto-8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt-125m-quanto-8bit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\modeling_utils.py:2844\u001b[0m, in \u001b[0;36mPreTrainedModel.push_to_hub\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[0;32m   2843\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tags\n\u001b[1;32m-> 2844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\utils\\hub.py:930\u001b[0m, in \u001b[0;36mPushToHubMixin.push_to_hub\u001b[1;34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m files_timestamps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files_timestamps(work_dir)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# Save all files.\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_pretrained(work_dir, max_shard_size\u001b[38;5;241m=\u001b[39mmax_shard_size, safe_serialization\u001b[38;5;241m=\u001b[39msafe_serialization)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;66;03m# Update model card if needed:\u001b[39;00m\n\u001b[0;32m    933\u001b[0m model_card\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(work_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\modeling_utils.py:2529\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[1;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[0;32m   2524\u001b[0m quantization_serializable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hf_quantizer, HfQuantizer) \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mis_serializable\n\u001b[0;32m   2526\u001b[0m )\n\u001b[0;32m   2528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quantization_serializable:\n\u001b[1;32m-> 2529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2530\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model is quantized with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_quantizer\u001b[38;5;241m.\u001b[39mquantization_config\u001b[38;5;241m.\u001b[39mquant_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and is not serializable - check out the warnings from\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2531\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the logger on the traceback to understand the reason why the quantized model is not serializable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2532\u001b[0m     )\n\u001b[0;32m   2534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m   2535\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2536\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2537\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The model is quantized with QuantizationMethod.QUANTO and is not serializable - check out the warnings from the logger on the traceback to understand the reason why the quantized model is not serializable."
     ]
    }
   ],
   "source": [
    "quantized_model.push_to_hub(\"opt-125m-quanto-8bit\")\n",
    "tokenizer.push_to_hub(\"opt-125m-quanto-8bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dab569-7aab-4e8f-a415-aa58ee77c37b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
