{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce20160-aced-4453-bffb-7d4723aa1e10",
   "metadata": {},
   "source": [
    "# Multi Query Retriever\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_query.MultiQueryRetriever.html\n",
    "\n",
    "1. Create the LLM (Cohere)\n",
    "2. Create the VectorDB to be used as retriever\n",
    "3. Create Multi Query Retriever\n",
    "4. Test\n",
    "\n",
    "#### When to use?\n",
    "* The chunks are small in size with overlapping semantic meanings\n",
    "* Chunks are sensitive to queries\n",
    "* Use for end user facing chatbots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaaf446-8d56-4a89-aaf5-e4fcd40a45a8",
   "metadata": {},
   "source": [
    "## 1. Create LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8f8870-7e6d-49e3-a4ca-71a557d812f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "import logging\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "llm = create_cohere_chat_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb385703-8e46-42da-92a8-56aa261bf85a",
   "metadata": {},
   "source": [
    "## 2. Create Vectorstore, Text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b4b381-d2e0-4070-9985-ab6746fdd626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.chroma.Chroma object at 0x0000023D38A45150>\n"
     ]
    }
   ],
   "source": [
    "# Create the Chroma vector store\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma(collection_name=\"full_documents\", embedding_function=embedding_function) \n",
    "\n",
    "# Load the docs\n",
    "loader = DirectoryLoader('./util', glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Smaller chunks stored in the vector DB\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "chunked_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# Add the documents to vector store\n",
    "vector_store.add_documents(chunked_documents)\n",
    "\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209506c4-7593-4be6-9e77-ed2eaa4a1ca2",
   "metadata": {},
   "source": [
    "## 3. Create the MultiQueryRetriever\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8414d889-09e3-480d-9bba-4d770be552f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create the MQR object\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever,\n",
    "    llm = llm\n",
    ")\n",
    "\n",
    "# To check out the generated queries\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8596b4-efc6-4379-ac52-c8e3c1db4e04",
   "metadata": {},
   "source": [
    "## 4. Checkout the behavior of MQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d956c1dd-492d-4dbe-9b5e-73a536bbfe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : What data is used to train ChatGPT?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: [\"What kinds of data were used to train ChatGPT's language model? \", '', \"Can you provide details on the datasets used to train ChatGPT's AI? \", '', 'Are there any specific data requirements for training large language models like ChatGPT, and if so, what are they?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Title How ChatGPT and our language models are developed\\n\\nSource\\n\\nhttps://help.openai.com/en/articles/7842364\\n\\nhow\\n\\nchatgpt\\n\\nand\\n\\nour\\n\\nlanguage\\n\\nmodels\\n\\nare\\n\\ndeveloped', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='As mentioned in the previous section, ChatGPT does not copy or store training information in a database. Instead, it learns about associations between words, and those learnings help the model update its numbers/weights. The model then uses those weights to predict and generate new words in', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='How does the development of ChatGPT comply with privacy laws? We use training information lawfully. Large language models have many applications that provide significant benefits and are already helping people create content, improve customer service, develop software, customize education, support', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='Abstract OpenAI’s large language models, including the models that power ChatGPT, are developed using three primary sources of information: (1) information that is publicly available on the internet, (2) information that we license from third parties, and (3) information that our users or our human', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='for bespoke systems.', metadata={'source': 'util\\\\guide-to-customizing-LLM.txt'}), Document(page_content='users or our human trainers provide.', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='1 million-token window, prompt engineering is becoming viable for large amounts of data as well..', metadata={'source': 'util\\\\guide-to-customizing-LLM.txt'}), Document(page_content='We respond to objection requests and similar rights. As a result of learning language, ChatGPT responses may sometimes include personal information about individuals whose personal information appears multiple times on the public internet (for example, public figures). Individuals in certain', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='Is personal information used to teach ChatGPT? A large amount of data on the internet relates to people, so our training information does incidentally include personal information. We don’t actively seek out personal information to train our models.', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='What type of information is used to teach ChatGPT? As noted above, ChatGPT and our other services are developed using (1) information that is publicly available on the internet, (2) information that we license from third parties, and (3) information that our users or human trainers provide. This', metadata={'source': 'util\\\\chatgpt-how-it-is-developed.txt'})]\n"
     ]
    }
   ],
   "source": [
    "# Test input\n",
    "input = [\"What is RAG?\",\n",
    "         \"How is fine tuning different than RAG?\",\n",
    "         \"What data is used to train ChatGPT?\",\n",
    "         \"What are the benefits of generative AI?\"]\n",
    "\n",
    "# Change index to select the question\n",
    "ndx = 2\n",
    "\n",
    "print(\"Question :\", input[ndx])\n",
    "\n",
    "results = multi_query_retriever.invoke(input = input[ndx])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a1e51-769d-41a9-96ff-62d220ecc56f",
   "metadata": {},
   "source": [
    "## 5. Create the retriever chain with MQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852a9c45-e4dc-474b-858b-49c0391ab86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Q&A chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "# Create the chain with MQR\n",
    "rag_chain = create_retrieval_chain(multi_query_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66bd48-33e0-4762-810e-ff125e98113d",
   "metadata": {},
   "source": [
    "## 6. Test the chain's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dead1ffa-0fcf-42e3-bcc9-95f455820880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['What information can be retrieved about the concept of \"retrieval\" within a specific context?', '', 'What are the relevant documents that discuss retrieval in a particular setting or scenario?', '', 'Are there any studies or reports that examine the process, methods, or applications of retrieval in a defined context?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAG, or Retrieval Augmented Generation, is a process where AI searches for relevant information to answer a query, using a source of data, rather than just relying on its training.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vague question\n",
    "input = \"retrieval for context\"\n",
    "\n",
    "response = rag_chain.invoke({\"input\": input})\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f02937-231c-456c-b4ad-4b386150ede9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf81a8b-0b3d-43bb-9495-3d8922052f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8003c7c3-657c-4008-981b-54b28daabfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04768fd8-c4c0-4df2-a1be-461ebc2c7ddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
