{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35ab6920-418a-473b-a239-4e7188426375",
   "metadata": {},
   "source": [
    "# Contextual Compression\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/contextual_compression/\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html\n",
    "\n",
    "Community compressors\n",
    "\n",
    "https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.document_compressors\n",
    "\n",
    "\n",
    "* Using ChromaDB as base retriever\n",
    "* Using Cohere command as LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a010c350-3a35-4f8e-b5ed-19b79b6f2f02",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb3a19d-7f5e-43fb-9b79-1bfc5c2f2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "\n",
    "import warnings \n",
    "# Settings the warnings to be ignored \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b89adb-70a5-45e3-a692-c022fae81ea5",
   "metadata": {},
   "source": [
    "## 1. Create an LLM\n",
    "The LLM will be used by the compression strategy classes\n",
    "\n",
    "* Cohere command model\n",
    "* Cohere embedding model\n",
    "\n",
    "#### Note\n",
    "* You must adjust the location of the API key file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5919d8a9-8af8-41a6-ae29-341a35ae3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "llm = create_cohere_chat_llm()\n",
    "\n",
    "llm_embeddings = CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2257fd7-b727-4a10-8784-69e24c578c13",
   "metadata": {},
   "source": [
    "## 2. Utility function\n",
    "\n",
    "* Pretty prints the documents before/after compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74a3c50-93eb-45cd-af80-de51bd48e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_documents(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(\"#\",i)\n",
    "        print(doc.page_content)\n",
    "\n",
    "def  dump_before_after_compression(base_retriever, compressor, question) :  #(: #bef, aft):\n",
    "    results_before = base_retriever.invoke(question)\n",
    "    results_after = compressor.invoke(question)\n",
    "    \n",
    "    print(\"BEFORE. Doc count = \", len(results_before))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print_documents(results_before)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"AFTER. Doc count = \", len(results_after))\n",
    "    print_documents(results_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6127200-7818-4489-940d-da62a639df28",
   "metadata": {},
   "source": [
    "## 3. Setup base retriever\n",
    "\n",
    "* using ChromaDB as a base retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89609cb6-ba57-4f1e-aa86-171ae4666ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Chroma vector store\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma(collection_name=\"full_documents\", embedding_function=embedding_function) \n",
    "\n",
    "# Load sample docs\n",
    "loader = DirectoryLoader('./util', glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunking\n",
    "doc_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "chunked_documents = doc_splitter.split_documents(docs)\n",
    "\n",
    "# Add to vector DB\n",
    "vector_store.add_documents(chunked_documents)\n",
    "\n",
    "# Base retrievers\n",
    "vector_store_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d92d8-d207-4145-9256-2c63ef32c352",
   "metadata": {},
   "source": [
    "## 4. LLMChainExtractor\n",
    "\n",
    "Uses an LLM to extract relevant parts of a document.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_extract.LLMChainExtractor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "743a2bb8-497f-4a2a-a23f-2ffc589ca7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compressor\n",
    "llm_chain_extractor_compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Create the retriever\n",
    "llm_chain_extractor_compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=vector_store_retriever, \n",
    "    base_compressor=llm_chain_extractor_compressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f892b4-5b76-44cd-afd7-990166ddabb5",
   "metadata": {},
   "source": [
    "### Test\n",
    "* Apply compression to retrieved results\r\n",
    "* Print the before/after results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a89db60b-0d0d-4500-a005-4ad852a3e313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE. Doc count =  4\n",
      "--------------------------------------------------\n",
      "# 0\n",
      "Retrieval augmented generation (RAG)\n",
      "\n",
      "Retrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model’s training knowledge, AI apps architected for RAG can search your data for information relevant to a query, then pass that information into the prompt. This is similar to prompt engineering, except that the system can find and retrieve new context from your data with each interaction.\n",
      "# 1\n",
      "The RAG approach supports fresh data that’s constantly updated, private data that you connect, large-scale and multimodal data, and more — and it’s supported by an increasingly robust ecosystem of products, from simple integrations with databases to embedding APIs and other components for bespoke systems.\n",
      "\n",
      "Supervised fine\n",
      "\n",
      "tuning (SFT)\n",
      "# 2\n",
      "Which one to go for?\n",
      "\n",
      "The first question to consider is: do you need the model to always give a citation of a source grounded in your data? If yes, you will need to use RAG. RAG also has the benefit that you can control who has access to which grounding data (based on who’s calling the model). That will help you combat hallucinations and enhance interpretability of the results.\n",
      "# 3\n",
      "What is ChatGPT, and how does it work? ChatGPT is an artificial intelligence-based service that you can access via the internet. You can use ChatGPT to organize or summarize text, or to write new text. ChatGPT has been developed in a way that allows it to understand and respond to user questions and instructions. It does this by “reading” a large amount of existing text and learning how words tend to appear in context with other words. It then uses what it has learned to predict the next most\n",
      "--------------------------------------------------\n",
      "AFTER. Doc count =  3\n",
      "# 0\n",
      "Retrieval augmented generation (RAG)\n",
      "\n",
      "Retrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model’s training knowledge, AI apps architected for RAG can search your data for information relevant to a query, then pass that information into the prompt.\n",
      "# 1\n",
      "The RAG approach supports fresh data that’s constantly updated, private data that you connect, large-scale and multimodal data, and more — and it’s supported by an increasingly robust ecosystem of products, from simple integrations with databases to embedding APIs and other components for bespoke systems.\n",
      "# 2\n",
      "The first question to consider is: do you need the model to always give a citation of a source grounded in your data? If yes, you will need to use RAG. RAG also has the benefit that you can control who has access to which grounding data (based on who’s calling the model). That will help you combat hallucinations and enhance interpretability of the results.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is rag?\"\n",
    "\n",
    "dump_before_after_compression(vector_store_retriever, llm_chain_extractor_compressor_retriever, question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eea23b-77a2-485b-9630-43776dd63809",
   "metadata": {},
   "source": [
    "## 5. LLM Chain Filter\n",
    "\n",
    "Drops documents  that are not relevant for the query.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.chain_filter.LLMChainFilter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e516b-f381-42de-aa5c-cc8121c7b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compressor\n",
    "llm_chain_filter_compressor = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "# Create the retriever\n",
    "llm_chain_filter_compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=vector_store_retriever, \n",
    "    base_compressor=llm_chain_filter_compressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b0eae8-cee5-4946-84a4-710505908282",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is rag?\"\n",
    "\n",
    "dump_before_after_compression(vector_store_retriever, llm_chain_filter_compressor_retriever, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdeb66-b8fe-40a5-a1b1-e2cc9dcde36e",
   "metadata": {},
   "source": [
    "## 6. Embeddings Filter\n",
    "\n",
    "Uses embeddings to drop documents unrelated to the query.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.embeddings_filter.EmbeddingsFilter.html\n",
    "\n",
    "Making an extra LLM call over each retrieved document is expensive and slow. The EmbeddingsFilter provides a cheaper and faster option by embedding the documents and query and only returning those documents which have sufficiently similar embeddings to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28264c0-32c6-4264-9f6c-81d2734007c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compressor\n",
    "# Play with the threshold to understand the behavior\n",
    "similarity_threshold = 0.5\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=llm_embeddings, similarity_threshold=similarity_threshold)\n",
    "\n",
    "# Create the retriever\n",
    "llm_embeddings_filter_compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=vector_store_retriever, \n",
    "    base_compressor=embeddings_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d016f2-c588-42f4-ba3d-960c16b2a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is rag?\"\n",
    "\n",
    "dump_before_after_compression(vector_store_retriever, llm_embeddings_filter_compressor_retriever, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30817100-2421-4b5b-8dff-9e648728aa85",
   "metadata": {},
   "source": [
    "## 7. Compressor pipeline\n",
    "\n",
    "Document compressor that uses a pipeline of Transformers.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.base.DocumentCompressorPipeline.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6903b67-8aca-46a2-8c87-9d148bec0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [llm_chain_filter_compressor, embeddings_filter]\n",
    "pipeline_compressor = DocumentCompressorPipeline(transformers=transformers)\n",
    "# Create the retriever\n",
    "pipeline_compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=vector_store_retriever, \n",
    "    base_compressor=pipeline_compressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f865d-a9c1-4b1b-a090-3404fd15887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is rag?\"\n",
    "\n",
    "dump_before_after_compression(vector_store_retriever, pipeline_compressor_retriever, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6638eee2-f4b2-43a9-8557-5b0389e72bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
