{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbad3768-9963-462b-ace5-dc643fe0fe40",
   "metadata": {},
   "source": [
    "# Setup & Test API Key\n",
    "\n",
    "This notebook is used for testing the API keys setup for model/service providers.\n",
    "\n",
    "### Test a specific provider's key\n",
    "1. Run the cell : **Setup the environment**\n",
    "2. Run the cells for the provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d890e1-3b66-43c9-8144-b4107ebe20fd",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aa554-5228-4cf1-a0c4-94ac71013943",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a0183a-7f18-4197-8aaa-0e966f898031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# CHANGE THIS TO POINT TO YOUR OWN FILE\n",
    "# Load the file that contains the API keys \n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a92ed4-e478-4fa0-b5a9-a06ace7f0c65",
   "metadata": {},
   "source": [
    "## Google API key\n",
    "\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38369ec-0779-4548-bd5c-f9a4dac42d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_llm import create_google_llm\n",
    "\n",
    "# Try out the Gemini Flash model\n",
    "# model=\"gemini-1.5-flash\" - created by default\n",
    "llm = create_google_llm()\n",
    "\n",
    "# Uncomment to create the pro model\n",
    "# llm=create_google_llm(model=\"gemini-1.5-pro\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5914d2-3621-40f4-b9bf-47b01debdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in 5 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed22230-4477-475b-b21b-836fa12cd6fa",
   "metadata": {},
   "source": [
    "## HuggingFace token\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "#### Note\n",
    "Not all models are available for as endpoints. Check model's inference endpoint state to check if model endpoint is active:\n",
    "\n",
    "https://huggingface.co/models?inference=warm&other=endpoints_compatible\n",
    "\n",
    "**Warm**   Model inference endpoint is active\n",
    "\n",
    "**Cold**   Model will be loaded on request. Large models sometime timeout but, they become available after a couple of retries :)\n",
    "\n",
    "**Freezed** Model inference endpoint NO MORE available as there was not much interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece41d2c-53f0-4a03-ae82-7338908a1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\raj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_hugging_face_llm\n",
    "\n",
    "# Deafult HuggingFace model in use : mistralai/Mistral-7B-Instruct-v0.2\n",
    "# GATED MODEL !!! \n",
    "# Was your access request approved?  Check: https://huggingface.co/settings/gated-repos\n",
    "#\n",
    "# model= \"mistralai/Mistral-7B-Instruct-v0.2\" created by default\n",
    "llm = create_hugging_face_llm()\n",
    "\n",
    "# Small model - not as performant as Mistral\n",
    "# Select model by comment/uncomment\n",
    "# model='openai-community/gpt2-large'\n",
    "# model='microsoft/Phi-3-mini-4k-instruct'\n",
    "# llm=create_hugging_face_llm(repo_id=model)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26483ba3-dd2d-48a2-82c8-bb271b170abc",
   "metadata": {},
   "source": [
    "### Note\n",
    "Sometime HuggingFace models time out :( It happens as a result model being in cold state. Two ways to address:\n",
    "\n",
    "1. Just retry and it should work\n",
    "2. Use a different model (usually smaller models are more available)\n",
    "\n",
    "HfHubHTTPError: 504 Server Error: Gateway Timeout for url: https:...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5c2080-bc77-4ff0-842e-1b7db2033813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM, or Latent Linear Model, is a generative AI model that uses a lower-dimensional latent space to represent and generate complex data distributions, such as images or speech. It learns the underlying structure of the data by mapping it to a simpler, more interpretable representation in the latent space.\n",
      "\n",
      "By training an LLM on large datasets, we can generate new data samples that are similar to the training data but also exhibit novel and creative characteristics, as the model can explore the latent space beyond the boundaries of the training data.\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f0c27-a584-4b4a-9115-7098f7db911c",
   "metadata": {},
   "source": [
    "## Cohere API Key\n",
    "\n",
    "COHERE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12d716-9c40-47bb-b330-a99dd75983c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_llm import create_chere_llm\n",
    "\n",
    "# Try out the Gemini Flash model\n",
    "# model= \"mistralai/Mistral-7B-Instruct-v0.2\" created by default\n",
    "llm = create_hugging_face_llm()\n",
    "\n",
    "# Uncomment to create the pro model\n",
    "# Small model - not as performant as Mistral\n",
    "model='openai-community/gpt2-large'\n",
    "llm=create_hugging_face_llm(repo_id=model)\n",
    "\n",
    "llm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
