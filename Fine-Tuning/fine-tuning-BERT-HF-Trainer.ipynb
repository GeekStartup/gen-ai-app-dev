{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b719d460-3a4a-4c8d-8b7c-10275dbd8465",
   "metadata": {},
   "source": [
    "# Transformers trainer class\n",
    "https://huggingface.co/docs/transformers/en/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cb9609-82b4-4d91-97ad-9b32aceb9aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.2.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.47.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.3.1.post300)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.15.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a5fd2-e48d-452e-9d98-1c5b9be89667",
   "metadata": {},
   "source": [
    "## 1. Use GPU/CUDA for fine-tuning\n",
    "\n",
    "All frameworks provide support for running training and fine-tuning on GPU/CUDA devices.\n",
    "\n",
    "```\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model     = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "inputs    = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "model     = model.to(device)\n",
    "outputs   = model(**inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194066cb-2c88-4c27-8df9-18cbe2c66a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Identify the device available for training\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212283d-fcfe-47aa-8cfc-4173b90024c4",
   "metadata": {},
   "source": [
    "## 2. Define simple eval function & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9055770-62a9-4a7e-b37b-500452df2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-30 21:14:27.112162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-30 21:14:27.127927: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-30 21:14:27.132887: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-30 21:14:27.144476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label = 0 :  Negative      Label = 1 : Positive\n",
      "Text: This movie was fantastic! I really enjoyed it.\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: I didn't like this product. It broke after one use.\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: The book was okay, not great but not terrible either.\n",
      "Predicted Label: 1\n",
      "\n",
      "Text: it is all fun\n",
      "Predicted Label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Run test with base model\n",
    "MODEL_ID=\"google-bert/bert-base-cased\"\n",
    "\n",
    "# Create the instance of the model - ignore the warning\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2, torch_dtype=\"auto\").to(device)\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Define the inference function\n",
    "def predict_label_with_given_model(use_model, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Predict the label for a given text using the fine-tuned model from the trainer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for inference.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted label.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # PyTorch tensors\n",
    "        truncation=True,      # Truncate if the text is too long\n",
    "        padding=True          # Pad to the max length in the batch\n",
    "    ).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No gradient calculation during inference\n",
    "        outputs = use_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()  # Get the label with the highest score\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Testing the function\n",
    "def run_simple_eval(use_model):\n",
    "    # Example texts for testing\n",
    "    test_texts = [\n",
    "        \"This movie was fantastic! I really enjoyed it.\",\n",
    "        \"I didn't like this product. It broke after one use.\",\n",
    "        \"The book was okay, not great but not terrible either.\",\n",
    "        \"it is all fun\"\n",
    "    ]\n",
    "\n",
    "    # Run inference and print results\n",
    "    print(\"Label = 0 :  Negative      Label = 1 : Positive\")\n",
    "    for text in test_texts:\n",
    "        label = predict_label_with_given_model(use_model,text)\n",
    "        print(f\"Text: {text}\\nPredicted Label: {label}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Run evaluation on the base model\n",
    "run_simple_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a2d79-6af2-408b-9d74-32d92003b062",
   "metadata": {},
   "source": [
    "## 3. Load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8dd5339-9484-4e32-bd70-1b7b46765bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in 'Train' :  25000\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(\"Number of records in 'Train' : \",len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110945c0-b650-4fd6-befa-457ed1456ea7",
   "metadata": {},
   "source": [
    "## 4. (optional) Reduce the size of the training & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00ebb3aa-a87f-45b5-ac4b-b4bdc34d32a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust this to change the number of train and test rows\n",
    "TOTAL_NUMBER_RECORDS = 5000\n",
    "\n",
    "# Reduce the size of the dataset\n",
    "reduced_dataset=dataset['train'].shuffle(seed=42).select(range(TOTAL_NUMBER_RECORDS))\n",
    "\n",
    "# Split the dataset into train and test\n",
    "reduced_dataset = reduced_dataset.train_test_split(train_size=0.9)\n",
    "\n",
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8568fd0-da0d-46be-bf4c-03fe563f2223",
   "metadata": {},
   "source": [
    "## 5. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1bcc0c-60e4-4514-8a41-123a354a5ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e0e35d3c314e15a2ce621115564184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64bbb9fa80fb447c88829bdab5472d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Tokenize function used by map\n",
    "# Move the tokenized data to device (GPU)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "\n",
    "# Tokenize using map\n",
    "tokenized_datasets = reduced_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Uncommenting this line will lead to an error as the trainer expects dataset to have a column with the name 'label'\n",
    "# ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask\n",
    "# tokenized_datasets = tokenized_datasets.rename_column('label','type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dd98a-4530-4e1b-a996-21adb8cd34c9",
   "metadata": {},
   "source": [
    "## 6. Setup TrainingArguments & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbd13d6-10c9-4174-91bd-44dddb6d6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Output directory for the training\n",
    "output_dir = \"./temp\"\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    # Directory for logs and outputs from trainer\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "\n",
    "    # Hyperparameters\n",
    "    eval_strategy = \"epoch\",                # Can be set to \"steps\" but will lead to longer training times\n",
    "    per_device_train_batch_size=16,         # Default = 8. Adjust based on the VRAM/GPU. High value can lead to OOM errors\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,                     # Higher epoch means longer training time\n",
    "    # max_steps=100,                        # Change this for forced stopping - good for experimentation\n",
    "    learning_rate=0.00005,                  # The initial learning rate for AdamW optimizer\n",
    "    warmup_steps=0,\n",
    "    warmup_ratio=0,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",                    # Using the default optimizer\n",
    "\n",
    "    # Reporting & Logging - refer documentation for more args\n",
    "    report_to='none',                       # \"azure_ml\", \"clearml\", \"codecarbon\", \"comet_ml\", \"dagshub\", \"dvclive\", \"flyte\", \"mlflow\", \"neptune\", \"tensorboard\", and \"wandb\"\n",
    "    logging_strategy=\"steps\",\n",
    "    # logging_steps=50,                       # default = 500, Number of update steps between two logs if logging_strategy=\"steps\". Otherwise a number between 0 & 1, interprtted as a ratio of train steps\n",
    "\n",
    "    # Reports to - integration with MLOPS & ML experiments tooling\n",
    "    run_name=\"Fine-Tune-BERT-IMDB\",\n",
    "    \n",
    "\n",
    "    # Save strategy - refer documentation for more args\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    # Hub strategy -  - refer documentation for more args\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# Setup traininer with minimilitic st of arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe43d5-f0ae-4b92-96fd-f1ab91723f2a",
   "metadata": {},
   "source": [
    "## 7. Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cc890-f839-4d55-a1aa-10a96a185e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/564 04:32 < 02:50, 1.27 it/s, Epoch 1.23/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.223525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f80bf-5e0e-4f7e-a662-4200cb972902",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = trainer.model\n",
    "\n",
    "# Run evaluation on the base model\n",
    "run_simple_eval(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9aa0e9-30b1-408c-bfd2-b81059e14d52",
   "metadata": {},
   "source": [
    "## 8. Save the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375f226-2562-4edf-9d3d-2935ab2a445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = output_dir + \"/fine-tuned-BERT-IMDB\"\n",
    "\n",
    "trainer.save_model(model_folder)\n",
    "tokenizer.save_pretrained(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c20d4-1429-4f82-a43d-4d83226086f2",
   "metadata": {},
   "source": [
    "## 9. Load fine-tuned model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991fc4f-0096-4e9d-abb0-d4e0593dbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "print(\"Provide the HuggingFace API token:\")\n",
    "\n",
    "hf_token = getpass.getpass()\n",
    "\n",
    "hub_model_id = \"acloudfan/fine-tuned-BERT-IMDB\"\n",
    "\n",
    "fine_tuned_model.push_to_hub(hub_model_id, hf_token=hf_token)\n",
    "tokenizer.push_to_hub(hub_model_id, hf_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb8291-f840-4f3f-a527-3b82c7ae4cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
