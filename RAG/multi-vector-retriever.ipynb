{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b96abf-2f0d-469c-83ac-a9301828c62d",
   "metadata": {},
   "source": [
    "# Multi Vector Retriever\n",
    "https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/multi_vector/\n",
    "\n",
    "Learn to use LangChain *Multi Vector Retriever* class for **Hypothetical Questions** embeddings strategy.\n",
    "\n",
    "\r",
    "* You must adjust the location of the key file in code.\n",
    "* Sample uses GPT 3.5 but you may use a different LLM - keep in mind that code/prompt adjustments may be needed in case different LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cfa553-d84a-445d-96c9-b865b04bed1b",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9a0308-636f-4e7a-805d-274ccb67de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486330a-d037-4f1d-ab83-0add9a9ca2bd",
   "metadata": {},
   "source": [
    "## 1. Create an LLM\n",
    "\n",
    "Note:\n",
    "\n",
    "* You MUST adjust the location of key file in the code\n",
    "* Sample uses GPT 3.5 which will cost you depending on the usage. You may use a different LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e55dbe72-a1ef-44eb-b04e-1ae6e633be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "llm = create_gpt_chat_llm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ec2717-5c6b-47db-b483-6f6d1dee02b3",
   "metadata": {},
   "source": [
    "## 2. Setup prompt for generating questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d484f9-2832-47a5-815e-ea2cc39c3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "         The provided document will be used as a context by a chatbot application.\n",
    "         You need to come up with a list 5 to 10 hypothetical questions that can be answered from this document\n",
    "         Output the questions as a valid JSON array.\n",
    "\n",
    "         Document:\n",
    "         {document}\n",
    "\n",
    "         Questions: [JSON array of questions]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b133f7f-4067-4e9a-bd08-77ce58d4c905",
   "metadata": {},
   "source": [
    "## 3. Setup a chain\n",
    "\n",
    "Output is in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3564db79-4ccd-4d11-b5e9-2d64a82be0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Set up a 3 step chain\n",
    "qa_chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d6772-e157-41b5-8e1b-0166025eeaea",
   "metadata": {},
   "source": [
    "## 4. Generate questions & create documents\n",
    "\n",
    "* This may be done for chunks or entire document. Decision depends on the size of the document. To keep things simple we will do it for the entire document.\n",
    "\n",
    "* Sample documents are read from a subfolder\n",
    "* Questions are generated using the chain\n",
    "* A LangChain *Document* is created with the question as the page content\n",
    "* The document instances are added to a collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922678f6-0466-42af-b6b3-8df7d7fee574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample docs\n",
    "loader = DirectoryLoader('./util', glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "questions_doc_collection = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    input = prompt_template.format(document=doc.page_content)\n",
    "    questions = qa_chain.invoke(input)\n",
    "\n",
    "    # Ad doc_id to metadata\n",
    "    metadata = doc.metadata\n",
    "    metadata['doc-id'] = \"doc-\" + str(i)\n",
    "\n",
    "    # Create documents from questions\n",
    "    for question in questions:\n",
    "        document = Document(page_content=question, metadata=metadata)\n",
    "        questions_doc_collection.append(document)\n",
    "\n",
    "# questions_doc_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28153dad-6255-48a0-8dea-8f4b5cb15e47",
   "metadata": {},
   "source": [
    "## 5. Create ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9f7a29-63ce-4338-b715-eeb30af0b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of ChromaDB and add the documents\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store = Chroma(collection_name=\"full_documents\", embedding_function=embedding_function) \n",
    "vector_store.add_documents(questions_doc_collection)\n",
    "\n",
    "vector_store_retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a2c14f0-c330-46ba-98bd-18a747254857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the retriever\n",
    "# print(vector_store_retriever.invoke(\"what is RAG\"))\n",
    "# print(vector_store_retriever.invoke(\"what is fine tuning\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b108c-f39b-4927-bcdb-a3e8f571709c",
   "metadata": {},
   "source": [
    "## 6. Create MultiVectorRetriever\n",
    "\n",
    "Uses the File System store that manages the documents on the filesystem. Requires the files to have an \"id\" that correlates the document to the question. \n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.multi_vector.MultiVectorRetriever.html\n",
    "\n",
    "https://api.python.langchain.com/en/latest/storage/langchain.storage.file_system.LocalFileStore.html#langchain.storage.file_system.LocalFileStore.mset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654ff296-277b-4367-9d3c-3a257d65d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "file_store = LocalFileStore(\"./temp\")\n",
    "\n",
    "# Loop through the docs and add to file store\n",
    "# This may be moved to earlier cell for performance but kept here for clarity\n",
    "for doc in docs:\n",
    "    file_store.mset([(doc.metadata['doc-id'],doc.page_content.encode())])\n",
    "\n",
    "# Create the multi vector class instance\n",
    "mvr_retriever = MultiVectorRetriever(\n",
    "    vectorstore=vector_store,\n",
    "    docstore=file_store,\n",
    "    id_key=\"doc-id\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67866251-aa59-4bf1-9a6d-f5ea67ea7063",
   "metadata": {},
   "source": [
    "## 7. Test\n",
    "\n",
    "* Test directly against the vector store\r\n",
    "* Test the vector store retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7555f9d4-925f-415f-85bd-3d153f79cfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct query against the vector store\n",
      "-------------------------------------\n",
      "[Document(page_content='When should one consider using RAG over prompt engineering?', metadata={'doc-id': 'doc-1', 'source': 'util\\\\guide-to-customizing-LLM.txt'}), Document(page_content='What is the difference between prompt engineering and retrieval augmented generation (RAG)?', metadata={'doc-id': 'doc-1', 'source': 'util\\\\guide-to-customizing-LLM.txt'}), Document(page_content='How does ChatGPT use the associations between words to update its numbers/weights?', metadata={'doc-id': 'doc-0', 'source': 'util\\\\chatgpt-how-it-is-developed.txt'}), Document(page_content='In what ways does ChatGPT comply with privacy laws in its development?', metadata={'doc-id': 'doc-0', 'source': 'util\\\\chatgpt-how-it-is-developed.txt'})]\n",
      "\n",
      "Query against the MVR. Number of docs =  2\n",
      "-----------------------------------------------\n",
      "[b\"Blog Source\\n\\nhttps://cloud.google.com/blog/products/ai\\n\\nmachine\\n\\nlearning/to\\n\\ntune\\n\\nor\\n\\nnot\\n\\nto\\n\\ntune\\n\\na\\n\\nguide\\n\\nto\\n\\nleveraging\\n\\nyour\\n\\ndata\\n\\nwith\\n\\nllms\\n\\nBlog Title To tune or not to tune? A guide to leveraging your data with LLMs\\n\\nMethods to bring your data to foundation models Before we get started visualizing a generative AI application, we need to understand the ways in which LLMs and other foundation models can interact with your data.\\n\\nPrompt engineering\\n\\nThe easiest way to facilitate interactions between a model and your data is to include the data in the instructions, or system prompt, sent to the model. In this approach, the model doesn\\xe2\\x80\\x99t need to be changed or adapted \\xe2\\x80\\x94 which is very powerful and attractive, but also potentially limiting for some use cases. For example, whereas static information could be easily added to a system prompt and used to steer interactions, the same isn\\xe2\\x80\\x99t true for information that is frequently updated, such as sports scores or airline prices.\\n\\nRetrieval augmented generation (RAG)\\n\\nRetrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model\\xe2\\x80\\x99s training knowledge, AI apps architected for RAG can search your data for information relevant to a query, then pass that information into the prompt. This is similar to prompt engineering, except that the system can find and retrieve new context from your data with each interaction.\\n\\nThe RAG approach supports fresh data that\\xe2\\x80\\x99s constantly updated, private data that you connect, large-scale and multimodal data, and more \\xe2\\x80\\x94 and it\\xe2\\x80\\x99s supported by an increasingly robust ecosystem of products, from simple integrations with databases to embedding APIs and other components for bespoke systems.\\n\\nSupervised fine\\n\\ntuning (SFT)\\n\\nIf you want to give a model specific instructions for a well-defined task, you might consider SFT, also often referred to as Parameter Efficient Fine Tuning (PEFT). This can work well for tasks like classification, or creating structured outputs from free text.\\n\\nTo perform supervised fine tuning, you need to have the model learn from input-output pairs that you provide the model. For example, if you want to classify meeting transcripts into different categories (for example, \\xe2\\x80\\x9cmarketing,\\xe2\\x80\\x9d \\xe2\\x80\\x9clegal.\\xe2\\x80\\x9d \\xe2\\x80\\x9ccustomer-support\\xe2\\x80\\x9d), then you\\xe2\\x80\\x99ll need to provide the supervised tuning process multiple transcripts along with the categories of the meetings. The tuning process will learn what you consider to be the right classification for your meetings.\\n\\nReinforcement Learning from Human Feedback (RLHF)\\n\\nWhat if your goal is not well described into categories or isn\\xe2\\x80\\x99t easy to quantify? For example, suppose you want a model to have a certain tone (perhaps to have a brand voice, or to be formal in specific ways). Reinforcement Learning from Human Feedback, or RLHF, is a technique that creates a model that is tuned to your specific needs, enhanced by a human\\xe2\\x80\\x99s preferences.\\n\\nThe algorithm looks like the following in a nutshell: You provide your data in the form of input prompts and output responses, but the output responses have to be in pairs: two plausible responses but one that you prefer over the other. For example, one might be correct but generic while the other is both correct and uses a language style that you want in your outputs.\\n\\nDistillation\\n\\nThe clever technique of distillation combines two goals: creating a smaller model that is faster and quicker to process data, and making that model more specific to your tasks. It works by using a larger foundation model to \\xe2\\x80\\x9cteach\\xe2\\x80\\x9d a smaller model, and by focusing that teaching on your data and task. For example, imagine you want to double-check all your emails to make them more formal\\xe2\\x80\\x94and you want to do this with a smaller model. To achieve this, you give the input (the original text and the instruction \\xe2\\x80\\x9cmake this email more formal\\xe2\\x80\\x9d) to the large model, and it gives you the output (the re-written email). Now, equipped with your inputs and the large model outputs, you can train a small specialized model to learn to reproduce this specific task. In addition to the foundation models input/output pairs, you can provide your own as well.\\n\\nWhich one to go for?\\n\\nThe first question to consider is: do you need the model to always give a citation of a source grounded in your data? If yes, you will need to use RAG. RAG also has the benefit that you can control who has access to which grounding data (based on who\\xe2\\x80\\x99s calling the model). That will help you combat hallucinations and enhance interpretability of the results.\\n\\nIf you don\\xe2\\x80\\x99t have those requirements, you\\xe2\\x80\\x99ll need to decide if prompt engineering is enough, or if you need to tune the model. If you have only a small amount of data, then prompt engineering may suffice \\xe2\\x80\\x94 and with context windows continuing to grow, as evidenced by Gemini 1.5\\xe2\\x80\\x99s 1 million-token window, prompt engineering is becoming viable for large amounts of data as well..\\n\\nIf you choose tuning, then you\\xe2\\x80\\x99ll need to consider tuning options in accordance with how specific and difficult to quantify your desired model\\xe2\\x80\\x99s behavior is. In cases where your desired model produces an output that's difficult to describe, human involvement is probably needed, so RLHF is the way to go. Otherwise, different tuning methods could be chosen considering how much personalization you need for your model, your budget, and how fast serving speed you require.\\n\\nThis decision tree is a simplified version of the reasoning we describe:\\n\\nhttps://storage.googleapis.com/gweb\\n\\ncloudblog\\n\\npublish/images/Screenshot_2024\\n\\n05\\n\\n16_at_5.37.34PM.max\\n\\n1200x1200.png\\n\\nWhy not combine methods? You may ask: why can\\xe2\\x80\\x99t I use more methods? For example: I want to fine-tune my model to have my brand voice and I also want it to use only my data to generate answers (RAG). That is also possible and often the best option! You can tune a model and then use it to perform another task. Or you can also tune an LLM and then perform in-context prompt engineering on it to make sure the model behaves as desired. To sum up, you can freely combine the aforementioned methods at your convenience.\", b'Title How ChatGPT and our language models are developed\\n\\nSource\\n\\nhttps://help.openai.com/en/articles/7842364\\n\\nhow\\n\\nchatgpt\\n\\nand\\n\\nour\\n\\nlanguage\\n\\nmodels\\n\\nare\\n\\ndeveloped\\n\\nAbstract OpenAI\\xe2\\x80\\x99s large language models, including the models that power ChatGPT, are developed using three primary sources of information: (1) information that is publicly available on the internet, (2) information that we license from third parties, and (3) information that our users or our human trainers provide.\\n\\nThis article provides an overview of the publicly available information we use to help develop our models and how we collect and use that information in compliance with privacy laws. To understand how we collect and use information from users of our services, including how to opt out of having ChatGPT conversations used to help teach our models, please see our Privacy Policy and this help center article.\\n\\nWhat is ChatGPT, and how does it work? ChatGPT is an artificial intelligence-based service that you can access via the internet. You can use ChatGPT to organize or summarize text, or to write new text. ChatGPT has been developed in a way that allows it to understand and respond to user questions and instructions. It does this by \\xe2\\x80\\x9creading\\xe2\\x80\\x9d a large amount of existing text and learning how words tend to appear in context with other words. It then uses what it has learned to predict the next most likely word that might appear in response to a user request, and each subsequent word after that. This is similar to auto-complete capabilities on search engines, smartphones, and email programs.\\n\\nAs an example, during the model learning process (called \\xe2\\x80\\x9ctraining\\xe2\\x80\\x9d), we might have a model try to complete the sentence: \\xe2\\x80\\x9cinstead of turning left, she turned ___.\\xe2\\x80\\x9d Before training, the model will respond with random words, but as it reads and learns from many lines of text, it better understands this type of sentence and can predict the next word more accurately. It then repeats this process across a very large number of sentences.\\n\\nBecause there are many possible words that could come next in this sentence (e.g., instead of turning left, she turned \\xe2\\x80\\x9cright,\\xe2\\x80\\x9d \\xe2\\x80\\x9caround,\\xe2\\x80\\x9d or \\xe2\\x80\\x9cback\\xe2\\x80\\x9d), there is an element of randomness in the way a model can respond, and in many cases our models will answer the same question in different ways.\\n\\nMachine learning models are made up of large strings of numbers, called \\xe2\\x80\\x9cweights\\xe2\\x80\\x9d or \\xe2\\x80\\x9cparameters,\\xe2\\x80\\x9d and code that interprets and executes those numbers. Models do not contain or store copies of information that they learn from. Instead, as a model learns, some of the numbers that make up the model change slightly to reflect what it has learned. In the example above, the model read information that helped it improve from predicting random incorrect words to predicting more accurate words, but all that actually happened in the model itself was that the numbers changed slightly. The model did not store or copy the sentences that it read.\\n\\nWhat type of information is used to teach ChatGPT? As noted above, ChatGPT and our other services are developed using (1) information that is publicly available on the internet, (2) information that we license from third parties, and (3) information that our users or human trainers provide. This article focuses on the first set: information that is publicly available on the internet.\\n\\nFor this set of information, we only use publicly available information that is freely and openly available on the Internet \\xe2\\x80\\x93 for example, we do not seek information behind paywalls or from the \\xe2\\x80\\x9cdark web.\\xe2\\x80\\x9d We apply filters and remove information that we do not want our models to learn from or output, such as hate speech, adult content, sites that primarily aggregate personal information, and spam. We then use the information to teach our models.\\n\\nAs mentioned in the previous section, ChatGPT does not copy or store training information in a database. Instead, it learns about associations between words, and those learnings help the model update its numbers/weights. The model then uses those weights to predict and generate new words in response to a user request. It does not \\xe2\\x80\\x9ccopy and paste\\xe2\\x80\\x9d training information \\xe2\\x80\\x93 much like a person who has read a book and sets it down, our models do not have access to training information after they have learned from it.\\n\\nIs personal information used to teach ChatGPT? A large amount of data on the internet relates to people, so our training information does incidentally include personal information. We don\\xe2\\x80\\x99t actively seek out personal information to train our models.\\n\\nWe use training information only to help our models learn about language and how to understand and respond to it. We do not and will not use any personal information in training information to build profiles about people, to contact them, to advertise to them, to try to sell them anything, or to sell the information itself.\\n\\nOur models may learn from personal information to understand how things like names and addresses fit within language and sentences, or to learn about famous people and public figures. This makes our models better at providing relevant responses.\\n\\nWe also take steps to reduce the processing of personal information when training our models. For example, we remove websites that aggregate large volumes of personal information and we try to train our models to reject requests for private or sensitive information about people.\\n\\nHow does the development of ChatGPT comply with privacy laws? We use training information lawfully. Large language models have many applications that provide significant benefits and are already helping people create content, improve customer service, develop software, customize education, support scientific research, and much more. These benefits cannot be realized without a large amount of information to teach the models. In addition, our use of training information is not meant to negatively impact individuals, and the primary sources of this training information are already publicly available. For these reasons, we base our collection and use of personal information that is included in training information on legitimate interests under privacy laws like the GDPR, as explained in more detail in our Privacy Policy. We have also completed a data protection impact assessment to help ensure we are collecting and using this information legally and responsibly.\\n\\nWe respond to objection requests and similar rights. As a result of learning language, ChatGPT responses may sometimes include personal information about individuals whose personal information appears multiple times on the public internet (for example, public figures). Individuals in certain jurisdictions can object to the processing of their personal information by our models in our Privacy Portal. Individuals also may have the right to access, correct, restrict, delete, or transfer their personal information that may be included in our training information. You can exercise these rights by reaching out to dsar@openai.com.\\n\\nPlease be aware that, in accordance with privacy laws, some rights may not be absolute. We may decline a request if we have a lawful reason for doing so. However, we strive to prioritize the protection of personal information, and comply with all applicable privacy laws. If you feel we have not adequately addressed an issue, you have the right to lodge a complaint with your local supervisory authority.']\n"
     ]
    }
   ],
   "source": [
    "# Test question = input\n",
    "question = \"what is gpt and rag\"\n",
    "\n",
    "# Get the similar questions from vector store\n",
    "print(\"Direct query against the vector store\")\n",
    "print(\"-------------------------------------\")\n",
    "vector_store_retriever_results = vector_store_retriever.invoke(input=question)\n",
    "print(vector_store_retriever_results)\n",
    "\n",
    "# Get the context\n",
    "mvr_retriever_results = mvr_retriever.invoke(input=question)\n",
    "print(\"\\nQuery against the MVR. Number of docs = \", len(mvr_retriever_results))\n",
    "print(\"-----------------------------------------------\")\n",
    "print(mvr_retriever_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd1433-dce7-4f13-b60b-1d248d28432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b6106-ce76-49d5-b317-5f0341b0991f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
