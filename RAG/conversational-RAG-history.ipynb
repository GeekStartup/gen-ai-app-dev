{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684d20c8-64a4-4cba-869c-5caae4be165f",
   "metadata": {},
   "source": [
    "# Conversational RAG with Query History Retriever\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/\n",
    "\n",
    "https://cloud.google.com/blog/products/ai-machine-learning/to-tune-or-not-to-tune-a-guide-to-leveraging-your-data-with-llms\n",
    "\n",
    "## LangChain Chroma\n",
    "https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97424890-7c4e-4d0d-b5da-d99a96a7a5c9",
   "metadata": {},
   "source": [
    "## 1. Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29ef1da-a8a0-4ab7-8b4d-434ed29cb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "llm = create_gpt_chat_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10246fb1-2847-4d75-87e4-3f50667aea63",
   "metadata": {},
   "source": [
    "## 2. Setup vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e05f80fe-d966-4e2d-a3cd-8dd42128bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a couple of Blogs \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Sample blogs on RAG that we will add to vector database\n",
    "url1 = \"https://cloud.google.com/blog/products/ai-machine-learning/to-tune-or-not-to-tune-a-guide-to-leveraging-your-data-with-llms\"\n",
    "url2 = \"https://aws.amazon.com/blogs/aws/build-rag-and-agent-based-generative-ai-applications-with-new-amazon-titan-text-premier-model-available-in-amazon-bedrock/\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url1,url2)\n",
    ")\n",
    "\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef81260-3c48-4fc0-a5a7-f4ae653321a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chunk the blogs\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "159e29a9-e930-4155-92ee-6ebb8aa46fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add chunks to the ChromaDB\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# load it into Chroma using default embedding all-MiniLM-L6-v2\n",
    "collection_name = 'sample-blog'\n",
    "collection_metadata = {'embedding': 'all-MiniLM-L6-v2'}\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_store = Chroma(collection_name=collection_name, collection_metadata=collection_metadata, embedding_function=embedding_function)\n",
    "vector_store.add_documents(chunked_documents)\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219759e-df8b-4739-b61a-d9c35676393b",
   "metadata": {},
   "source": [
    "## 2. Chat history retriever\n",
    "\n",
    "\n",
    "An agent is used for creating the *input* for retriever input based on the conversation context.\n",
    "\n",
    "#### Without history\n",
    "query -> retriever\r\n",
    "#### With history:\r\n",
    "(query, conversation history) -> LLM -> rephrased query -> retriev\n",
    "\n",
    "https://api.python.langchain.com/en/latest/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html\n",
    "\n",
    "###### Prompt requires the *chat_history* \n",
    "\n",
    "If there is no chat_history, then the input is just passed directly to the retriever. If there is chat_history, then the prompt and LLM will be used to generate a search query. That search query is then passed to the retriever.\n",
    "\n",
    "#### MessagePlaceHolder\n",
    "Used for passing a list of messages in the prompt.\n",
    "\n",
    "https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.MessagesPlaceholder.htmler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d80beefd-a06c-4b85-9bcb-f08708a6b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# This is the prompt used for generating the input/query from chat history and user input\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# This is where you create the retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6018a8-4914-4d60-9d61-1820cc771355",
   "metadata": {},
   "source": [
    "## 3. History aware conversation retriever chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "779a61ca-4f07-4dd8-9619-b75b0037f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# This the prompt that is sent to LLM for generating response to user input\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3052caa-dfa4-49fc-8e63-f6e8b9ab6f6b",
   "metadata": {},
   "source": [
    "## 4. Tests\n",
    "\n",
    "Because our chain includes a \"chat_history\" input, the caller needs to manage the chat history. We can achieve this by appending input and output messages to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2192a9e-f473-4730-8684-d48305a133df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def  invoke_llm(input):\n",
    "    response = rag_chain.invoke({\"input\": input, \"chat_history\": chat_history})\n",
    "    chat_history.extend(\n",
    "        [\n",
    "            HumanMessage(content=input),\n",
    "            AIMessage(content=response[\"answer\"]),\n",
    "        ]\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c378d6fc-a43d-45fd-81d8-eaa07d91ed38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG stands for Retrieval-Augmented Generation. It is an approach that ensures model outputs are grounded on your data by searching for information relevant to a query within your data and passing that information into the prompt. RAG allows for the retrieval of new context from data with each interaction, supporting fresh, updated, private, and large-scale data.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"What is RAG?\"\n",
    "\n",
    "response = invoke_llm(input)\n",
    "\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b05da8-7a20-41ba-ace5-3598797c33a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG focuses on ensuring model outputs are grounded on data by searching for relevant information within the data, while fine-tuning involves giving specific instructions to a model for a well-defined task, such as classification. RAG allows for the retrieval of new context from data with each interaction, while fine-tuning is more suitable for tasks like structured outputs from free text or classification.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = \"How is it different than fine tuning?\"\n",
    "response = invoke_llm(input)\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd051c5a-b983-4c3c-845f-def706ece690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f13ce-30a6-494a-bd4a-b5ad3c977a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fb2ba-786d-468d-ad0a-7032aae705e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48d5b3-55a2-4d3e-be83-1b4c5fb9edb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94462d86-7478-450c-8bdf-10fbba838ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
