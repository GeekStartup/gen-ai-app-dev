{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d8c020-6d47-4769-a709-19d5e24a206b",
   "metadata": {},
   "source": [
    "# Demonstrates the working of a sequential chain\n",
    "\n",
    "Create a Sequential Chain that takes 3 variables (A, B and C).\n",
    "\n",
    "**Input**\n",
    "* Three words {A}, {B}, {C}\n",
    "\n",
    "**Chain# 1:**  \n",
    "* Creates a partial sentence starting with {A}. The size of the partial sentence = 3\n",
    "* Creates output variable with name = *output_1*\n",
    "\n",
    "**Chain# 2:**  \n",
    "* Extends the partial sentence from chain#1 by using {B}\n",
    "* Creates output variable with name = *output_2*\n",
    "\n",
    "**Chain# 3**\n",
    "* Completes the sentence using the word {C}\n",
    "* Creates output variable with name = *output_3*\n",
    "\n",
    "**Output**\n",
    "* Three words {A}, {B}, {C}, {output_1}, {output_2}, {output_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d86098-1930-458e-bba7-33340c851d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "from IPython.display import JSON\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be750c-16b8-4012-919f-9befcec761a3",
   "metadata": {},
   "source": [
    "#### Select LLM to use\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "To try out different LLMs, call appropriate function below.\n",
    "\n",
    "* You may see different or even incorrect results\n",
    "* Adjust the prompts as needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a7d3ac8-211b-497e-bbdb-eb75926647b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Cohere\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "### Cohere command\n",
    "def create_cohere_command():\n",
    "    return Cohere(model=\"command\", max_tokens=3000)\n",
    "\n",
    "### HuggingFace model\n",
    "def  create_hugging_face():\n",
    "    \n",
    "    llm_hf = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xxl', \n",
    "        model_kwargs={ \"temperature\": \"0.9\" }\n",
    "    )\n",
    "    \n",
    "    ## We need this to warm up the model :)\n",
    "    print(llm_hf.invoke(\"hello model\"))\n",
    "    return llm_hf\n",
    "\n",
    "### Open AI\n",
    "def create_openai_gpt():\n",
    "    return ChatOpenAI(temperature=0.4) #model_name=model_name)\n",
    "\n",
    "\n",
    "### Change function name to try out different provider or model\n",
    "llm = create_openai_gpt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db30dea-6d9a-4e44-9741-8dea5198446b",
   "metadata": {},
   "source": [
    "## Chain 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cc3684-41bf-4cce-98c9-1a3300bdafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains  import LLMChain, SequentialChain\n",
    "\n",
    "template_1 = \"\"\"\n",
    "Use the word '{A}' as the start of a partial sentence. \n",
    "\n",
    "Predict two words that will come after '{A}'\n",
    "\n",
    "Your output will be in the following format.\n",
    "\n",
    "Sentence:\n",
    "Put the partial sentence here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd6fd63-f577-4e21-ad51-c47506e6f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_1 = PromptTemplate(\n",
    "                        template=template_1,\n",
    "                        input_variables = [\"A\", \"B\", \"C\"]\n",
    "                    )\n",
    "\n",
    "llm_chain_1 = LLMChain(\n",
    "    prompt = prompt_template_1,\n",
    "    llm = llm,\n",
    "    output_key = 'output_1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e87ef0-6478-4cf6-8a56-9e6d94523363",
   "metadata": {},
   "source": [
    "## Chain 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc295d4a-c9ad-4d74-8280-b174826912ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\"\n",
    "Extend the following partial sentence by using the word {B}. \n",
    "\n",
    "Partial Sentence:\n",
    "{output_1}\n",
    "\n",
    "Your output will be in the following format.\n",
    "\n",
    "Sentence:\n",
    "Put the partial sentence here.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_2 = PromptTemplate(\n",
    "                        template=template_2,\n",
    "                        input_variables = [\"B\",\"output_1\"]\n",
    "                    )\n",
    "\n",
    "llm_chain_2 = LLMChain(\n",
    "    prompt = prompt_template_2,\n",
    "    llm = llm,\n",
    "    output_key = 'output_2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2413f-9085-49ce-bcc7-0cc8bf5a3b04",
   "metadata": {},
   "source": [
    "## Chain 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5654b9da-ace1-4114-a052-79edbda072f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_3 = \"\"\"\n",
    "Complete the following partial sentence by using the word '{C}'. \n",
    "\n",
    "Partial Sentence:\n",
    "{output_2}\n",
    "\n",
    "Your output will be in the following format.\n",
    "\n",
    "Sentence:\n",
    "Put the complete sentence here.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_3 = PromptTemplate(\n",
    "                        template=template_3,\n",
    "                        input_variables = [\"C\",\"output_2\"]\n",
    "                    )\n",
    "\n",
    "llm_chain_3 = LLMChain(\n",
    "    prompt = prompt_template_3,\n",
    "    llm = llm,\n",
    "    output_key = 'output_3',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b479ef-9127-48dc-9d9e-f9432d89b468",
   "metadata": {},
   "source": [
    "## Sequential chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e926393-7c6a-41f6-b209-7cd11ffb43fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "sequential_chain = SequentialChain(\n",
    "                    chains = [llm_chain_1, llm_chain_2, llm_chain_3],\n",
    "                    input_variables = [\"A\", \"B\", \"C\"],\n",
    "                    output_variables = [\"A\", \"B\", \"C\", \"output_1\", \"output_2\", \"output_3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79cbc9-a0a2-42c9-9bae-81b4761d5843",
   "metadata": {},
   "source": [
    "## Infrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cb84811-edd9-4352-b238-c77b83c3a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sequential_chain.invoke({\"A\":\"power\", \"B\":\"energy\", \"C\":\"cities\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f695a062-cf9d-49e9-bb35-26306ddd17f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "A": "power",
       "B": "energy",
       "C": "cities",
       "output_1": "The power of the king is great.",
       "output_2": "The king has great energy.",
       "output_3": "The king has great energy and he is going to build new cities."
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7014ef04-eb05-4b6c-9c94-a13065b4cc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-xxl', timeout=None)>, repo_id='google/flan-t5-xxl', task='text2text-generation', model_kwargs={'temperature': '0.9'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822b187-4a7f-4a4e-a559-b6d93236fd7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
