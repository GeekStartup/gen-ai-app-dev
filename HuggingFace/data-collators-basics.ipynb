{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12480d84-5c27-4139-879d-b12f82371de4",
   "metadata": {},
   "source": [
    "# Data collators\n",
    "\n",
    "Data collators are essential in natural language processing (NLP) because they simplify the process of preparing data for model training. In NLP tasks, input sequences can vary in length, and models typically require fixed-length sequences to process them in batches. Data collators handle tasks like **padding** (to ensure sequences are the same length), **masking** (to ignore padding tokens during training), and **batching** (to group multiple examples together efficiently). Without data collators, these tasks would need to be manually managed, which can be complex and error-prone. By automating these steps, data collators streamline data preprocessing, improve memory efficiency, and ensure models focus only on relevant tokens, ultimately enhancing training performance and simplicity.\n",
    "\n",
    "[DataCollator documentation](https://huggingface.co/docs/transformers/en/main_classes/data_collator)\n",
    "\n",
    "[Padding & truncation documentation](https://huggingface.co/docs/transformers/en/pad_truncation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec20cad-76fe-4a78-8a42-ee86c1aa105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a1847-7a05-41d8-b9b4-882b406aa816",
   "metadata": {},
   "source": [
    "## DefaultDataCollator\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DefaultDataCollator\n",
    "\n",
    "`DefaultDataCollator` is a versatile and simple data collator used for a variety of natural language processing tasks. It is particularly effective for tasks where sequences need to be padded to the same length, such as **text classification**, **named entity recognition (NER)**, and **sentiment analysis**. This collator automatically handles the padding of input sequences to the maximum length within a batch, optimizing memory usage and ensuring that models process sequences of equal length. Additionally, it generates **attention masks** to differentiate between real tokens and padding, helping the model ignore padding during training. By offering a straightforward solution for dynamic padding, `DefaultDataCollator` streamlines the preparation of data for training, improving both **efficiency** and **performance** without requiring complex configuration. It is suitable for a wide range of tasks and models, making it an essential tool in standard NLP workflows.\n",
    "\n",
    "* **Manual Batching**: We manually select the examples to include in a batch (examples[:2]).\n",
    "* **Dynamic Padding**: If the tokenized examples have different lengths, DefaultDataCollator ensures they are padded to the same length.\n",
    "* **Scalable**: You can extend this approach to process larger datasets by splitting them into chunks (manual batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a83ea3e-f547-4152-a5e5-c7ddfe0807a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    {\"text\": \"Hello, how are you?\"},\n",
    "    {\"text\": \"This is the longest sequence in the batch\"},\n",
    "    {\"text\": \"Shortest one\"}\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4f87f86-3e30-4943-b3e1-50b861a7eb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff46abc67fe44f7b04a93c803ed68b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=10)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d45f67c9-5b43-439f-9e9b-456113c72e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0],\n",
      "        [  101,  2023,  2003,  1996,  6493,  5537,  1999,  1996, 14108,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "# Initialize DefaultDataCollator\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Convert the dataset into a list of examples\n",
    "examples = [tokenized_dataset[i] for i in range(len(tokenized_dataset))]\n",
    "\n",
    "# Manually create a batch (e.g., first two examples)\n",
    "# Batch size = 2\n",
    "batch = data_collator(examples[:2])\n",
    "\n",
    "# Print the collated batch\n",
    "print(batch)\n",
    "print(batch['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8004d60-6d72-4746-8364-c262a08df60a",
   "metadata": {},
   "source": [
    "## DataCollatorWithPadding\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithPadding\n",
    "\n",
    "The DataCollatorWithPadding is a Hugging Face utility that dynamically pads tokenized sequences in a batch to match the longest sequence in that batch. It's particularly useful when working with variable-length inputs. `DataCollatorWithPadding` is particularly useful in scenarios where sequences of varying lengths need to be processed efficiently. It is ideal for tasks like **text classification**, **question answering**, and other NLP tasks, where the input sequences can differ in length. The collator dynamically pads the sequences to the length of the longest sequence in the batch, ensuring that the padding is minimized, which optimizes memory usage. It also handles **masking** for padded tokens, ensuring that the model only attends to actual tokens during training. This dynamic padding approach makes it suitable for a wide range of models, including those for tasks like **named entity recognition (NER)** or **sentiment analysis**, where input sequences can vary in size. By reducing unnecessary padding, it enhances **training efficiency** and **model performance**.\n",
    "\n",
    "1. **Dynamic Padding**:\n",
    "* Automatically pads sequences to the length of the longest sequence in the batch.\n",
    "Reduces the amount of padding compared to fixed-length padding, saving memory and computation.\n",
    "\n",
    "2. **Tokenization-Aware**:\n",
    "* Uses the tokenizer's padding strategies, ensuring consistency with special tokens (e.g., [PAD]).\n",
    "\n",
    "3. **Task-Agnostic**:\n",
    "* Can be used for a wide range of NLP tasks, including classification, question answering, and more.\n",
    "\n",
    "4. **Integration**:\n",
    "\n",
    "* Designed to be used directly with PyTorch's DataLoader or manually for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c035587-8608-4c35-8add-93309f28d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Tokenize individaul sample with padding = max_length  =======\n",
      "{'text': 'Hello, how are you?'}    len[input_ids] =  100\n",
      "{'text': 'This is the longest sequence in the batch'}    len[input_ids] =  100\n",
      "{'text': 'Shortest one'}    len[input_ids] =  100\n",
      "======================== Tokenize without padding ======\n",
      "{'text': 'Hello, how are you?'}    len[input_ids] =  8\n",
      "{'text': 'This is the longest sequence in the batch'}    len[input_ids] =  10\n",
      "{'text': 'Shortest one'}    len[input_ids] =  4\n",
      "======================== Padding with DataCollatorWithPadding ======\n",
      "tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0],\n",
      "        [  101,  2023,  2003,  1996,  6493,  5537,  1999,  1996, 14108,   102],\n",
      "        [  101, 20047,  2028,   102,     0,     0,     0,     0,     0,     0]])\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    {\"text\": \"Hello, how are you?\"},\n",
    "    {\"text\": \"This is the longest sequence in the batch\"},\n",
    "    {\"text\": \"Shortest one\"}\n",
    "]\n",
    "\n",
    "print(\"======================= Tokenize individaul sample with padding = max_length  =======\")\n",
    "# Tokenize the data with padding set to max_length - creates a list of dicts\n",
    "# Assume the longest sequence in the data is length = 100\n",
    "tokenized_data = [tokenizer(d[\"text\"], truncation=True, padding=\"max_length\", max_length=100) for d in data]\n",
    "for i, dat in enumerate(tokenized_data):\n",
    "    print(data[i],'   len[input_ids] = ', len(dat['input_ids']))\n",
    "\n",
    "print(\"======================== Tokenize without padding ======\")\n",
    "tokenized_data = [tokenizer(d[\"text\"], truncation=True) for d in data]\n",
    "\n",
    "for i, dat in enumerate(tokenized_data):\n",
    "    print(data[i],'   len[input_ids] = ', len(dat['input_ids']))\n",
    "\n",
    "print(\"======================== Dynamic padding with DataCollatorWithPadding ======\")\n",
    "\n",
    "# Use DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Prepare a batch\n",
    "batch = data_collator(tokenized_data)\n",
    "\n",
    "# Print batch keys and tensor shapes\n",
    "print(batch['input_ids'])\n",
    "\n",
    "# In a batch all input_ids are packed in a single tensor of equal length\n",
    "# [x, y]  x = Size of the batch, y = size of the ['input_ids']\n",
    "# Padding length = Size of the longest sequence in the batch\n",
    "print(batch['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dd33b-a822-4610-8751-21a0bf3cc06b",
   "metadata": {},
   "source": [
    "## DataCollatorForSeq2Seq\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq\n",
    "\n",
    "`DataCollatorForSeq2Seq` is specifically designed for sequence-to-sequence tasks such as machine translation, summarization, and text generation. It is ideal for models like T5, BART, or MarianMT, where both **input sequences** (e.g., source texts) and **target sequences** (e.g., labels) are required. This collator dynamically pads both the input and target sequences to the length of the longest sequence in the batch, ensuring efficient memory usage and reducing unnecessary padding. It also creates **attention masks** for both inputs and labels, allowing the model to ignore padded tokens during training. By handling padding and masking seamlessly for both input and target sequences, `DataCollatorForSeq2Seq` simplifies batch preparation and improves **training efficiency** for sequence-to-sequence tasks. This makes it especially useful for tasks like **machine translation**, **summarization**, and **question answering**, where the model must process pairs of source and target texts.\n",
    "\n",
    "1. **Dynamic Padding**:\n",
    "\n",
    "* Automatically pads sequences to the maximum length in the batch, reducing unnecessary padding compared to padding to a fixed length.\n",
    "* Ensures both input sequences (e.g., source texts) and target sequences (e.g., labels) are padded to the same length for proper batching.\n",
    "\n",
    "2. **Supports Labels**:\n",
    "\n",
    "* Specially designed for Seq2Seq tasks where both input and target sequences are needed.\n",
    "* The collator will handle padding for both input_ids and labels (target sequences).\n",
    "\n",
    "3. **Masking for Padding**:\n",
    "\n",
    "* Creates attention_mask tensors for both inputs and labels, ensuring the model ignores padded tokens during training.\n",
    "* Masks are set to 1 for actual tokens and 0 for padding tokens, ensuring no contribution from padding tokens during loss calculation.\n",
    "\n",
    "3. **Efficient for Seq2Seq Models**:\n",
    "\n",
    "* Specifically tailored for sequence-to-sequence models like T5, BART, or MarianMT, which require both source and target sequences.\n",
    "* It makes batch preparation easier and more efficient for such models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5599932e-c754-4a3b-b91c-4051141271bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer and model (e.g., T5 for summarization)\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Source & Target text\n",
    "data = [\n",
    "    {\"source_text\": \"Translate English to French: How are you?\", \"target_text\": \"Comment ça va?\"},\n",
    "    {\"source_text\": \"Translate English to French: I love programming.\", \"target_text\": \"J'aime programmer.\"}\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feb80bc-1b9c-4d4b-8f0f-d205529628fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba04b80e1534e34be04ee5b8f19b04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize input (source) and target (label) texts\n",
    "    model_inputs = tokenizer(examples[\"source_text\"], max_length=50, truncation=True, )\n",
    "    labels = tokenizer(examples[\"target_text\"], max_length=50, truncation=True, )\n",
    "\n",
    "    # Add labels to the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Fix to a BUG\n",
    "# Need to remove text columns otherwise, you will get the error: \n",
    "# ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`source_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['source_text', 'target_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c72ce6e-9173-4d4a-87a3-b2bc4b44d355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[30355,    15,  1566,    12,  2379,    10,   571,    33,    25,    58,\n",
      "             1],\n",
      "        [30355,    15,  1566,    12,  2379,    10,    27,   333,  6020,     5,\n",
      "             1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[5257, 3664,  409,   58,    1, -100, -100],\n",
      "        [ 446,   31, 9595, 2486,   52,    5,    1]]), 'decoder_input_ids': tensor([[   0, 5257, 3664,  409,   58,    1,    0],\n",
      "        [   0,  446,   31, 9595, 2486,   52,    5]])}\n",
      "input_ids: torch.Size([2, 11])\n",
      "attention_mask: torch.Size([2, 11])\n",
      "labels: torch.Size([2, 7])\n",
      "decoder_input_ids: torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Use DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Example: Prepare a batch\n",
    "batch = data_collator([tokenized_dataset[i] for i in range(len(data))])\n",
    "\n",
    "print(batch)\n",
    "# Check the output\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key}: {value.shape if hasattr(value, 'shape') else value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754faf7-6305-4121-b4fd-3b526103295f",
   "metadata": {},
   "source": [
    "## DataCollatorForLanguageModeling\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
    "\n",
    "`DataCollatorForLanguageModeling` is useful in several scenarios, particularly when working with language models. It is ideal for **Masked Language Modeling (MLM)**, as seen in models like BERT, where a percentage of tokens are masked and the model must predict them. It is also applicable to **Causal Language Modeling (CLM)**, used in models like GPT, where the model predicts the next token in a sequence. Additionally, it is helpful in **Text Generation** tasks, where models generate text based on context, such as with GPT-based models. Furthermore, it ensures **Efficient Language Model Training** by applying dynamic padding, which optimizes memory usage and improves training efficiency, especially when handling variable-length sequences.\n",
    "\n",
    "1. **Supports Both MLM and CLM**:\n",
    "* Handles both masked and causal language modeling tasks, making it versatile.\n",
    "  \n",
    "2. **Dynamic Padding**:\n",
    "* Efficient padding that reduces unnecessary computation and memory usage.\n",
    "\n",
    "3. **Attention Masking**:\n",
    "* Ensures padding tokens are ignored in the attention mechanism, making the model focus on real tokens only.\n",
    "\n",
    "4. **Prepares Tensors for Training**:\n",
    "* The collator returns the inputs, attention masks, and labels in the correct format for training language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ff903df-23b0-429e-b18a-469198e371e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs with masking:\n",
      " tensor([[ 101, 2023, 2003, 1037,  103, 6251, 1012,  102,    0],\n",
      "        [ 101,  103, 2742, 2005,  103, 2653,  103,  103,  102]])\n",
      "Labels:\n",
      " tensor([[ -100,  -100,  -100,  -100,  7099,  -100,  1012,  -100,  -100],\n",
      "        [ -100,  2178,  -100,  -100, 16520,  -100, 11643,  1012,  -100]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Another example for masked language modeling.\"\n",
    "]\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_data = tokenizer(data, truncation=True, max_length=10, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Use DataCollatorForLanguageModeling\n",
    "# mlm_probability=0.15, each token has a 15% chance of being masked.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# Prepare a batch\n",
    "batch = data_collator([tokenized_data[\"input_ids\"][i] for i in range(len(data))])\n",
    "\n",
    "# Print masked inputs\n",
    "# input_ids: Contains the tokenized sentences with some tokens replaced by [MASK] (or randomized).\n",
    "# labels: Contains the original token IDs, with masked positions preserved and all other positions set to -100 (ignored by the loss function).\n",
    "print(\"Input IDs with masking:\\n\", batch[\"input_ids\"])\n",
    "print(\"Labels:\\n\", batch['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73dfd9-3722-485b-bdd2-fb531622d3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
