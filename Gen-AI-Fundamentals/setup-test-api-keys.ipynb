{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbad3768-9963-462b-ace5-dc643fe0fe40",
   "metadata": {},
   "source": [
    "# Setup & Test API Key\n",
    "\n",
    "This notebook is used for testing the API keys setup for model/service providers.\n",
    "\n",
    "### Test a specific provider's key\n",
    "1. Run the cell : **Setup the environment**\n",
    "2. Run the cells for the provider\n",
    "3. The code makes an explicit call to *api_key_check(\"API_KEY_NAME\")*  but you may use: create_xxx_llm(api_key_prompt=True) to do avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d890e1-3b66-43c9-8144-b4107ebe20fd",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the utilities & packages by uncommenting & running the cell below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfa0306-8598-479c-ad72-b1fc017c5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b1ece8-18d6-4a7b-ba84-fc65b724adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     C:\\Users\\raj\\anaconda3\n",
      "gen-ai-app-dev-course  *  C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda info --env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aa554-5228-4cf1-a0c4-94ac71013943",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a0183a-7f18-4197-8aaa-0e966f898031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# CHANGE THIS TO POINT TO YOUR OWN FILE\n",
    "# Load the file that contains the API keys \n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f0b3ad-a8ae-4704-9ef4-ebf1fa7e4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a92ed4-e478-4fa0-b5a9-a06ace7f0c65",
   "metadata": {},
   "source": [
    "## Google API key\n",
    "\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c38369ec-0779-4548-bd5c-f9a4dac42d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  GOOGLE_API_KEY  already set in environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAI(model='gemini-1.5-flash', google_api_key=SecretStr('**********'), client=genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       "))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_google_llm\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Try out the Gemini Flash model\n",
    "# model=\"gemini-1.5-flash\" - created by default - you may try (model=\"gemini-1.5-pro\")\n",
    "llm = create_google_llm()\n",
    "\n",
    "# The above 2 lines is equivalent to the next line :-)\n",
    "# llm = create_google_llm(api_key_prompt=True)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5914d2-3621-40f4-b9bf-47b01debdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs, or Large Language Models, are powerful AI systems trained on massive amounts of text data. They can understand and generate human-like text, making them adept at tasks like translation, writing, and answering questions. LLMs learn patterns and relationships in language, allowing them to predict and generate coherent text based on the input they receive. These models are still under development, but they hold immense potential for revolutionizing communication and information access.  However, it's crucial to use LLMs responsibly, considering their potential biases and ethical implications. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in 5 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed22230-4477-475b-b21b-836fa12cd6fa",
   "metadata": {},
   "source": [
    "## HuggingFace token\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "#### Note\n",
    "Not all models are available for as endpoints. Check model's inference endpoint state to check if model endpoint is active:\n",
    "\n",
    "https://huggingface.co/models?inference=warm&other=endpoints_compatible\n",
    "\n",
    "**Warm**   Model inference endpoint is active\n",
    "\n",
    "**Cold**   Model will be loaded on request. Large models sometime timeout but, they become available after a couple of retries :)\n",
    "\n",
    "**Freezed** Model inference endpoint NO MORE available as there was not much interest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece41d2c-53f0-4a03-ae82-7338908a1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  HUGGINGFACEHUB_API_TOKEN  already set in environment.\n",
      "Key:  HUGGINGFACEHUB_API_TOKEN  already set in environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token='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', stop_sequences=[], server_kwargs={}, model_kwargs={}, model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_hugging_face_llm\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Deafult HuggingFace model in use : mistralai/Mistral-7B-Instruct-v0.2\n",
    "# GATED MODEL !!! \n",
    "# Was your access request approved?  Check: https://huggingface.co/settings/gated-repos\n",
    "#\n",
    "# model= \"mistralai/Mistral-7B-Instruct-v0.2\" created by default\n",
    "llm = create_hugging_face_llm(api_key_prompt=True)\n",
    "\n",
    "# Small model - not as performant as Mistral\n",
    "# Select model by comment/uncomment\n",
    "# model='openai-community/gpt2-large'\n",
    "# model='microsoft/Phi-3-mini-4k-instruct'\n",
    "# llm=create_hugging_face_llm(repo_id=model)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26483ba3-dd2d-48a2-82c8-bb271b170abc",
   "metadata": {},
   "source": [
    "### Note\n",
    "Sometime HuggingFace models time out :( It happens as a result model being in cold state. Two ways to address:\n",
    "\n",
    "1. Just retry and it should work\n",
    "2. Use a different model (usually smaller models are more available)\n",
    "\n",
    "HfHubHTTPError: 504 Server Error: Gateway Timeout for url: https:...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5c2080-bc77-4ff0-842e-1b7db2033813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "LLM (Limited Memory Model) in generative AI refers to a class of models that can generate text or other sequences of data based on patterns learned from the input data, while maintaining a limited amount of context from previous outputs in memory to inform the generation of subsequent tokens. This allows for more efficient and focused generation, but may sacrifice some of the long-term coherence and contextual understanding that larger model architectures can provide.\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f0c27-a584-4b4a-9115-7098f7db911c",
   "metadata": {},
   "source": [
    "## Cohere API Key\n",
    "\n",
    "COHERE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed12d716-9c40-47bb-b330-a99dd75983c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  COHERE_API_KEY  already set in environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import Cohere`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cohere(client=<cohere.client.Client object at 0x000001F19D017B00>, async_client=<cohere.client.AsyncClient object at 0x000001F19E9351C0>, cohere_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_cohere_llm\n",
    "\n",
    "# # Check if the key is available\n",
    "# api_key = api_key_check(\"COHERE_API_KEY\")\n",
    "\n",
    "#  Create the default model : following would not require an explicit call to api_key_check\n",
    "llm = create_cohere_llm(api_key_prompt=True)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29efc70-7bb4-4670-b773-f34493b70912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM stands for large language model and is used in generative AI to analyze and produce human-like language, focusing on predicting what word, phrase, or sentence comes next in a sequence. \n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196450f-965b-42f4-9b2d-f5f1a4cadbfd",
   "metadata": {},
   "source": [
    "## Olama\n",
    "DOES NOT REQUIRE AN API KEY !! but the Ollama server should be running.\n",
    "\n",
    "**Note**\n",
    "1. Install langchain Ollama\n",
    "\n",
    "```\n",
    "pip install -U langchain-ollama\n",
    "```\n",
    "\n",
    "2. Run the server\n",
    "\n",
    "```\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528b4dfb-7635-4c1c-92b5-322c32c33a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_llm import create_ollama_llm\n",
    "\n",
    "llm = create_ollama_llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c832454c-8ac4-4e3a-83fc-c9ba9c65ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs (Large Language Models) are a type of AI trained on massive text datasets, enabling them to understand and generate human-like text. Within generative AI, LLMs serve as powerful tools for creating various textual outputs, including stories, articles, code, and dialogue.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73e1e1-87db-4fd5-9822-072b06a1da04",
   "metadata": {},
   "source": [
    "## Groq API Key\n",
    "\n",
    "Groq_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa289357-9333-4b41-ae13-eefe0b062e7e",
   "metadata": {},
   "source": [
    "T.B.D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
