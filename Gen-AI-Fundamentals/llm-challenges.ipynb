{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bff477c",
   "metadata": {},
   "source": [
    "# LLM Challenges\n",
    "\n",
    "You can use either the InferenceClient or the HTTP API invocation.\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation\n",
    "\n",
    "**Note**\n",
    "* A return value of 503 indicates that the model is i cold state and is loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4515c-7734-474b-bb7f-4332e4ab8313",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the HF API Token\n",
    "\n",
    "Uncomment the code in the cells and run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed22c1f-31ce-4c47-84c2-e4de146f3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   312  100   312    0     0   3244      0 --:--:-- --:--:-- --:--:--  3319\n",
      "'chmod' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "## The script is downloaded and run to setup the utils folder\n",
    "\n",
    "!curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "!chmod u+x gcsetup.sh\n",
    "!./gcsetup.sh\n",
    "!rm ./gcsetup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a5e71",
   "metadata": {},
   "source": [
    "## Setup the enviornment varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b45c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# Sets up keys : HUGGINGFACEHUB_API_TOKEN, OPENAI_API_KEY, ...\n",
    "\n",
    "# setting path for utils package\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8c1f7a6-f88d-4b93-a631-f50f9a5f936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HUGGINGFACEHUB_API_TOKEN=os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694e045",
   "metadata": {},
   "source": [
    "## Create LLM for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7abfdb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'google/gemma-2-2b-it',\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f3e7f2",
   "metadata": {},
   "source": [
    "## 1. Hallucination\n",
    "\n",
    "Some models are better than others. Try out a couple of models to figure out the ones that hallucinate more than other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a6b51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"define LLM in the context of biology\\nLLM stands for 'Lysin-like motif'. It is a type of protein motif that is found in many different types of proteins, including those involved in the processing and transport of proteins within cells.\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"define LLM in the context of biology\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(hugging_face_model_ids[0])\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04015520",
   "metadata": {},
   "source": [
    "## 2. Dated knowledge\n",
    "\n",
    "**Note:**\n",
    "You will also observe hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ce47a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'as of today, who is the prime minister of UK?\\nAs of today, the Prime Minister of the United Kingdom is Boris Johnson.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out the models & your own prompts\n",
    "# text = \"who won the 2022 super bowl?\"  # LA Rams vs Cincinnati Bengals  https://en.wikipedia.org/wiki/Super_Bowl_LVI\n",
    "text = \"as of today, who is the prime minister of UK\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[0])\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a080bf8b",
   "metadata": {},
   "source": [
    "## 3. Missing context\n",
    "\n",
    "Model is not aware of your enterprise's business domain or model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b97a64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'what is your return policy?\\nWe offer a 30-day return policy on all products. If you are not satisfied with your purchase, you can return the product for a full refund.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out the models and your own prompts\n",
    "text = \"what is your return policy?\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  # 1 = mistralai/Mistral-7B-Instruct-v0.2\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d7fdd",
   "metadata": {},
   "source": [
    "## 4. Bias\n",
    "\n",
    "Some models are trained to prevent biases. Try out the model **index= 1 'tiiuae/falcon-7b-instruct'** to see its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07d95e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'for the engineering jobs we should only hire men because 1) they are stronger and 2) they are more logical and less emotional.\\n\\n1. Strength is not a requirement for engineering jobs. In fact, many engineering jobs require a high degree of dexterity and fine motor skills, which women often possess in greater numbers than men. Additionally, the use of machinery and power tools has been largely automated, reducing the need for brute strength in engineering roles.\\n2. The idea that men are more logical and less emotional than'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"for the engineering jobs we should only hire men because \"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  # 2 = openlm-research/open_llama_3b_v2\n",
    "# llm.text_generation(text, max_new_tokens=75)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[2])\n",
    "llm_client.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbe0f5-61c7-4f85-951c-283d0085bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
