{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a5d27c",
   "metadata": {},
   "source": [
    "# Roberta Pre-Training with *wikismall* dataset\n",
    "\n",
    "We will train the model for MLM\n",
    "\n",
    "1. Load the dataset [acloudfan/wikismall](https://huggingface.co/datasets/acloudfan/wikismall)\n",
    "2. Tokenize the dataset using *RobertaTokenizerFast*\n",
    "3. Data pre-processing to get it ready for training : Grouping of data to chunks of fixed size\n",
    "4. Setup DataCollatorForLanguageModeling to (a) create batches (b) randomply mask tokens\n",
    "5. Setup TrainingArguments\n",
    "6. Create model with random weights\n",
    "7. Setup trainer & run training\n",
    "8. Push trained model to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe46774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for Google Colab\n",
    "# !pip install datasets torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac098a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DownloadConfig, load_dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, \n",
    "                          DataCollatorForLanguageModeling, \n",
    "                          TrainingArguments, \n",
    "                          Trainer, \n",
    "                          AutoConfig, \n",
    "                          AutoModelForMaskedLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0933c60",
   "metadata": {},
   "source": [
    "## 1. Load Dataset \n",
    "\n",
    "The dataset is already split into 'train' and 'validation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b974be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset_name='acloudfan/wikismall'\n",
    "\n",
    "# Load\n",
    "raw_datasets = load_dataset(dataset_name)\n",
    "\n",
    "# Shuffling is not optional\n",
    "# raw_datasets = raw_datasets.shuffle(seed=23)\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d1ea5",
   "metadata": {},
   "source": [
    "## 2. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer\n",
    "tokenizer_name = 'roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# Maximum sequence length supported by the model\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to tokenize\n",
    "text_column_name = \"text\" \n",
    "\n",
    "# Pad/Truncate to max_seq_length\n",
    "# The DataCollator optimizes the processing if the special token mask is provided (return_special_tokens_mask=True)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name], \n",
    "                     return_special_tokens_mask=True)\n",
    "    \n",
    "#     return tokenizer(examples[text_column_name], \n",
    "#                      return_special_tokens_mask=True, \n",
    "#                      padding='max_length', \n",
    "#                      max_length=max_seq_length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c891eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features from the datasets - these are column names\n",
    "# In the tokenized dataset we will remove all columns from the original dataset\n",
    "# IGNORE THE WARNING - we will take care of adjusting the sequence lengths later (block_size)\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "                    tokenize_function,\n",
    "                    batched=True,\n",
    "                    remove_columns=column_names,\n",
    "                )\n",
    "\n",
    "# Note down the number of rows\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0afee1",
   "metadata": {},
   "source": [
    "## 3. Data pre-processing to get it ready for training : Grouping of data to chunks of fixed size\n",
    "\n",
    "### Data Pre-processing \n",
    "\n",
    "Since the data is already clean, we don't need to carry out any cleaning. Only thing we need to do is creation of chunks that of data that will be fed for training. This chunking mechanism will make the training efficient.\n",
    "\n",
    "Two steps:\n",
    "\n",
    "#### 1. Define a chunking function\n",
    "\n",
    "Receives an array of tokenized sentences (input_ids, attention_mask, special_tokens). Function concatenates each feature array and then breaks the concatenated keys into chunks of fixed length (=block_size). If the last chunk is < block_size, it gets dropped; this is done to keep the logic simple.\n",
    "\n",
    "1. Decide on a chunk or block size\n",
    "2. Function arguments = an array of tokenized dataset rows (N number of rows)\n",
    "3. In tokenized dataset:\n",
    "   * Concatenate the values in each of the features in the tokenized_datsets\n",
    "   * Create chunks for each feature = block_size\n",
    "   * Discard last chunk if its lenth < block_size\n",
    "   \n",
    "#### 2. Use the Dataset.map function to apply chunking\n",
    "\n",
    "Checkouut documentation to understanding how *map* function is applied to a batch of rows in a Dataset.\n",
    "\n",
    "https://huggingface.co/docs/datasets/v2.15.0/en/package_reference/main_classes#datasets.Dataset.map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0249c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define chunking function\n",
    "\n",
    "def group_text_tokens(examples):\n",
    "    \n",
    "    block_size = 128\n",
    "      \n",
    "    # Concatenate input_ids, special_token_mask, attention_mask \n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    \n",
    "    # Get the length of concatenated arrays/tensors\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Split the concatenated keys into arrays of size = block_size\n",
    "    # Size of last block may be < block_size\n",
    "    #    1. You can either drop the last block\n",
    "    #    2. Or you can pad input_ids to make its len = block_size, do not forget the attention_mask/special_tokens\n",
    "    \n",
    "    # To keep things simple - we will drop the last block\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Copy the input_ids to a new column 'labels'\n",
    "    # This new columns get utilized in a later part\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b81fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply function to batches\n",
    "\n",
    "# Take advantage of multiple core/gpu by setting the *num_proc* > 1\n",
    "# Batch size may be adjusted - higher means less data loss\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_text_tokens,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# Note down the number of rows\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0097c75",
   "metadata": {},
   "source": [
    "## 4. Create the data collator\n",
    "\n",
    "Since we are training Roberta for MLM, we will use the DataCollatorForLanguageModeling which has the inbuilt capability of masking the tokens.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/data_collator#transformers.DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the collator for use by Trainer\n",
    "\n",
    "MLM_PROBABILITY = 0.15\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=MLM_PROBABILITY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86872cd",
   "metadata": {},
   "source": [
    "## 5. Setup training arguments\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "#### Adjust the training arguments to experiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6750a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"wikismall-roberta-mlm-training\",\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 50,\n",
    "    log_level = 'warning',\n",
    "    use_cpu = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391ed4ea",
   "metadata": {},
   "source": [
    "## 6. Create model with randomly initialized weights\n",
    "\n",
    "1. Create the configuration from checkpoint = 'roberta-base'\n",
    "2. Use the configuration to create an instance of the untrained Roberta model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint='roberta-base'\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForMaskedLM.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865d3f2",
   "metadata": {},
   "source": [
    "## 7. Setup trainer & run training\n",
    "\n",
    "Note:\n",
    "\n",
    "Training loss = No Log may be encountered. This generally happens when low volumes of data are used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce9118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b22b60",
   "metadata": {},
   "source": [
    "## 8. Push the model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bda99",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN='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'\n",
    "\n",
    "model_name = 'wikismall-roberta'\n",
    "\n",
    "model.push_to_hub(model_name, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40d6d96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
