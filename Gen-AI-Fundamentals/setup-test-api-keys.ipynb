{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbad3768-9963-462b-ace5-dc643fe0fe40",
   "metadata": {},
   "source": [
    "# Setup & Test API Key\n",
    "\n",
    "This notebook is used for testing the API keys setup for model/service providers.\n",
    "\n",
    "### Test a specific provider's key\n",
    "1. Run the cell : **Setup the environment**\n",
    "2. Run the cells for the provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d890e1-3b66-43c9-8144-b4107ebe20fd",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b44d73b-a342-4aca-9d54-fc4db1352731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl -H 'Authorization: token the_token' -L -o pip-requirements.txt https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/pip-requirements.txt\n",
    "# pip -install --quiet -r pip-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aa554-5228-4cf1-a0c4-94ac71013943",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a0183a-7f18-4197-8aaa-0e966f898031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# CHANGE THIS TO POINT TO YOUR OWN FILE\n",
    "# Load the file that contains the API keys \n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f0b3ad-a8ae-4704-9ef4-ebf1fa7e4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a92ed4-e478-4fa0-b5a9-a06ace7f0c65",
   "metadata": {},
   "source": [
    "## Google API key\n",
    "\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38369ec-0779-4548-bd5c-f9a4dac42d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  GOOGLE_API_KEY  already set in environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAI(model='gemini-1.5-flash', google_api_key=SecretStr('**********'), client=genai.GenerativeModel(\n",
       "    model_name='models/gemini-1.5-flash',\n",
       "    generation_config={},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       "))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_google_llm\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Try out the Gemini Flash model\n",
    "# model=\"gemini-1.5-flash\" - created by default\n",
    "llm = create_google_llm()\n",
    "\n",
    "# Uncomment to create the pro model\n",
    "# llm=create_google_llm(model=\"gemini-1.5-pro\")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa5914d2-3621-40f4-b9bf-47b01debdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs, or Large Language Models, are powerful AI systems trained on massive amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.  They work by learning patterns and relationships in the data, allowing them to predict the next word in a sequence and generate coherent text. LLMs are still under development, but they have the potential to revolutionize how we interact with computers and information. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in 5 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed22230-4477-475b-b21b-836fa12cd6fa",
   "metadata": {},
   "source": [
    "## HuggingFace token\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "#### Note\n",
    "Not all models are available for as endpoints. Check model's inference endpoint state to check if model endpoint is active:\n",
    "\n",
    "https://huggingface.co/models?inference=warm&other=endpoints_compatible\n",
    "\n",
    "**Warm**   Model inference endpoint is active\n",
    "\n",
    "**Cold**   Model will be loaded on request. Large models sometime timeout but, they become available after a couple of retries :)\n",
    "\n",
    "**Freezed** Model inference endpoint NO MORE available as there was not much interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece41d2c-53f0-4a03-ae82-7338908a1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  HUGGINGFACEHUB_API_TOKEN  already set in environment.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\raj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2', model='mistralai/Mistral-7B-Instruct-v0.2', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.2', timeout=120)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_hugging_face_llm\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Deafult HuggingFace model in use : mistralai/Mistral-7B-Instruct-v0.2\n",
    "# GATED MODEL !!! \n",
    "# Was your access request approved?  Check: https://huggingface.co/settings/gated-repos\n",
    "#\n",
    "# model= \"mistralai/Mistral-7B-Instruct-v0.2\" created by default\n",
    "llm = create_hugging_face_llm()\n",
    "\n",
    "# Small model - not as performant as Mistral\n",
    "# Select model by comment/uncomment\n",
    "# model='openai-community/gpt2-large'\n",
    "# model='microsoft/Phi-3-mini-4k-instruct'\n",
    "# llm=create_hugging_face_llm(repo_id=model)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26483ba3-dd2d-48a2-82c8-bb271b170abc",
   "metadata": {},
   "source": [
    "### Note\n",
    "Sometime HuggingFace models time out :( It happens as a result model being in cold state. Two ways to address:\n",
    "\n",
    "1. Just retry and it should work\n",
    "2. Use a different model (usually smaller models are more available)\n",
    "\n",
    "HfHubHTTPError: 504 Server Error: Gateway Timeout for url: https:...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5c2080-bc77-4ff0-842e-1b7db2033813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". LLM, or Language Model Maximization, is a concept in generative AI that refers to the optimization of language models to generate human-like text by maximizing their likelihood of producing target sequences, often through large-scale training on diverse datasets. It is a key approach in developing advanced AI text generators that can create coherent and contextually accurate responses.\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f0c27-a584-4b4a-9115-7098f7db911c",
   "metadata": {},
   "source": [
    "## Cohere API Key\n",
    "\n",
    "COHERE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed12d716-9c40-47bb-b330-a99dd75983c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  COHERE_API_KEY  already set in environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 0.3.0. An updated version of the class exists in the langchain-cohere package and should be used instead. To use it run `pip install -U langchain-cohere` and import as `from langchain_cohere import Cohere`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cohere(client=<cohere.client.Client object at 0x0000028CC08FA300>, async_client=<cohere.client.AsyncClient object at 0x0000028CC27A1340>, cohere_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_cohere_llm\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"COHERE_API_KEY\")\n",
    "\n",
    "#  Create the default model\n",
    "llm = create_cohere_llm()\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29efc70-7bb4-4670-b773-f34493b70912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM stands for large language model and is primarily used in the context of conversational AI and generative AI, as the size and quality of the data fed into the model allows it to understand and generate human-like responses andcreate content such as essays, poems and code with increased credibility the larger it becomes. \n",
      "\n",
      "Generative AI is a type of AI application that uses an LLM to generate new content, including images, videos, and text, based on the information it has been trained on. \n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d546c15-543d-483c-80b4-877e03a3ad01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
