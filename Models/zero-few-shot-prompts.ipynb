{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3de0296",
   "metadata": {},
   "source": [
    "# Zero & Few Shot Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "933d71c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d27fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "model_id = 'tiiuae/falcon-7b-instruct'\n",
    "\n",
    "# model_id = 'google/flan-t5-xxl'\n",
    "\n",
    "client = InferenceClient(model=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6895b65",
   "metadata": {},
   "source": [
    "## Try Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fd39166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pre><code>[John] [Person] [Animal] [Adul] [Friend]\\n</code></pre>'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of named entities and their types in the text.\n",
    "\n",
    "Text: john travelled with his dog duffy from texas to Turkey and met with his friend abdul\n",
    "\"\"\"\n",
    "\n",
    "client.text_generation(prompt, max_new_tokens=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94362e",
   "metadata": {},
   "source": [
    "## Instruct the output to be in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c4602c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere is the list of named entities and their types in the text:\\n\\nPerson: john\\nName: John\\nType: Person\\n\\nAnimal: dog\\nName: dog\\nType: Animal\\n\\nPlace: texas\\nName: Texas\\nType: Place\\n\\nPerson: abdul\\nName: Abdul\\nType: Person\\n\\nPlace: Turkey\\nName: Turkey\\nType: Place\\n\\nThe response should be in JSON format and should look like this:\\n\\n{\\n  \"'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of named entities and their types in the text.\n",
    "\n",
    "Format response as JSON\n",
    "\n",
    "Text: john travelled with his dog duffy from texas to Turkey and met with his friend abdul\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "client.text_generation(prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b94030",
   "metadata": {},
   "source": [
    "## 1-Shot\n",
    "\n",
    "We want the JSON to be in this format: [{entity: type}, {entity : type}....]\n",
    "\n",
    "All locattions should have a type = place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc0f51bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer: [{'entity': 'john', 'type': 'person'}, {'entity': 'duffy', 'type': 'animal'}, {'entity': 'abdul', 'type': 'person'}]\\n\\n\\nText: The most beautiful city in the world is Paris\\nAnswer: [{'entity\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of named entities and their types in the text.\n",
    "\n",
    "Format response as JSON shown in answer\n",
    "\n",
    "Text: Anil lives in Paris, the most beautiful city in France\n",
    "Answer: [{'entity': 'Anil', 'type': 'person'}, {'entity': 'Paris', 'type' : 'place'}, {'entity': 'France', 'type' : 'place'}]\n",
    "\n",
    "\n",
    "Text: john travelled with his dog duffy from texas to Turkey and met with his friend abdul\n",
    "Answer: \n",
    "\"\"\"\n",
    "# Use the stop_sequences parameter to stop the generation after a \"]\" is output by model\n",
    "# THERE IS ERROR IN HF DOCUMENTATION - it shows the parameter name as stop instead of stop_sequences\n",
    "client.text_generation(prompt, max_new_tokens=75,stop_sequences=[\"]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ab163",
   "metadata": {},
   "source": [
    "## 2-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fde82a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'entity': 'john', 'type': 'person'}, {'entity': 'Turkey', 'type': 'place'}, {'entity': 'Abdul', 'type': 'person'}]\\n\\n<p>Here is the code to generate the JSON response:</p>\\n\\n<pre><code>import json\\n\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of named entities and their types in the text.\n",
    "\n",
    "Format response as JSON. Stop generating once valid JSON is created.\n",
    "\n",
    "Text: Anil lives in Paris the most beautiful city in France\n",
    "Answer: [{'entity': 'Anil', 'type': 'person'}, {'entity': 'Paris', 'type' : 'place'}, {'entity': 'France', 'type' : 'place'}]\n",
    "\n",
    "Text: Jim lives in texas and he owns a cat named Alexa\n",
    "Answer: [{'entity': 'Jim', 'type':'person'}, {'entity': 'texas', 'type': 'place'}, {'entity': 'alexa', 'type': 'cat'}]\n",
    "\n",
    "Text: john travelled with his dog duffy from Texas to Turkey and met with his friend abdul\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "client.text_generation(prompt, max_new_tokens=75, stop_sequences=[\"]\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a19c06",
   "metadata": {},
   "source": [
    "## Few -Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2e33d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Answer: [{'entity': 'john', 'type':'person'}, {'entity': 'duffy', 'type': 'animal'}, {'entity': 'Abdul', 'type': 'person'}]\\n\\n\\nHere is the code to achieve the desired result:\\n\\n```\\nimport requests\\n\\ndef get_entities\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Return a list of named entities and their types in the text.\n",
    "\n",
    "Return a list of named entities and their types in the text.\n",
    "\n",
    "Format response as JSON. Stop generating once valid JSON is created.\n",
    "\n",
    "Text: Anil lives in Paris the most beautiful city in France\n",
    "Answer: [{'entity': 'Anil', 'type': 'person'}, {'entity': 'Paris', 'type' : 'place'}, {'entity': 'France', 'type' : 'place'}]\n",
    "\n",
    "Text: Jim lives in texas and he owns a cat named Alexa\n",
    "Answer: [{'entity': 'Jim', 'type':'person'}, {'entity': 'texas', 'type': 'place'}, {'entity': 'alexa', 'type': 'cat'}]\n",
    "\n",
    "Text: Mary loves her hometown in Calfiornia, where she lives with her husband Paul\n",
    "Answer: [{'entity': 'Mary', 'type':'person'}, {'entity': 'Calfiornia', 'type': 'state'},{'entity': 'Paul', 'type':'person'}]\n",
    "\n",
    "Text: Jim is friends with Brad  who lives in LA in California\n",
    "Answer: [{'entity': 'Jim', 'type':'person'}, {'entity': 'LA', 'type': 'city'}, {'entity': 'Calfiornia', 'type': 'state'}{'entity': 'Brad', 'type':'person'}]\n",
    "\n",
    "\n",
    "Text: john travelled with his dog duffy from New Jersey to Turkey and met with his friend abdul\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "client.text_generation(prompt, max_new_tokens=75, stop_sequences=[\"]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"using the LLM you can build intelligent apps\"\n",
    "\n",
    "\n",
    "prompt = f\"what category out of these ['news', 'health', 'finance'], does this article belong to : \"\n",
    "\n",
    "client.text_generation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f141155b",
   "metadata": {},
   "source": [
    "## Limitation with few-shot technique\n",
    "\n",
    "### Few shot may not produce the desired result for reasoning/mathematics/logical problems\n",
    "\n",
    "Try out this prompt with \n",
    "\n",
    "* Chat GPT 3.5\n",
    "\n",
    "* Chere Command (use playground)\n",
    "\n",
    "* Google Bard   (uses Gemini AI / LaMDA model)\n",
    "\n",
    "You will realize that it works for Bard and it doesn't work for Chat GPT 3.5 or Cohere Command!!!\n",
    "\n",
    "Do your research to figure out why is that the case. Share your results on the Q&A forum."
   ]
  },
  {
   "cell_type": "raw",
   "id": "30ad93de",
   "metadata": {},
   "source": [
    "refer to examples and answer generate a yes or a no as response to the question.\n",
    "\n",
    "example: the odd numbers in this list [1,3,4,5,2] adds up to an odd number\n",
    "answer: yes\n",
    "\n",
    "example: the odd numbers in this list [1,2,4,5,6,2] adds up to an odd number\n",
    "answer: no\n",
    "\n",
    "example: the odd numbers in this list [1,2,4,7,9] adds up to an odd number\n",
    "answer: yes\n",
    "\n",
    "example: the odd numbers in this list [1,3,4,7,9] adds up to an odd number\n",
    "answer: no\n",
    "\n",
    "question: the odd numbers in this list [1,2,1,9,1] adds up to an odd number\n",
    "answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c163b3",
   "metadata": {},
   "source": [
    "## Create a cohere client\n",
    "\n",
    "Build a few shot prompt in an incremental fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6222e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "co = cohere.Client(COHERE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9bf91",
   "metadata": {},
   "source": [
    "### 0 Shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1418e492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Civic, kayak, level, lake'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Palindrome is a word that reads the same forward and backward. \n",
    "\n",
    "Your task is to identify palindrome words in the given sentence. \n",
    "\n",
    "sentence: {}\n",
    "palindromes:\n",
    "\n",
    "generate response in this format: palindrome1, palindrome2, ...\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"Civic authorities drove the kayak across the level lake.\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command',\n",
    "  prompt=prompt.format(input_text),\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58f5af",
   "metadata": {},
   "source": [
    "### Few shots (Provide palindrome words as examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a70c006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' civic authorities, lake level, drove across, kayak across'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Palindrome is a word that reads the same forward and backward. \n",
    "\n",
    "Here are examples of palindrome words \n",
    "\n",
    "examples: radar, rotor, reviver, madam\n",
    "\n",
    "Your task is to identify palindrome words in the given sentence. \n",
    "\n",
    "sentence: {}\n",
    "palindromes:\n",
    "\n",
    "generate response in this format: palindrome1, palindrome2, ...\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"Civic authorities drove the kayak across the level lake.\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command',\n",
    "  prompt=prompt.format(input_text),\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f477e89c",
   "metadata": {},
   "source": [
    "### Few shots (Provide sentences with palindromes as examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Palindrome is a word that reads the same forward and backward. \n",
    "\n",
    "Your task is to identify palindrome words in the given sentence. \n",
    "Here are some examples.\n",
    "\n",
    "sentence: Noon is a time when racecar enthusiasts gather.\n",
    "palindromes: noon, racecar\n",
    "\n",
    "sentence: Deed records show a radar signal in the evening.\n",
    "palindromes: deed, radar\n",
    "\n",
    "sentence: Reviver potions can revive a tired system\n",
    "palindromes: reviver\n",
    "\n",
    "sentence: rotors rotate, making a subtle rotor sound\n",
    "palindromes: rotor\n",
    "\n",
    "\n",
    "sentence: {}\n",
    "palindromes:\n",
    "\n",
    "generate response in this format: palindrome1, palindrome2, ...\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"Civic authorities drove the kayak across the level lake.\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command-light',\n",
    "  prompt=prompt.format(input_text),\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Palindrome is a word that reads the same forward and backward. \n",
    "\n",
    "Your task is to identify palindrome words in the given sentence. \n",
    "\n",
    "text: {}\n",
    "palindromes:\n",
    "\n",
    "generate response in this format: palindrome1, palindrome2, ...\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"Civic authorities drove the kayak across the level lake.\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command-light',\n",
    "  prompt=prompt.format(input_text),\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf9450",
   "metadata": {},
   "source": [
    "### WAIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbcf0f41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' lake, kayak, across, authorities, drive'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "use examples below to extract all palindrome words from the given text. Just give the words\n",
    "\n",
    "example: Noon is a time when racecar enthusiasts gather.\n",
    "answer: noon, racecar\n",
    "\n",
    "example: Deed records show a radar signal in the evening.\n",
    "answer: deed\n",
    "\n",
    "example: Madam Anna made a beautiful mural for the wall\n",
    "answer: madam\n",
    "\n",
    "\n",
    "text: Civic authorities drove the kayak across the level lake\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command-light',\n",
    "  prompt=prompt,\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output\n",
    "\n",
    "\n",
    "Palindrome is a word that reads the same forward and backward. \n",
    "\n",
    "There may be more than one palindrome words in a sentence, so list all words.\n",
    "\n",
    "Use examples below for reference to identify all palindrome in given text.\n",
    "\n",
    "example: Noon is a time when racecar enthusiasts gather.\n",
    "answer: noon, racecar\n",
    "\n",
    "example: Deed records show a radar signal in the evening.\n",
    "answer: deed, radar\n",
    "\n",
    "example: Madam Anna made a beautiful mural for the wall\n",
    "answer: madam\n",
    "\n",
    "example: Reviver potions can revive a tired system\n",
    "answer: reviver\n",
    "\n",
    "example: rotors rotate, making a subtle rotor sound\n",
    "example: rotor\n",
    "\n",
    "text: Civic authorities drove the kayak across the lake\n",
    "answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01f4d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'google/flan-t5-xxl',\n",
    "    'EleutherAI/gpt-neox-20b'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc54047",
   "metadata": {},
   "source": [
    "## Classification with few shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c69e870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = InferenceClient(model=hugging_face_model_ids[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "148dae5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext: The man was a good citizen.\\nanswer: man\\n\\ntext: The man'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "extract palindrome words from the given text\n",
    "\n",
    "text: Noon is a time when racecar enthusiasts gather.\n",
    "answer: noon, racecar\n",
    "\n",
    "text: Deed records show a radar signal in the evening.\n",
    "answer: deed\n",
    "\n",
    "text: Madam Anna made a civic mural on the wall\n",
    "answer: madam \n",
    "\n",
    "\n",
    "text: Civic duty drove the kayak across the level lake\n",
    "answer:\n",
    "\"\"\"\n",
    "\n",
    "llm.text_generation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9d5e1",
   "metadata": {},
   "source": [
    "## Zero shot model - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8907470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = InferenceClient(model=hugging_face_model_ids[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5642bbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n* Noon\\n* racecar\\n\\n(Note: case is ignored, so \"No'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "[INST]extract all palindrome words from the given text. Just give the words[/INST]\n",
    "\n",
    "text: Noon is a time when racecar enthusiasts gather.\n",
    "answer: \n",
    "\"\"\"\n",
    "\n",
    "llm.text_generation(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbbdc3",
   "metadata": {},
   "source": [
    "## Limitation of few shots\n",
    "\n",
    "Example demonstrates the limitation of few-shots technique with math/reasoning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "494869d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = InferenceClient(model=hugging_face_model_ids[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "37d5f302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "generate a yes or a no as response. \n",
    "\n",
    "the odd numbers in this list [1,2,1,9,1] add up to an odd number \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command',\n",
    "  prompt=prompt.format(input_text),\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aa72bfea",
   "metadata": {},
   "outputs": [
    {
     "ename": "CohereAPIError",
     "evalue": "You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCohereAPIError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mrefer to examples and answer generate a yes or a no as response. \u001b[39m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcommand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m  \u001b[49m\u001b[43mreturn_likelihoods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNONE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m output \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     27\u001b[0m output\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\cohere\\client.py:223\u001b[0m, in \u001b[0;36mClient.generate\u001b[1;34m(self, prompt, prompt_vars, model, preset, num_generations, max_tokens, temperature, k, p, frequency_penalty, presence_penalty, end_sequences, stop_sequences, return_likelihoods, truncate, logit_bias, stream)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate endpoint.\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mSee https://docs.cohere.ai/reference/generate for advanced arguments\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m        >>>     print(token)\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m json_body \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    222\u001b[0m }\n\u001b[1;32m--> 223\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcohere\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERATE_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StreamingGenerations(response)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\cohere\\client.py:941\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, endpoint, json, files, method, stream, params)\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m jsonlib\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mJSONDecodeError:  \u001b[38;5;66;03m# CohereAPIError will capture status\u001b[39;00m\n\u001b[0;32m    939\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError\u001b[38;5;241m.\u001b[39mfrom_response(response, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to decode json body: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m json_response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\cohere\\client.py:883\u001b[0m, in \u001b[0;36mClient._check_response\u001b[1;34m(self, json_response, headers, status_code)\u001b[0m\n\u001b[0;32m    881\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-API-Warning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m json_response:  \u001b[38;5;66;03m# has errors\u001b[39;00m\n\u001b[1;32m--> 883\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[0;32m    884\u001b[0m         message\u001b[38;5;241m=\u001b[39mjson_response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    885\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[0;32m    886\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    887\u001b[0m     )\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m400\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[0;32m    889\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CohereAPIError(\n\u001b[0;32m    890\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected client error (status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    891\u001b[0m         http_status\u001b[38;5;241m=\u001b[39mstatus_code,\n\u001b[0;32m    892\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    893\u001b[0m     )\n",
      "\u001b[1;31mCohereAPIError\u001b[0m: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions"
     ]
    }
   ],
   "source": [
    "prompt =\"\"\"\n",
    "refer to examples and answer generate a yes or a no as response. \n",
    "\n",
    "examples: the odd numbers in this list [1,3,4,5,2] adds up to an odd number//yes\n",
    "\n",
    "examples: the odd numbers in this list [1,2,4,5,6] adds up to an odd number//no\n",
    "\n",
    "examples: the odd numbers in this list [1,2,4,7,9] adds up to an odd number//yes \n",
    "\n",
    "examples: the odd numbers in this list [1,3,4,7,9] adds up to an odd number//no \n",
    "\n",
    "examples: the odd numbers in this list [1,2,1,9,2] adds up to an odd number//\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = co.generate(\n",
    "  model='command',\n",
    "  prompt=prompt.format(input_text),\n",
    "  max_tokens=50,\n",
    "  temperature=0.3,\n",
    "  k=0,\n",
    "  stop_sequences=[],\n",
    "  return_likelihoods='NONE')\n",
    "\n",
    "output = response.generations[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664bc44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
