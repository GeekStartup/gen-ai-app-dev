{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03941d",
   "metadata": {},
   "source": [
    "# Exercise Solution : Prompt Engineering Best Practices\n",
    "## General Best Practices\n",
    "\n",
    "1. Separation between parts\n",
    "2. Detailed, clear & specific instructions\n",
    "3. Avoid ambiguity \n",
    "4. Favor positive instructions\n",
    "5. Instruct the model to play a role\n",
    "6. Provide trusted information\n",
    "7. Specify response characteristics\n",
    "8. Set guardrails for response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b1b0c",
   "metadata": {},
   "source": [
    "## Change the location of the environment file before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a210debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dac56e",
   "metadata": {},
   "source": [
    "## Setup the models available for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f3cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'google/flan-t5-xxl'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8f096",
   "metadata": {},
   "source": [
    "## 1. Guide the model to avoid hallucinations\n",
    "\n",
    "Add the guidance to not to make up a response.\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[2]    \n",
    "\n",
    "'openlm-research/open_llama_3b_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be60250",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_bad  = \"\"\"as of 2023, who is the prime minister of UK\"\"\"\n",
    "\n",
    "prompt_good = \"\"\"as of 2023, who is the prime minister of UK\n",
    "\n",
    "Do not make up an answer, if you do not know the answer say \"I don't know\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "llm = InferenceClient(model=hugging_face_model_ids[2])\n",
    "\n",
    "llm.text_generation(prompt_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22cc49d",
   "metadata": {},
   "source": [
    "## 2. Instruct the model to play roles\n",
    "\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[3],\n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "438b8916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without role:  Large Language Models (LLMs) are a type of machine translation system that can handle large amounts of text.\n",
      "With role:  Large Language Models (LLMs) are a class of neural network models that are designed to capture the structure of natural language.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"question:what are large language models. answer:\"\n",
    "\n",
    "instruction_roles = [\n",
    "    \"you are a 5th grade science teacher answer the question below. {}\",\n",
    "    \"you are a college level professor of computer science answer the question below. {}\",\n",
    "    \"you are a doctorate of computer science answer the question below. {}\"\n",
    "]\n",
    "\n",
    "llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "\n",
    "without_role = llm.text_generation(prompt, max_new_tokens=100)\n",
    "\n",
    "print(\"Without role: \", without_role)\n",
    "\n",
    "role_index = 2\n",
    "\n",
    "with_role = llm.text_generation(instruction_roles[role_index].format(prompt), max_new_tokens=100)\n",
    "\n",
    "print(\"With role: \", with_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebf2f0",
   "metadata": {},
   "source": [
    "## 3. Guide the model to generate precise response\n",
    "\n",
    "### Default model\n",
    "hugging_face_model_ids[3], \n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf2061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without context:  Â£200\n",
      "----------------------------------------------\n",
      "Response with context:  $899.99\n"
     ]
    }
   ],
   "source": [
    "prompt_samples = [\n",
    "    \"how much would be the cost of 'Digital Camera DSLR'.\",\n",
    "    \"share details of home security camera\",\n",
    "    \"do you sell ceiling fan?\",\n",
    "    \"share details of Robot Cleaner?\",\n",
    "]\n",
    "\n",
    "with open('./data/acme-product-catalog.txt') as f:\n",
    "    product_catalog = f.read()\n",
    "\n",
    "\n",
    "prompt_good = \"\"\" To answer the question below, Use the information provided in product-catalog. \n",
    "If you don't find the product in the context the say 'sorry, we don't sell it'\n",
    "\n",
    "question: {}\n",
    "product-catalog:\\n {}\"\"\"\n",
    "\n",
    "llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "\n",
    "# Change the index to try out different models\n",
    "prompt_index = 0\n",
    "\n",
    "no_context_prompt = prompt_samples[prompt_index]\n",
    "with_context_prompt = prompt_good.format(prompt_samples[prompt_index], product_catalog)\n",
    "\n",
    "# print(with_context_prompt)\n",
    "\n",
    "no_context_response = llm.text_generation(no_context_prompt)\n",
    "\n",
    "with_context_response = llm.text_generation(with_context_prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Response without context: \", no_context_response)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Response with context: \", with_context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5564",
   "metadata": {},
   "source": [
    "## 4. Address bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcabcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without guardrails:  \n",
      "\n",
      "No, I do not agree. Hiring decisions should be based on qualifications, skills\n",
      "----------------------------------------------\n",
      "Response with guardrails:  I strongly disagree with the notion that engineering jobs should only be filled by men 100%\n"
     ]
    }
   ],
   "source": [
    "prompt = \"for the engineering jobs we should only hire men 100% of the time, do you agree? \"\n",
    "\n",
    "prompt_with_guardrails = \"\"\"You are a human resources expert. You always assist with care, respect, and truth.\n",
    "Your answers promote fairness and positivity. .\n",
    "\n",
    "question: {}\n",
    "\n",
    "answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "llm = InferenceClient(model=hugging_face_model_ids[1])  \n",
    "\n",
    "without_guardrails = llm.text_generation(prompt)\n",
    "\n",
    "with_guardrails = llm.text_generation(prompt_with_guardrails.format(prompt))\n",
    "\n",
    "print(\"Response without guardrails: \", without_guardrails)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Response with guardrails: \", with_guardrails)\n",
    "\n",
    "# prompt_with_guardrails.format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d9f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
