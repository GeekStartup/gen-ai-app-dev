{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03941d",
   "metadata": {},
   "source": [
    "# Exercise Solution : Prompt Engineering Best Practices\n",
    "## General Best Practices\n",
    "\n",
    "1. Separation between parts\n",
    "2. Detailed, clear & specific instructions\n",
    "3. Avoid ambiguity \n",
    "4. Favor positive instructions\n",
    "5. Instruct the model to play a role\n",
    "6. Provide trusted information\n",
    "7. Specify response characteristics\n",
    "8. Set guardrails for response\n",
    "\n",
    "**Note**\n",
    "\n",
    "* You may run into 503 issues, wait for some time and try again\n",
    "* Use a different model in case of 503 that doesnt get resolved\n",
    "* Keep in mind models may behave differently !! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f28ef-30c2-4a94-85a7-eb20c2c4f238",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the HF API Token\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f09a21-53a7-4d83-a153-ab3de2e32122",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The script is downloaded and run to setup the utils folder\n",
    "\n",
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh\n",
    "# !mv ./gen-ai-app-dev/Models/data ./data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b1b0c",
   "metadata": {},
   "source": [
    "## Change the location of the environment file before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a210debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806ce6df-e8ee-410a-b368-82b69274b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  HUGGINGFACEHUB_API_TOKEN  already set in environment.\n"
     ]
    }
   ],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dac56e",
   "metadata": {},
   "source": [
    "## Setup the models available for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f3cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'google/flan-t5-xxl'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8f096",
   "metadata": {},
   "source": [
    "## 1. Guide the model to avoid hallucinations\n",
    "\n",
    "Add the guidance to not to make up a response.\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[2]    \n",
    "\n",
    "'openlm-research/open_llama_3b_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be60250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'as of 2023, who is the prime minister of UK\\n\\nDo not make up an answer, if you do not know the answer say \"I don\\'t know\"\\n\\n\\nA: The Prime Minister of the United Kingdom is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime Minister.\\nThe Prime Minister is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime Minister.\\nThe Prime Minister is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime Minister.\\nThe Prime Minister is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_bad  = \"\"\"as of 2023, who is the prime minister of UK\"\"\"\n",
    "\n",
    "# Fixed prompt\n",
    "prompt_good = \"\"\"as of 2023, who is the prime minister of UK\n",
    "\n",
    "Do not make up an answer, if you do not know the answer say \"I don't know\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "## Inference Client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[2])\n",
    "# llm.text_generation(prompt_good)\n",
    "\n",
    "## HTTP Post\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[2])\n",
    "llm_client.invoke(prompt_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22cc49d",
   "metadata": {},
   "source": [
    "## 2. Instruct the model to play roles\n",
    "\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[3],\n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"question:what are large language models. answer:\"\n",
    "\n",
    "instruction_roles = [\n",
    "    \"you are a 5th grade science teacher answer the question below. {}\",\n",
    "    \"you are a college level professor of computer science answer the question below. {}\",\n",
    "    \"you are a doctorate of computer science answer the question below. {}\"\n",
    "]\n",
    "\n",
    "## Inference Client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "# without_role = llm.text_generation(prompt, max_new_tokens=100)\n",
    "# print(\"Without role: \", without_role)\n",
    "\n",
    "## try different roles for LLM\n",
    "# role_index = 2\n",
    "# with_role = llm.text_generation(instruction_roles[role_index].format(prompt), max_new_tokens=100)\n",
    "# print(\"With role: \", with_role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af68d01-5a05-4b0b-afde-10bc82b7e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without role:  [{'generated_text': 'question:what are large language models. answer: Large language models are artificial intelligence (AI) systems designed to generate human-like text based on the input they receive. These models are trained on vast amounts of text data, allowing them to understand and generate language with a high degree of accuracy and fluency. They can be used for various applications, such as text completion, translation, summarization, and chatbots. Some popular large language models include BERT, GPT-3, and T5. These models are called \"large\"'}]\n",
      "With role:  [{'generated_text': 'you are a doctorate of computer science answer the question below. question:what are large language models. answer: Large language models are artificial intelligence systems designed to generate human-like text based on the input they receive. These models are trained on vast amounts of text data, allowing them to understand and generate language with a high degree of fluency and accuracy. They use deep learning techniques, specifically a type of neural network called a transformer, to analyze the input data and generate appropriate responses. These models can be used for various applications such as text generation, translation, summarization, and question answering. They have'}]\n"
     ]
    }
   ],
   "source": [
    "## HTTP Post\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[3])\n",
    "without_role = llm_client.invoke(prompt)\n",
    "print(\"Without role: \", without_role)\n",
    "\n",
    "## try different roles for LLM\n",
    "role_index = 2\n",
    "with_role = llm_client.invoke(instruction_roles[role_index].format(prompt))\n",
    "print(\"With role: \", with_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebf2f0",
   "metadata": {},
   "source": [
    "## 3. Guide the model to generate precise response\n",
    "\n",
    "### Default model\n",
    "hugging_face_model_ids[3], \n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2061bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl (Request ID: BxWC4AaufSlLIQeOl5yaY)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     24\u001b[0m with_context_prompt \u001b[38;5;241m=\u001b[39m prompt_good\u001b[38;5;241m.\u001b[39mformat(prompt_samples[prompt_index], product_catalog)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(with_context_prompt)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m no_context_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mtext_generation(no_context_prompt)\n\u001b[0;32m     30\u001b[0m with_context_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mtext_generation(with_context_prompt)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse without context: \u001b[39m\u001b[38;5;124m\"\u001b[39m, no_context_response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2060\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001b[0;32m   2037\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_generation(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2038\u001b[0m             prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m   2039\u001b[0m             details\u001b[38;5;241m=\u001b[39mdetails,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2058\u001b[0m             watermark\u001b[38;5;241m=\u001b[39mwatermark,\n\u001b[0;32m   2059\u001b[0m         )\n\u001b[1;32m-> 2060\u001b[0m     raise_text_generation_error(e)\n\u001b[0;32m   2062\u001b[0m \u001b[38;5;66;03m# Parse output\u001b[39;00m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\inference\\_common.py:460\u001b[0m, in \u001b[0;36mraise_text_generation_error\u001b[1;34m(http_error)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp_error\u001b[39;00m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;66;03m# Otherwise, fallback to default error\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m http_error\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:2031\u001b[0m, in \u001b[0;36mInferenceClient.text_generation\u001b[1;34m(self, prompt, details, stream, model, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001b[0m\n\u001b[0;32m   2029\u001b[0m \u001b[38;5;66;03m# Handle errors separately for more precise error messages\u001b[39;00m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2031\u001b[0m     bytes_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost(json\u001b[38;5;241m=\u001b[39mpayload, model\u001b[38;5;241m=\u001b[39mmodel, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, stream\u001b[38;5;241m=\u001b[39mstream)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2033\u001b[0m     match \u001b[38;5;241m=\u001b[39m MODEL_KWARGS_NOT_USED_REGEX\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     hf_raise_for_status(response)\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/google/flan-t5-xxl (Request ID: BxWC4AaufSlLIQeOl5yaY)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "prompt_samples = [\n",
    "    \"how much would be the cost of 'Digital Camera DSLR'.\",\n",
    "    \"share details of home security camera\",\n",
    "    \"do you sell ceiling fan?\",\n",
    "    \"share details of Robot Cleaner?\",\n",
    "]\n",
    "\n",
    "with open('./data/acme-product-catalog.txt') as f:\n",
    "    product_catalog = f.read()\n",
    "\n",
    "\n",
    "prompt_good = \"\"\" To answer the question below, Use the information provided in product-catalog. \n",
    "If you don't find the product in the context the say 'sorry, we don't sell it'\n",
    "\n",
    "question: {}\n",
    "product-catalog:\\n {}\"\"\"\n",
    "\n",
    "llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "\n",
    "# Change the index to try out different models\n",
    "prompt_index = 0\n",
    "\n",
    "no_context_prompt = prompt_samples[prompt_index]\n",
    "with_context_prompt = prompt_good.format(prompt_samples[prompt_index], product_catalog)\n",
    "\n",
    "# print(with_context_prompt)\n",
    "\n",
    "no_context_response = llm.text_generation(no_context_prompt)\n",
    "\n",
    "with_context_response = llm.text_generation(with_context_prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Response without context: \", no_context_response)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Response with context: \", with_context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5564",
   "metadata": {},
   "source": [
    "## 4. Address bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcabcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without guardrails:  \n",
      "\n",
      "No, I do not agree. Hiring decisions should be based on qualifications, skills\n",
      "----------------------------------------------\n",
      "Response with guardrails:  I strongly disagree with the notion that engineering jobs should only be filled by men 100%\n"
     ]
    }
   ],
   "source": [
    "prompt = \"for the engineering jobs we should only hire men 100% of the time, do you agree? \"\n",
    "\n",
    "prompt_with_guardrails = \"\"\"You are a human resources expert. You always assist with care, respect, and truth.\n",
    "Your answers promote fairness and positivity. .\n",
    "\n",
    "question: {}\n",
    "\n",
    "answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "llm = InferenceClient(model=hugging_face_model_ids[1])  \n",
    "\n",
    "without_guardrails = llm.text_generation(prompt)\n",
    "\n",
    "with_guardrails = llm.text_generation(prompt_with_guardrails.format(prompt))\n",
    "\n",
    "print(\"Response without guardrails: \", without_guardrails)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Response with guardrails: \", with_guardrails)\n",
    "\n",
    "# prompt_with_guardrails.format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d9f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
