{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf34c90-c493-4556-9ae4-4bf1e8357e74",
   "metadata": {},
   "source": [
    "# Merger Retriever a.k.a. Lord of the Retrievers (LOTR)\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/retrievers/merger_retriever/\n",
    "\n",
    "* Takes a list of retrievers\n",
    "* Merges the results into a single list\n",
    "\n",
    "The startegy improves the accuracy.\n",
    "\n",
    "* Reduces the risk of bias\n",
    "* Ranks the results\n",
    "\n",
    "#### Demo:\n",
    "* ChromaDB vector store\n",
    "* WikipediaRetriever\n",
    "* RedundantFilterCompressor\n",
    "* LongContet\n",
    "\n",
    "**PS:** Requires the Wikipedia package\n",
    "https://python.langchain.com/v0.1/docs/integrations/retrievers/wikipedia/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92b607-8e13-4274-8b72-865a23a71efc",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60596289-def4-4966-824f-e332498799b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Retrievers & transformers\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers import MergerRetriever\n",
    "from langchain_community.document_transformers import EmbeddingsClusteringFilter\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "# Embeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd3d1b-349e-4d1d-b5c2-caebbd5bfcbd",
   "metadata": {},
   "source": [
    "## 1. Create an LLM\n",
    "The LLM will be used by the retrievers\n",
    "\n",
    "* Cohere command model\n",
    "* Cohere embedding model\n",
    "\n",
    "#### Note\n",
    "* You must adjust the location of the API key file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44cf261f-b344-40bd-8d60-f730e22e40f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Load the file that contains the API keys - OPENAI_API_KEY\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_chat_llm import create_gpt_chat_llm, create_cohere_chat_llm\n",
    "\n",
    "# Try with GPT\n",
    "llm = create_cohere_chat_llm()\n",
    "\n",
    "llm_embeddings = CohereEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7181d-e3f2-4089-ab19-0b96a26b5b43",
   "metadata": {},
   "source": [
    "## 2. Utility function\n",
    "\n",
    "* Prints the size information\n",
    "* Pretty prints the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b772a72-c89e-4d0f-851a-53b3999252f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_documents(docs):\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(\"#\",i)\n",
    "        print(doc.page_content)\n",
    "\n",
    "def dump_results_info(result):\n",
    "    print(\"Doc count = \", len(result))\n",
    "    page_content_length=0\n",
    "    for doc in result:\n",
    "        page_content_length = page_content_length + len(doc.page_content)\n",
    "    print(\"Context size = \", page_content_length)\n",
    "    print_documents(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441990c-2fd1-4ad1-a94b-24a587d70fb3",
   "metadata": {},
   "source": [
    "## 3. Setup VectorDB retrievers\n",
    "\n",
    "* Create 2 vector stores with different chunks sizes (200 & 500)\n",
    "* using ChromaDB as a retriever "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8411973-bbeb-49c9-8a8f-541ad05f5bd2",
   "metadata": {},
   "source": [
    "#### Chunk size = 200, search_type=similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70aa4d51-32d3-4510-b580-8174656a9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Chroma vector store #1 \n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store_1 = Chroma(collection_name=\"rag_documents\", embedding_function=embedding_function) \n",
    "\n",
    "# Load sample docs\n",
    "loader = DirectoryLoader('./util', glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunking\n",
    "doc_splitter_1 = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "chunked_documents_1 = doc_splitter_1.split_documents(docs)\n",
    "\n",
    "# Add to vector DB\n",
    "vector_store_1.add_documents(chunked_documents_1)\n",
    "\n",
    "# Base retrievers\n",
    "vector_store_retriever_1 = vector_store_1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269e481-287e-491e-b816-4400b9118684",
   "metadata": {},
   "source": [
    "#### Chunk size = 500, search_type=mmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb512453-de66-4e1f-a919-d2242e774208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Chroma vector store #2 \n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vector_store_2 = Chroma(collection_name=\"rag_documents\", embedding_function=embedding_function) \n",
    "\n",
    "# Chunking\n",
    "doc_splitter_2 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunked_documents_2 = doc_splitter_2.split_documents(docs)\n",
    "\n",
    "# Add to vector DB\n",
    "vector_store_2.add_documents(chunked_documents_2)\n",
    "\n",
    "# Base retrievers\n",
    "vector_store_retriever_2 = vector_store_2.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abdfaa-4bd5-4af5-a3df-015ad2d88e14",
   "metadata": {},
   "source": [
    "## 4. Wikipedia Retriever\n",
    "\n",
    "* Use Wikipedia to get the documents of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e98474-b6fa-4306-ae94-019497936bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(search_kwargs={\"k\": 5}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313a6c0-4407-42ad-b946-d27166a84a3f",
   "metadata": {},
   "source": [
    "## 5. Combine retrievers using Merger Retriver\n",
    "\n",
    "https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.merger_retriever.MergerRetriever.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39dd2f3d-1cc6-4111-96b8-6540251e5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The 2 vector retriever and 1 wiki retriever\n",
    "merger_retriever = MergerRetriever(retrievers=[vector_store_retriever_1,vector_store_retriever_2, wikipedia_retriever])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30fe17-0230-4cec-8e03-8893a68ebf39",
   "metadata": {},
   "source": [
    "## 6. Apply document compressor\n",
    "\n",
    "https://api.python.langchain.com/en/latest/document_transformers/langchain_community.document_transformers.embeddings_redundant_filter.EmbeddingsClusteringFilter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d604adf-ae6b-4a66-a5cc-bf43090b8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create embedding clustering filter\n",
    "filter_ordered_by_retriever = EmbeddingsClusteringFilter(\n",
    "    embeddings=llm_embeddings,\n",
    "    num_clusters=5,\n",
    "    num_closest=1,\n",
    "    sorted=True,\n",
    ")\n",
    "\n",
    "# Create document compressor pipeline\n",
    "pipeline = DocumentCompressorPipeline(transformers=[filter_ordered_by_retriever])\n",
    "\n",
    "# Create compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline, base_retriever=merger_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04a2ccb-6093-4fe3-9814-81acbf28eb71",
   "metadata": {},
   "source": [
    "## 7. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ddaad-74bf-48ee-9bf3-9c5a0d87c6e8",
   "metadata": {},
   "source": [
    "##### Output from merger_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84a05f9f-b865-4844-bdaf-b9dfa53b9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc count =  13\n",
      "Context size =  9672\n",
      "# 0\n",
      "Retrieval augmented generation (RAG)\n",
      "\n",
      "Retrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model’s training knowledge, AI apps architected for RAG can search your data for information relevant to a query, then pass that information into the prompt. This is similar to prompt engineering, except that the system can find and retrieve new context from your data with each interaction.\n",
      "# 1\n",
      "Retrieval augmented generation (RAG)\n",
      "\n",
      "Retrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model’s training knowledge, AI apps architected for RAG can search your data for information relevant to a query, then pass that information into the prompt. This is similar to prompt engineering, except that the system can find and retrieve new context from your data with each interaction.\n",
      "# 2\n",
      "Prompt engineering is the process of structuring an instruction that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\n",
      "A prompt for a text-to-text language model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison → house, chat → cat, chien →\" (the expected response being dog), an approach called few-shot learning.\n",
      "When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
      "\n",
      "\n",
      "== In-context learning ==\n",
      "Prompt engineering is enabled by in-context learning, defined as a model's ability to temporarily learn from prompts. The ability for in-context learning is an emergent ability of large language models. In-context learning itself is an emergent property of model scale, meaning breaks in downstream scaling laws occur such that its efficacy increases at a different rate in larger models than in smaller models.\n",
      "In contrast to training and fine-tuning for each specific task, which are not temporary, what has been learnt during in-context learning is of a temporary nature. It does not carry the temporary contexts or biases, except the ones already present in the (pre)training dataset, from one conversation to the other. This result of \"mesa-optimization\" within transformer layers, is a form of meta-learning or \"learning to learn\".\n",
      "\n",
      "\n",
      "== History ==\n",
      "In 2018, researchers first proposed that all previously separate tasks in NLP could be cast as a question answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like \"What is the sentiment\" or \"Translate this sentence to German\" or \"Who is the president?\"\n",
      "In 2021, researchers fine-tuned one generatively pretrained model (T0) on performing 12 NLP tasks (using 62 datasets, as each task can have multiple datasets). The model showed good performance on new tasks, surpassing models trained directly on just performing one task (without pretraining). To solve a task, T0 is given the task in a structured prompt, for example If {{premise}} is true, is it also true that {{hypothesis}}? ||| {{entailed}}. is the prompt used for making T0 solve entailment.\n",
      "A repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022.\n",
      "In 2022 the chain-of-thought prompting technique was proposed by Google researchers.\n",
      "In 2023 several text-to-text and text-to-image prompt databases were publicly available.\n",
      "\n",
      "\n",
      "== Text-to-text ==\n",
      "\n",
      "\n",
      "=== Chain-of-thought ===\n",
      "Chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. Chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought. It allows large language models to overcome difficulties with some reasoning tasks that require logical thinking and multiple steps to solve, such as arithmetic or commonsense reasoning questions.\n",
      "For example, given the question \"Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\", a CoT prompt might indu\n",
      "# 3\n",
      "Title\n",
      "\n",
      "What is generative AI?\n",
      "\n",
      "Source\n",
      "\n",
      "https://aws.amazon.com/what\n",
      "\n",
      "is/generative\n",
      "\n",
      "ai/\n",
      "# 4\n",
      "Title\n",
      "\n",
      "What is generative AI?\n",
      "\n",
      "Source\n",
      "\n",
      "https://aws.amazon.com/what\n",
      "\n",
      "is/generative\n",
      "\n",
      "ai/\n",
      "# 5\n",
      "A vector database, vector store or vector search engine is a database that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more Approximate Nearest Neighbor (ANN) algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\n",
      "Vector databases can be used for similarity search, multi-modal search, recommendations engines, large language models (LLMs), etc.\n",
      "Vector databases are also often used to implement Retrieval-Augmented Generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n",
      "\n",
      "\n",
      "== Techniques ==\n",
      "The most important techniques for similarity search on high-dimensional vectors include:\n",
      "\n",
      "Hierarchical Navigable Small World (HNSW) graphs\n",
      "Locality-sensitive Hashing (LSH) and Sketching\n",
      "Product Quantization (PQ)\n",
      "Inverted Files\n",
      "and combinations of these techniques.\n",
      "In recent benchmarks, HNSW-based implementations have been among the best performers. Conferences such as the International Conference on Similarity Search and Applications, SISAP and the Conference on Neural Information Processing Systems (NeurIPS) host competitions on vector search in large databases.\n",
      "\n",
      "\n",
      "== Implementations ==\n",
      "\n",
      "\n",
      "== See also ==\n",
      "Curse of dimensionality – Difficulties arising when analyzing data with many aspects (\"dimensions\")\n",
      "Machine learning – Study of algorithms that improve automatically through experience\n",
      "Nearest neighbor search – Optimization problem in computer science\n",
      "Recommender system – Information filtering system to predict users' preferences\n",
      "\n",
      "\n",
      "== References ==\n",
      "\n",
      "\n",
      "== External links ==\n",
      "Sawers, Paul (2024-04-20). \"Why vector databases are having a moment as the AI hype cycle peaks\". TechCrunch. Retrieved 2024-04-23.\n",
      "# 6\n",
      "Title\n",
      "\n",
      "What is generative AI?\n",
      "\n",
      "Source\n",
      "\n",
      "https://aws.amazon.com/what\n",
      "\n",
      "is/generative\n",
      "\n",
      "ai/\n",
      "# 7\n",
      "Generative AI can boost productivity for different kinds of workers:\n",
      "# 8\n",
      "This is a timeline of Amazon Web Services, which offers a suite of cloud computing services that make up an on-demand computing platform.\n",
      "\n",
      "\n",
      "== AWS Prelude ==\n",
      "\n",
      "\n",
      "== Full timeline ==\n",
      "\n",
      "\n",
      "== Partnerships ==\n",
      "\n",
      "\n",
      "== Amazon Web Services outages ==\n",
      "\n",
      "\n",
      "== See also ==\n",
      "List of Amazon products and services\n",
      "History of Amazon\n",
      "\n",
      "\n",
      "== References ==\n",
      "# 9\n",
      "Retrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model’s training knowledge, AI apps architected for RAG can search your data\n",
      "# 10\n",
      "For example, here's what generative AI can do for optimization:\n",
      "# 11\n",
      "What is generative AI? Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation. Generative AI is the next step in artificial intelligence. You can train it to learn human language, programming languages, art, chemistry,\n",
      "# 12\n",
      "and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation. Generative AI is the next\n"
     ]
    }
   ],
   "source": [
    "question = \"what is rag in generative ai?\"\n",
    "bef = merger_retriever.invoke(question)\n",
    "dump_results_info(bef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12869ce-1b9f-4369-93b9-bf5ffd460e5c",
   "metadata": {},
   "source": [
    "##### Output from pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b70c2c8-802c-4060-8e5c-2f3c707a90ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc count =  5\n",
      "Context size =  4836\n",
      "# 0\n",
      "Prompt engineering is the process of structuring an instruction that can be interpreted and understood by a generative AI model. A prompt is natural language text describing the task that an AI should perform.\n",
      "A prompt for a text-to-text language model can be a query such as \"what is Fermat's little theorem?\", a command such as \"write a poem about leaves falling\", or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, providing relevant context or assigning a role to the AI such as \"Act as a native French speaker\". A prompt may include a few examples for a model to learn from, such as asking the model to complete \"maison → house, chat → cat, chien →\" (the expected response being dog), an approach called few-shot learning.\n",
      "When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as \"a high-quality photo of an astronaut riding a horse\" or \"Lo-fi slow BPM electro chill with organic samples\". Prompting a text-to-image model may involve adding, removing, emphasizing and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic.\n",
      "\n",
      "\n",
      "== In-context learning ==\n",
      "Prompt engineering is enabled by in-context learning, defined as a model's ability to temporarily learn from prompts. The ability for in-context learning is an emergent ability of large language models. In-context learning itself is an emergent property of model scale, meaning breaks in downstream scaling laws occur such that its efficacy increases at a different rate in larger models than in smaller models.\n",
      "In contrast to training and fine-tuning for each specific task, which are not temporary, what has been learnt during in-context learning is of a temporary nature. It does not carry the temporary contexts or biases, except the ones already present in the (pre)training dataset, from one conversation to the other. This result of \"mesa-optimization\" within transformer layers, is a form of meta-learning or \"learning to learn\".\n",
      "\n",
      "\n",
      "== History ==\n",
      "In 2018, researchers first proposed that all previously separate tasks in NLP could be cast as a question answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like \"What is the sentiment\" or \"Translate this sentence to German\" or \"Who is the president?\"\n",
      "In 2021, researchers fine-tuned one generatively pretrained model (T0) on performing 12 NLP tasks (using 62 datasets, as each task can have multiple datasets). The model showed good performance on new tasks, surpassing models trained directly on just performing one task (without pretraining). To solve a task, T0 is given the task in a structured prompt, for example If {{premise}} is true, is it also true that {{hypothesis}}? ||| {{entailed}}. is the prompt used for making T0 solve entailment.\n",
      "A repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022.\n",
      "In 2022 the chain-of-thought prompting technique was proposed by Google researchers.\n",
      "In 2023 several text-to-text and text-to-image prompt databases were publicly available.\n",
      "\n",
      "\n",
      "== Text-to-text ==\n",
      "\n",
      "\n",
      "=== Chain-of-thought ===\n",
      "Chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. Chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought. It allows large language models to overcome difficulties with some reasoning tasks that require logical thinking and multiple steps to solve, such as arithmetic or commonsense reasoning questions.\n",
      "For example, given the question \"Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\", a CoT prompt might indu\n",
      "# 1\n",
      "Title\n",
      "\n",
      "What is generative AI?\n",
      "\n",
      "Source\n",
      "\n",
      "https://aws.amazon.com/what\n",
      "\n",
      "is/generative\n",
      "\n",
      "ai/\n",
      "# 2\n",
      "Retrieval augmented generation, or RAG, helps ensure model outputs are grounded on your data. Instead of relying on the model’s training knowledge, AI apps architected for RAG can search your data\n",
      "# 3\n",
      "For example, here's what generative AI can do for optimization:\n",
      "# 4\n",
      "What is generative AI? Generative artificial intelligence (generative AI) is a type of AI that can create new content and ideas, including conversations, stories, images, videos, and music. AI technologies attempt to mimic human intelligence in nontraditional computing tasks like image recognition, natural language processing (NLP), and translation. Generative AI is the next step in artificial intelligence. You can train it to learn human language, programming languages, art, chemistry,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\genai-course\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "aft = compression_retriever.invoke(question)\n",
    "\n",
    "dump_results_info(aft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4cdcc-d5f2-4afb-bce0-8aa85303cae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
