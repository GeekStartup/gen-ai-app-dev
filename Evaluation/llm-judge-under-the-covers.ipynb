{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9705e68e-fc86-4052-91fa-9b6979b00126",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge under the covers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62355dbb-5852-4fb3-9496-40b0bad0fa5f",
   "metadata": {},
   "source": [
    "## Setup environment\n",
    "\n",
    "Setup the key file path and add the utilities folder to the package path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e24e9b-2e6d-4ab5-833a-2a30188d79cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "\n",
    " \n",
    "# setting path\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95027ee9-7148-40d3-96c7-dbaaa88d756f",
   "metadata": {},
   "source": [
    "## 1. Setup the evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fb5019-1c3b-4690-9e10-3e6a2a79150f",
   "metadata": {},
   "source": [
    "### Q&A dataset without the prediction i.e., answer\n",
    "This dataset can be used for generating the LLM responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f353f6e-3f6f-4b5c-b088-af2f109c2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_llm import create_gpt_llm, create_cohere_llm, create_ollama_llm, create_hugging_face_llm, create_google_llm, create_ai21_llm\n",
    "\n",
    "evaluation_dataset = [\n",
    "  {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"context\": \"France is a country in Western Europe known for its rich history, culture, and landmarks such as the Eiffel Tower in Paris, the capital of France.\",\n",
    "    \"reference\": \"Paris\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who wrote 'Romeo and Juliet'?\",\n",
    "    \"context\": \"'Romeo and Juliet' is a famous tragedy written in the late 16th century by William Shakespeare, an English playwright known as the Bard of Avon.\",\n",
    "    \"reference\": \"William Shakespeare\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the chemical symbol for water?\",\n",
    "    \"context\": \"Water is a compound made up of two hydrogen atoms and one oxygen atom, essential for all known forms of life. Its chemical symbol is H2O.\",\n",
    "    \"reference\": \"H2O\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the tallest mountain in the world?\",\n",
    "    \"context\": \"The Himalayan mountain range in Asia includes some of the tallest peaks in the world, including the renowned Mount Everest, the tallest mountain in the world.\",\n",
    "    \"reference\": \"Mount Everest\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who painted the Mona Lisa?\",\n",
    "    \"context\": \"The 'Mona Lisa' is a world-famous portrait known for its enigmatic smile, created during the Italian Renaissance by Leonardo da Vinci.\",\n",
    "    \"reference\": \"Leonardo da Vinci\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the largest planet in the Solar System?\",\n",
    "    \"context\": \"The Solar System includes eight planets, with the largest being Jupiter, a gas giant known for its Great Red Spot.\",\n",
    "    \"reference\": \"Jupiter\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the process by which plants make food?\",\n",
    "    \"context\": \"Plants produce food by converting sunlight, carbon dioxide, and water into glucose through a process called photosynthesis, occurring in chloroplasts.\",\n",
    "    \"reference\": \"Photosynthesis\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Which element has the atomic number 1?\",\n",
    "    \"context\": \"The periodic table organizes elements by atomic number, with the first element being hydrogen, a light, colorless gas essential for life.\",\n",
    "    \"reference\": \"Hydrogen\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who is known as the Father of Computers?\",\n",
    "    \"context\": \"A 19th-century mathematician and inventor named Charles Babbage laid the foundations for modern computing by designing the Analytical Engine.\",\n",
    "    \"reference\": \"Charles Babbage\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the boiling point of water at sea level?\",\n",
    "    \"context\": \"At standard atmospheric pressure, water transitions from a liquid to a gas at 100°C, the boiling point of water at sea level.\",\n",
    "    \"reference\": \"100°C\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd7300-e900-4075-adf9-77bb94a90e1e",
   "metadata": {},
   "source": [
    "### Q&A dataset with the prediction i.e., answer\n",
    "Assume that the answer was generated by an LLM. This dataset is ready to be used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c672d17-0740-4aa3-94f5-3991a0023b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume the answers were generated\n",
    "evaluation_dataset_with_answer = evaluation_dataset = [\n",
    "  {\n",
    "    \"question\": \"What is the capital of France?\",\n",
    "    \"context\": \"France is a country in Western Europe known for its rich history, culture, and landmarks such as the Eiffel Tower in Paris, the capital of France.\",\n",
    "    \"reference\": \"Paris\",\n",
    "    \"answer\": \"The capital is Paris\"  # 60%: similar/close to reference\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who wrote 'Romeo and Juliet'?\",\n",
    "    \"context\": \"'Romeo and Juliet' is a famous tragedy written in the late 16th century by William Shakespeare, an English playwright known as the Bard of Avon.\",\n",
    "    \"reference\": \"William Shakespeare\",\n",
    "    \"answer\": \"William Shakespeare\"  # 20%: exact match\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the chemical symbol for water?\",\n",
    "    \"context\": \"Water is a compound made up of two hydrogen atoms and one oxygen atom, essential for all known forms of life. Its chemical symbol is H2O.\",\n",
    "    \"reference\": \"H2O\",\n",
    "    \"answer\": \"H₂O\"  # 60%: similar/close to reference (using subscript)\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the tallest mountain in the world?\",\n",
    "    \"context\": \"The Himalayan mountain range in Asia includes some of the tallest peaks in the world, including the renowned Mount Everest, the tallest mountain in the world.\",\n",
    "    \"reference\": \"Mount Everest\",\n",
    "    \"answer\": \"Everest\"  # 60%: similar/close to reference (abbreviated)\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who painted the Mona Lisa?\",\n",
    "    \"context\": \"The 'Mona Lisa' is a world-famous portrait known for its enigmatic smile, created during the Italian Renaissance by Leonardo da Vinci.\",\n",
    "    \"reference\": \"Leonardo da Vinci\",\n",
    "    \"answer\": \"Da Vinci\"  # 60%: similar/close to reference (abbreviated)\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the largest planet in the Solar System?\",\n",
    "    \"context\": \"The Solar System includes eight planets, with the largest being Jupiter, a gas giant known for its Great Red Spot.\",\n",
    "    \"reference\": \"Jupiter\",\n",
    "    \"answer\": \"Saturn\"  # 10%: relevant but incorrect\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the process by which plants make food?\",\n",
    "    \"context\": \"Plants produce food by converting sunlight, carbon dioxide, and water into glucose through a process called photosynthesis, occurring in chloroplasts.\",\n",
    "    \"reference\": \"Photosynthesis\",\n",
    "    \"answer\": \"Photosynthesis\"  # 20%: exact match\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Which element has the atomic number 1?\",\n",
    "    \"context\": \"The periodic table organizes elements by atomic number, with the first element being hydrogen, a light, colorless gas essential for life.\",\n",
    "    \"reference\": \"Hydrogen\",\n",
    "    \"answer\": \"Helium\"  # 10%: random value\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who is known as the Father of Computers?\",\n",
    "    \"context\": \"A 19th-century mathematician and inventor named Charles Babbage laid the foundations for modern computing by designing the Analytical Engine.\",\n",
    "    \"reference\": \"Charles Babbage\",\n",
    "    \"answer\": \"Babbage\"  # 60%: similar/close to reference (abbreviated)\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the boiling point of water at sea level?\",\n",
    "    \"context\": \"At standard atmospheric pressure, water transitions from a liquid to a gas at 100°C, the boiling point of water at sea level.\",\n",
    "    \"reference\": \"100°C\",\n",
    "    \"answer\": \"100 degrees Celsius\"  # 60%: similar/close to reference (paraphrased)\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ca5ba-c5f0-4012-8474-eedd30ed5b39",
   "metadata": {},
   "source": [
    "## 2. Setup Judge LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1ba355-6473-4ccf-b900-3f38451390c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may try different LLM as judges\n",
    "llm_judge  = create_gpt_llm(args={\"temperature\": 0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d071b6-9ede-4681-9cad-9dcaefab5810",
   "metadata": {},
   "source": [
    "### Judge prompt-1 : WITHOUT Reference\n",
    "Judge LLLM uses its parameteric knowledge to evaluate the answer as there is no ground truth i.e., reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f3ca32-3bcb-4e30-83e5-c6721e69185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm_prompt_no_reference = \"\"\"\n",
    "You are an expert evaluator tasked with judging the quality of answers generated by an AI model using the provided context. \n",
    "Your role is to assess whether the answer is correct, incorrect based on the provided question and context. \n",
    "Be objective and provide clear reasoning for your judgment.\n",
    "\n",
    "Your response should be a JSON with two attributes {{\"result\": correct | incorrect, \"comment\": your comments}}\n",
    "\n",
    "## context\n",
    "{context}\n",
    "\n",
    "## question\n",
    "{question}\n",
    "\n",
    "## answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "\n",
    "# Template\n",
    "judge_llm_prompt_no_reference_template = PromptTemplate.from_template(judge_llm_prompt_no_reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebce52e5-f94a-4f64-885b-65e37ab51561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test case#1  (no reference, good answer):  \n",
      "{\"result\": \"correct\", \"comment\": \"The answer is correct. Albert Einstein is widely recognized as the developer of the theory of relativity, which is mentioned in the context.\"}\n",
      "\n",
      "Test case#2 (no reference, bad answer):  \n",
      "{\"result\": incorrect, \"comment\": \"The answer is incorrect because the name order is reversed. The correct answer is Albert Einstein, as stated in the context.\"}\n"
     ]
    }
   ],
   "source": [
    "# Unit test the judge with this prompt\n",
    "unit_test_data = {\n",
    "  \"question\": \"Who developed the theory of relativity?\",\n",
    "  \"context\": \"Albert Einstein, a renowned physicist, developed the theory of relativity, which revolutionized our understanding of space, time, and gravity.\",\n",
    "  \"reference\": \"Albert Einstein\"\n",
    "}\n",
    "\n",
    "# Without reference : good answer\n",
    "answer = \"Albert Einstein\"\n",
    "judge_prompt = judge_llm_prompt_no_reference_template.format(question=unit_test_data['question'], context=unit_test_data['context'], answer=answer)\n",
    "result = llm_judge.invoke(judge_prompt)\n",
    "print(\"Test case#1  (no reference, good answer): \", result)\n",
    "\n",
    "# Without reference : bad answer\n",
    "answer = \"Einstein Albert\"\n",
    "judge_prompt = judge_llm_prompt_no_reference_template.format(question=unit_test_data['question'], context=unit_test_data['context'], answer=answer)\n",
    "result = llm_judge.invoke(judge_prompt)\n",
    "print(\"\\nTest case#2 (no reference, bad answer): \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df970904-4d16-4fe8-96c8-059628bd7eb4",
   "metadata": {},
   "source": [
    "### Judge prompt-2 : WITH Reference\n",
    "The judge LLM is provided with the ground truth, so that it can compare the generated answer with the reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5983f4d1-4c54-45e6-a9b5-25dbd8d6193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "judge_llm_prompt_with_reference = \"\"\"\n",
    "You are an expert evaluator tasked with judging the quality of answers generated by an AI model using the provided context. \n",
    "Your role is to assess whether the answer is correct, incorrect based on the provided question, context and ground truth.\n",
    "\n",
    "Be objective and provide clear reasoning for your judgment.\n",
    "\n",
    "Your response should be a JSON with two attributes {{\"result\": correct | incorrect, \"comment\": your comments}}\n",
    "\n",
    "## context\n",
    "{context}\n",
    "\n",
    "## question\n",
    "{question}\n",
    "\n",
    "## ground truth\n",
    "{reference}\n",
    "\n",
    "## answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "judge_llm_prompt_with_reference_template = PromptTemplate.from_template(judge_llm_prompt_with_reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54461459-8141-4ff6-8312-a69a07267f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case#3  (with reference, good answer):  \n",
      "{\"result\": \"correct\", \"comment\": \"The answer is correct because it directly matches the ground truth and is supported by the provided context and question.\"}\n",
      "\n",
      "Test case#4  (with reference, bad answer):  \n",
      "{\"result\": incorrect, \"comment\": \"The answer is incorrect because the name order is reversed. The correct answer is Albert Einstein.\"}\n"
     ]
    }
   ],
   "source": [
    "# With reference : good answer\n",
    "answer = \"Albert Einstein\"\n",
    "judge_prompt = judge_llm_prompt_with_reference_template.format(question=unit_test_data['question'], context=unit_test_data['context'], reference=unit_test_data['reference'], answer=answer)\n",
    "result = llm_judge.invoke(judge_prompt)\n",
    "print(\"\\nTest case#3  (with reference, good answer): \", result)\n",
    "\n",
    "# With reference : bad answer\n",
    "answer = \"Einstein Albert\"\n",
    "judge_prompt = judge_llm_prompt_with_reference_template.format(question=unit_test_data['question'], context=unit_test_data['context'], reference=unit_test_data['reference'], answer=answer)\n",
    "result = llm_judge.invoke(judge_prompt)\n",
    "print(\"\\nTest case#4  (with reference, bad answer): \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b53b56-5c81-4804-9fe2-1c69fe9e21ba",
   "metadata": {},
   "source": [
    "### Judge prompt-3 : WITH Reference + Score assignment\n",
    "The judge LLM is provided with the ground truth. Judge LLM is asked to assign a score to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb82b7a-a8b6-4993-9832-9b3667d8a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm_prompt_with_reference_score = \"\"\"\n",
    "You are an expert evaluator tasked with judging the quality of answers generated by an AI model using the provided context. \n",
    "\n",
    "Your role is to assess whether the answer is correct, relevant, or incorrect based on the provided question, context and ground truth.\n",
    "\n",
    "In addition, you will assign a score to the answer on a scale of 1 to 5, where:\n",
    "\n",
    "1: The answer is incorrect (does not match the ground truth and is irrelevant to the question).\n",
    "\n",
    "2: The answer is partially relevant but incorrect (somewhat related to the question but does not match the ground truth).\n",
    "\n",
    "3: The answer is relevant but incorrect (directly addresses the question but does not match the ground truth).\n",
    "\n",
    "4: The answer is mostly correct (matches the ground truth partially or is very close but not exact).\n",
    "\n",
    "5: The answer is fully correct (matches the ground truth exactly and is fully relevant to the question\n",
    "\n",
    "\n",
    "\n",
    "Be objective and provide clear reasoning for your judgment.\n",
    "\n",
    "\n",
    "Additional Scoring Consideration:\n",
    "1. Ease to Follow in the Absence of Context: The answer should be clear and self-contained, meaning it can be understood without requiring the context. \n",
    "If the answer is ambiguous, overly brief, or relies heavily on the context to be understood, it should receive a lower score.\n",
    "2. Minor mistakes in answer may be ignored\n",
    "\n",
    "Your response should be a JSON with two attributes {{\"result\": correct | relevant | incorrect, \"score\": score you assigned, \"comment\": your comments}}\n",
    "\n",
    "## context\n",
    "{context}\n",
    "\n",
    "## question\n",
    "{question}\n",
    "\n",
    "## ground truth\n",
    "{reference}\n",
    "\n",
    "## answer:\n",
    "{answer}\n",
    "\"\"\"\n",
    "judge_llm_prompt_with_reference_score_template = PromptTemplate.from_template(judge_llm_prompt_with_reference_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53502803-44e3-498f-936c-c7361b8fdac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test case#4  (with reference, good answer):  \n",
      "{\"result\": \"correct\", \"score\": 5, \"comment\": \"The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.\"}\n",
      "\n",
      "Test case#6  (with reference, good answer):  {\"result\": \"incorrect\", \"score\": 1, \"comment\": \"The answer does not match the ground truth and is irrelevant to the question.\"}\n"
     ]
    }
   ],
   "source": [
    "# With reference : good answer\n",
    "answer = \"Albert Einstein\"\n",
    "judge_prompt = judge_llm_prompt_with_reference_score_template.format(question=unit_test_data['question'], context=unit_test_data['context'], reference=unit_test_data['reference'], answer=answer)\n",
    "result = llm_judge.invoke(judge_prompt)\n",
    "print(\"\\nTest case#4  (with reference, good answer): \", result)\n",
    "\n",
    "# With reference : bad answer\n",
    "answer = \"Albert Smith\"\n",
    "judge_prompt = judge_llm_prompt_with_reference_score_template.format(question=unit_test_data['question'], context=unit_test_data['context'], reference=unit_test_data['reference'], answer=answer)\n",
    "result = llm_judge.invoke(judge_prompt)\n",
    "print(\"\\nTest case#6  (with reference, good answer): \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b56f4-b17b-47e4-909a-7804761460c4",
   "metadata": {},
   "source": [
    "## 3. Run evaluations with different judge LLM prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4055011b-d61d-457a-847d-e012c346dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for running the evaluations with different prompts, datasets & judge LLM\n",
    "import json\n",
    "\n",
    "def    run_evaluation(prompt_template, eval_dataset, judge_llm):\n",
    "\n",
    "    MAX_SCORE=5\n",
    "    results = []\n",
    "    total_score, correct_count, incorrect_count, relevant_count, accuracy = 0, 0, 0, 0, 0\n",
    "    for test_case in eval_dataset:\n",
    "        judge_prompt = judge_llm_prompt_with_reference_score_template.format(**test_case)\n",
    "        result = llm_judge.invoke(judge_prompt)\n",
    "        result = json.loads(result)\n",
    "        results.append(result)\n",
    "        print(result, \"answer: \", test_case['answer'], \"  reference: \", test_case['reference'] )\n",
    "\n",
    "        # Update the counts\n",
    "        if result['result'] == 'correct':\n",
    "            correct_count=correct_count+1\n",
    "        elif result['result'] == 'incorrect':\n",
    "            incorrect_count=incorrect_count+1\n",
    "        elif result['result'] == 'relevant':\n",
    "            relevant_count=relevant_count+1 \n",
    "\n",
    "    # Assign weights to correct | relevant | incorrect to calculate accuracy\n",
    "    weigth_correct = 1\n",
    "    weight_incorrect= 0\n",
    "    weight_relevant= 0.5\n",
    "    \n",
    "    # Accuracy is calculated if the score attribute is there in the \n",
    "    if \"score\" in results[0]:\n",
    "\n",
    "        for result in results:\n",
    "            total_score = total_score + result['score']\n",
    "\n",
    "        accuracy = total_score/len(eval_dataset)\n",
    "        accuracy_pct = (accuracy/MAX_SCORE)*100\n",
    "\n",
    "    else:\n",
    "        accuracy = (correct_count*weigth_correct  +  incorrect_count*weight_incorrect + relevant_count*weight_relevant)/len(eval_dataset)\n",
    "        accuracy_pct = (accuracy/MAX_SCORE)*100\n",
    "        \n",
    "            \n",
    "    print(\"Dataset size : \", len(eval_dataset))\n",
    "    print(\"Correct : \", correct_count)\n",
    "    print(\"Incorrect : \", incorrect_count)\n",
    "    print(\"Relevant : \", relevant_count)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"Accuracy percentage : \", accuracy_pct, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c2d951c-0173-4b5d-9bab-e25409e33b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is clear and self-contained, and can be understood without requiring the context.'} answer:  The capital is Paris   reference:  Paris\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:  William Shakespeare   reference:  William Shakespeare\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It matches the ground truth exactly and is clear and self-contained, without requiring the context.'} answer:  H₂O   reference:  H2O\n",
      "{'result': 'relevant', 'score': 4, 'comment': \"The answer is mostly correct as it mentions the correct mountain, but it is not fully correct as it does not include the full name 'Mount Everest'. Additionally, the answer is clear and self-contained, as it can be understood without requiring the context.\"} answer:  Everest   reference:  Mount Everest\n",
      "{'result': 'relevant', 'score': 4, 'comment': \"The answer is mostly correct as it is relevant and closely related to the ground truth. However, it is not fully correct as it does not include the first name 'Leonardo'.\"} answer:  Da Vinci   reference:  Leonardo da Vinci\n",
      "{'result': 'relevant', 'score': 3, 'comment': 'The answer is relevant as it mentions another planet in the Solar System, but it is incorrect as it is not the largest planet. The answer is also clear and self-contained, but it relies on the context to be fully understood.'} answer:  Saturn   reference:  Jupiter\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:  Photosynthesis   reference:  Photosynthesis\n",
      "{'result': 'incorrect', 'score': 1, 'comment': 'The answer is incorrect as it does not match the ground truth and is irrelevant to the question. Helium has an atomic number of 2, not 1.'} answer:  Helium   reference:  Hydrogen\n",
      "{'result': 'relevant', 'score': 4, 'comment': 'The answer is mostly correct as it mentions the correct last name of the person known as the Father of Computers. However, it is not fully correct as it does not include the first name, Charles.'} answer:  Babbage   reference:  Charles Babbage\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:  100 degrees Celsius   reference:  100°C\n",
      "Dataset size :  10\n",
      "Correct :  5\n",
      "Incorrect :  1\n",
      "Relevant :  4\n",
      "Accuracy :  4.1\n",
      "Accuracy percentage :  82.0 %\n"
     ]
    }
   ],
   "source": [
    "# Prompt: judge_llm_prompt_no_reference_template, judge_llm_prompt_with_reference_template, judge_llm_prompt_with_reference_score_template\n",
    "# Eval dataset : Dataset with simulated answers\n",
    "run_evaluation(judge_llm_prompt_no_reference, evaluation_dataset_with_answer, llm_judge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d4d122-01a0-42a6-967a-11646946ebd0",
   "metadata": {},
   "source": [
    "## 4. Evaluate a real LLM's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2214d0-8c3f-4eba-95ec-d4998d011d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCohere\u001b[0m\n",
      "Params: {'model': None, 'max_tokens': 256, 'temperature': 0.75, 'k': 0, 'p': 1, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'truncate': None}\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:   Paris.   reference:  Paris\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is clear and self-contained, and can be understood without requiring the context.'} answer:   William Shakespeare.    reference:  William Shakespeare\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:   H2O.    reference:  H2O\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is clear and self-contained, and can be understood without requiring the context.'} answer:   Mount Everest.    reference:  Mount Everest\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is clear and self-contained, and can be understood without requiring the context.'} answer:   Leonardo da Vinci.    reference:  Leonardo da Vinci\n",
      "{'result': 'fully correct', 'score': 5, 'comment': 'The answer is fully correct as it matches the ground truth exactly and is fully relevant to the question. It is also clear and self-contained, requiring no additional context to be understood.'} answer:   Jupiter. \n",
      "\n",
      "Would you like me to list the eight planets of the Solar System?    reference:  Jupiter\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:   photosynthesis   reference:  Photosynthesis\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It is also clear and self-contained, as it can be understood without requiring the context.'} answer:   Hydrogen   reference:  Hydrogen\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer matches the ground truth exactly and is fully relevant to the question. It is also clear and self-contained, requiring no additional context to be understood.'} answer:   Charles Babbage   reference:  Charles Babbage\n",
      "{'result': 'correct', 'score': 5, 'comment': 'The answer is fully correct and relevant to the question. It matches the ground truth exactly and is clear and self-contained, requiring no additional context to be understood.'} answer:   100°C   reference:  100°C\n",
      "Dataset size :  10\n",
      "Correct :  9\n",
      "Incorrect :  0\n",
      "Relevant :  0\n",
      "Accuracy :  5.0\n",
      "Accuracy percentage :  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Create cohere LLM\n",
    "llm_target_cohere = create_cohere_llm()\n",
    "\n",
    "# Create google LLM\n",
    "llm_target_google_gemini = create_google_llm(model='gemini-1.5-flash')\n",
    "\n",
    "# Change this to try different LLMs\n",
    "target_llm = llm_target_cohere\n",
    "\n",
    "print(target_llm)\n",
    "\n",
    "target_llm_prompt = \"\"\"\n",
    "You are a teacher who answers students' questions strictly based on the provided context. \n",
    "You must only use information from the given context to answer the question and avoid making up any information. \n",
    "Your answers Must be concise. Do not provide any commentary or preamble.\n",
    "If the context does not contain the information needed to answer the question, respond with: \"The context does not provide enough information to answer this question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "target_prompt_template = PromptTemplate(\n",
    "    template = target_llm_prompt,\n",
    "    input_variables = [\"context\",\"question\"]\n",
    ")\n",
    "\n",
    "def run_predictions(llm_target):\n",
    "\n",
    "    dataset_with_predictions = []\n",
    "\n",
    "    for i, qa_pair in enumerate(evaluation_dataset):\n",
    "        context = qa_pair['context']\n",
    "        question = qa_pair['question']\n",
    "        prompt_qa = target_prompt_template.format(context=context, question=question)\n",
    "    \n",
    "        # invoke target LLM for answer\n",
    "        answer = llm_target.invoke(prompt_qa)\n",
    "\n",
    "        # print(\"ANSWER: \", answer)\n",
    "        qa_pair['answer']=answer\n",
    "    \n",
    "        dataset_with_predictions.append(qa_pair)\n",
    "\n",
    "    return dataset_with_predictions\n",
    "\n",
    "# eval_dataset_cohere = run_predictions(llm_target_cohere)\n",
    "eval_dataset = run_predictions(target_llm)\n",
    "\n",
    "# Run evaluation\n",
    "# Try different judge prompts: \n",
    "# judge_llm_prompt_no_reference_template, judge_llm_prompt_with_reference_template, judge_llm_prompt_with_reference_score_template\n",
    "run_evaluation(judge_llm_prompt_no_reference, eval_dataset, llm_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d912243-eea3-4791-a9d0-0055b46b17ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
