{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e03941d",
   "metadata": {},
   "source": [
    "# Exercise Solution : Prompt Engineering Best Practices\n",
    "## General Best Practices\n",
    "\n",
    "1. Separation between parts\n",
    "2. Detailed, clear & specific instructions\n",
    "3. Avoid ambiguity \n",
    "4. Favor positive instructions\n",
    "5. Instruct the model to play a role\n",
    "6. Provide trusted information\n",
    "7. Specify response characteristics\n",
    "8. Set guardrails for response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67f28ef-30c2-4a94-85a7-eb20c2c4f238",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the HF API Token\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f09a21-53a7-4d83-a153-ab3de2e32122",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The script is downloaded and run to setup the utils folder\n",
    "\n",
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b1b0c",
   "metadata": {},
   "source": [
    "## Change the location of the environment file before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a210debc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806ce6df-e8ee-410a-b368-82b69274b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  HUGGINGFACEHUB_API_TOKEN  already set in environment.\n"
     ]
    }
   ],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dac56e",
   "metadata": {},
   "source": [
    "## Setup the models available for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f3cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "hugging_face_model_ids = [\n",
    "    'tiiuae/falcon-7b-instruct',\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    'openlm-research/open_llama_3b_v2',\n",
    "    'google/flan-t5-xxl'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8f096",
   "metadata": {},
   "source": [
    "## 1. Guide the model to avoid hallucinations\n",
    "\n",
    "Add the guidance to not to make up a response.\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[2]    \n",
    "\n",
    "'openlm-research/open_llama_3b_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be60250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'as of 2023, who is the prime minister of UK\\n\\nDo not make up an answer, if you do not know the answer say \"I don\\'t know\"\\n\\n\\nA: The Prime Minister of the United Kingdom is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime Minister.\\nThe Prime Minister is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime Minister.\\nThe Prime Minister is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime Minister.\\nThe Prime Minister is the leader of the Conservative Party.\\nThe leader of the Conservative Party is the Prime'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_bad  = \"\"\"as of 2023, who is the prime minister of UK\"\"\"\n",
    "\n",
    "# Fixed prompt\n",
    "prompt_good = \"\"\"as of 2023, who is the prime minister of UK\n",
    "\n",
    "Do not make up an answer, if you do not know the answer say \"I don't know\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "## Inference Client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[2])\n",
    "# llm.text_generation(prompt_good)\n",
    "\n",
    "## HTTP Post\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[2])\n",
    "llm_client.invoke(prompt_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22cc49d",
   "metadata": {},
   "source": [
    "## 2. Instruct the model to play roles\n",
    "\n",
    "\n",
    "### Default model\n",
    "\n",
    "hugging_face_model_ids[3],\n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438b8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"question:what are large language models. answer:\"\n",
    "\n",
    "instruction_roles = [\n",
    "    \"you are a 5th grade science teacher answer the question below. {}\",\n",
    "    \"you are a college level professor of computer science answer the question below. {}\",\n",
    "    \"you are a doctorate of computer science answer the question below. {}\"\n",
    "]\n",
    "\n",
    "## Inference Client\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "# without_role = llm.text_generation(prompt, max_new_tokens=100)\n",
    "# print(\"Without role: \", without_role)\n",
    "\n",
    "## try different roles for LLM\n",
    "# role_index = 2\n",
    "# with_role = llm.text_generation(instruction_roles[role_index].format(prompt), max_new_tokens=100)\n",
    "# print(\"With role: \", with_role)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8af68d01-5a05-4b0b-afde-10bc82b7e3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without role:  [{'generated_text': 'question:what are large language models. answer: Large language models are artificial intelligence (AI) systems designed to generate human-like text based on the input they receive. These models are trained on vast amounts of text data, allowing them to understand and generate language with a high degree of accuracy and fluency. They can be used for various applications, such as text completion, translation, summarization, and chatbots. Some popular large language models include BERT, GPT-3, and T5. These models are called \"large\"'}]\n",
      "With role:  [{'generated_text': 'you are a doctorate of computer science answer the question below. question:what are large language models. answer: Large language models are artificial intelligence systems designed to generate human-like text based on the input they receive. These models are trained on vast amounts of text data, allowing them to understand and generate language with a high degree of fluency and accuracy. They use deep learning techniques, specifically a type of neural network called a transformer, to analyze the input data and generate appropriate responses. These models can be used for various applications such as text generation, translation, summarization, and question answering. They have'}]\n"
     ]
    }
   ],
   "source": [
    "## HTTP Post\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[3])\n",
    "without_role = llm_client.invoke(prompt)\n",
    "print(\"Without role: \", without_role)\n",
    "\n",
    "## try different roles for LLM\n",
    "role_index = 2\n",
    "with_role = llm_client.invoke(instruction_roles[role_index].format(prompt))\n",
    "print(\"With role: \", with_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ebf2f0",
   "metadata": {},
   "source": [
    "## 3. Guide the model to generate precise response\n",
    "\n",
    "### Default model\n",
    "hugging_face_model_ids[3], \n",
    "\n",
    "'google/flan-t5-xxl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2061bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_samples = [\n",
    "    \"how much would be the cost of 'Digital Camera DSLR'.\",\n",
    "    \"share details of home security camera\",\n",
    "    \"do you sell ceiling fan?\",\n",
    "    \"share details of Robot Cleaner?\",\n",
    "]\n",
    "\n",
    "with open('./data/acme-product-catalog.txt') as f:\n",
    "    product_catalog = f.read()\n",
    "\n",
    "\n",
    "prompt_good = \"\"\" To answer the question below, Use the information provided in product-catalog. \n",
    "If you don't find the product in the context the say 'sorry, we don't sell it'\n",
    "\n",
    "question: {}\n",
    "product-catalog:\\n {}\"\"\"\n",
    "\n",
    "llm = InferenceClient(model=hugging_face_model_ids[3])\n",
    "\n",
    "# Change the index to try out different models\n",
    "prompt_index = 0\n",
    "\n",
    "no_context_prompt = prompt_samples[prompt_index]\n",
    "with_context_prompt = prompt_good.format(prompt_samples[prompt_index], product_catalog)\n",
    "\n",
    "# print(with_context_prompt)\n",
    "\n",
    "no_context_response = llm.text_generation(no_context_prompt)\n",
    "\n",
    "with_context_response = llm.text_generation(with_context_prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Response without context: \", no_context_response)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Response with context: \", with_context_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f5564",
   "metadata": {},
   "source": [
    "## 4. Address bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcabcf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response without guardrails:  \n",
      "\n",
      "No, I do not agree. Hiring decisions should be based on qualifications, skills\n",
      "----------------------------------------------\n",
      "Response with guardrails:  I strongly disagree with the notion that engineering jobs should only be filled by men 100%\n"
     ]
    }
   ],
   "source": [
    "prompt = \"for the engineering jobs we should only hire men 100% of the time, do you agree? \"\n",
    "\n",
    "prompt_with_guardrails = \"\"\"You are a human resources expert. You always assist with care, respect, and truth.\n",
    "Your answers promote fairness and positivity. .\n",
    "\n",
    "question: {}\n",
    "\n",
    "answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "llm = InferenceClient(model=hugging_face_model_ids[1])  \n",
    "\n",
    "without_guardrails = llm.text_generation(prompt)\n",
    "\n",
    "with_guardrails = llm.text_generation(prompt_with_guardrails.format(prompt))\n",
    "\n",
    "print(\"Response without guardrails: \", without_guardrails)\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"Response with guardrails: \", with_guardrails)\n",
    "\n",
    "# prompt_with_guardrails.format(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d9f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
