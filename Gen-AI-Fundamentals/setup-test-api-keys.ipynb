{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbad3768-9963-462b-ace5-dc643fe0fe40",
   "metadata": {},
   "source": [
    "# Setup & Test API Key\n",
    "\n",
    "This notebook is used for testing the API keys setup for model/service providers.\n",
    "\n",
    "### Test a specific provider's key\n",
    "1. Run the cell : **Setup the environment**\n",
    "2. Run the cells for the provider\n",
    "3. The code makes an explicit call to *api_key_check(\"API_KEY_NAME\")*  but you may use: create_xxx_llm(api_key_prompt=True) to do avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d890e1-3b66-43c9-8144-b4107ebe20fd",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the utilities & packages by uncommenting & running the cell below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cfa0306-8598-479c-ad72-b1fc017c5da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b1ece8-18d6-4a7b-ba84-fc65b724adae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!conda info --env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aa554-5228-4cf1-a0c4-94ac71013943",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a0183a-7f18-4197-8aaa-0e966f898031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# CHANGE THIS TO POINT TO YOUR OWN FILE\n",
    "# Load the file that contains the API keys \n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38f0b3ad-a8ae-4704-9ef4-ebf1fa7e4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a92ed4-e478-4fa0-b5a9-a06ace7f0c65",
   "metadata": {},
   "source": [
    "## Google API key\n",
    "\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38369ec-0779-4548-bd5c-f9a4dac42d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  GOOGLE_API_KEY  already set in environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000243F1CA7830>, default_metadata=(), model_kwargs={}))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_google_llm\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Try out the Gemini Flash model\n",
    "# model=\"gemini-1.5-flash\" - created by default - you may try (model=\"gemini-1.5-pro\")\n",
    "llm = create_google_llm()\n",
    "\n",
    "# The above 2 lines is equivalent to the next line :-)\n",
    "# llm = create_google_llm(api_key_prompt=True)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5914d2-3621-40f4-b9bf-47b01debdb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs are large language models, powerful AI systems trained on massive text datasets.  They learn patterns and relationships in language to generate human-like text.  This allows them to perform tasks like translation, summarization, and question answering.  Their capabilities are constantly improving through ongoing training and refinement.  However, they can sometimes produce inaccurate or nonsensical outputs.\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in 5 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed22230-4477-475b-b21b-836fa12cd6fa",
   "metadata": {},
   "source": [
    "## HuggingFace token\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "#### Note\n",
    "Not all models are available for as endpoints. Check model's inference endpoint state to check if model endpoint is active:\n",
    "\n",
    "https://huggingface.co/models?inference=warm&other=endpoints_compatible\n",
    "\n",
    "**Warm**   Model inference endpoint is active\n",
    "\n",
    "**Cold**   Model will be loaded on request. Large models sometime timeout but, they become available after a couple of retries :)\n",
    "\n",
    "**Freezed** Model inference endpoint NO MORE available as there was not much interest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26483ba3-dd2d-48a2-82c8-bb271b170abc",
   "metadata": {},
   "source": [
    "### Note\n",
    "Sometime HuggingFace models time out :( It happens as a result model being in cold state. Two ways to address:\n",
    "\n",
    "1. Just retry and it should work\n",
    "2. Use a different model (usually smaller models are more available)\n",
    "\n",
    "HfHubHTTPError: 504 Server Error: Gateway Timeout for url: https:...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5c2080-bc77-4ff0-842e-1b7db2033813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  HUGGINGFACEHUB_API_TOKEN  already set in environment.\n",
      "In the context of generative AI, a Large Language Model (LLM) is a type of artificial intelligence algorithm that uses complex neural networks to process and generate human-like language, enabling it to understand and produce coherent text, similar to how humans communicate. LLMs are trained on vast amounts of text data, allowing them to learn patterns, relationships, and structures of language, which they can then use to generate new text, respond to questions, and even create original content.\n"
     ]
    }
   ],
   "source": [
    "# # Query to be sent to model\n",
    "# query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# # Invoke the model with the query\n",
    "# response = llm.invoke( query)\n",
    "\n",
    "# # Print the response\n",
    "# print(response)\n",
    "\n",
    "# Changed : August 12, 2025\n",
    "# To address the model not found and unsupported errors\n",
    "\n",
    "# Check if the key is available\n",
    "api_key = api_key_check(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "llm_client = hf_rest_client(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "response = llm_client.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f0c27-a584-4b4a-9115-7098f7db911c",
   "metadata": {},
   "source": [
    "## Cohere API Key\n",
    "\n",
    "COHERE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed12d716-9c40-47bb-b330-a99dd75983c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:  COHERE_API_KEY  already set in environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\OneDrive\\Documents\\Courses\\Generative-AI\\workspace\\gen-ai-app-dev\\Gen-AI-Fundamentals\\..\\utils\\create_llm.py:36: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
      "  llm = Cohere(**args)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Cohere(client=<cohere.client.Client object at 0x00000198B449B980>, async_client=<cohere.client.AsyncClient object at 0x00000198B58E6990>, cohere_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.create_llm import create_cohere_llm\n",
    "\n",
    "# # Check if the key is available\n",
    "# api_key = api_key_check(\"COHERE_API_KEY\")\n",
    "\n",
    "#  Create the default model : following would not require an explicit call to api_key_check\n",
    "llm = create_cohere_llm(api_key_prompt=True)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b29efc70-7bb4-4670-b773-f34493b70912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM stands for Large Language Model and is used in generative AI to produce text output that follows the patterns of human language. By using algorithms that automate the analysis and predict the next word in a sequence,  LLMs can generate realistic and coherent messages. \n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4196450f-965b-42f4-9b2d-f5f1a4cadbfd",
   "metadata": {},
   "source": [
    "## Olama\n",
    "DOES NOT REQUIRE AN API KEY !! but the Ollama server should be running.\n",
    "\n",
    "**Note**\n",
    "1. Install langchain Ollama\n",
    "\n",
    "```\n",
    "pip install -U langchain-ollama\n",
    "```\n",
    "\n",
    "2. Run the server\n",
    "\n",
    "```\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "https://python.langchain.com/docs/integrations/llms/ollama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "528b4dfb-7635-4c1c-92b5-322c32c33a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_llm import create_ollama_llm\n",
    "\n",
    "llm = create_ollama_llm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c832454c-8ac4-4e3a-83fc-c9ba9c65ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMs (Large Language Models) are a type of AI trained on massive text datasets, enabling them to understand and generate human-like text. Within generative AI, LLMs serve as powerful tools for creating various textual outputs, including stories, articles, code, and dialogue.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to be sent to model\n",
    "query = \"explain LLM in the context of generative ai in 2 sentences\"\n",
    "\n",
    "# Invoke the model with the query\n",
    "response = llm.invoke( query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd73e1e1-87db-4fd5-9822-072b06a1da04",
   "metadata": {},
   "source": [
    "## Groq API Key\n",
    "\n",
    "Set the Groq_API_KEY in the key file\n",
    "\n",
    "Try out Groq with Llamma using the instructions:  https://genai.acloudfan.com/20.dev-environment/ex-0-setup-groq-key/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd198d-2808-45aa-b565-e0cac8464e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
