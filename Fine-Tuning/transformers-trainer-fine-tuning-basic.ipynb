{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce45bc6e-0dfd-45bb-870a-41cdc5355b73",
   "metadata": {},
   "source": [
    "# Fine tune with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0ac83e-72cf-46c9-a33d-2b4577a8b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b428c9a-56b4-4adb-aa55-dd89b1482979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load a small pre-trained model and tokenizer\n",
    "# Using distilgpt2, which is a small, lightweight GPT-2 model\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bae2fe0-02b8-48c1-861b-1270eec460d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27879e3d98348f5af45ac02870c0a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439fd6232b76429bbb9f298dc12174fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Define a simple instruction dataset\n",
    "# For demonstration purposes, we create a basic dataset with prompt-response pairs.\n",
    "# Replace this with a real instruction-tuning dataset for actual use.\n",
    "instruction_data = {\n",
    "    \"train\": [\n",
    "        {\"prompt\": \"Translate to French: Hello, how are you?\", \"response\": \"Bonjour, comment ça va?\"},\n",
    "        {\"prompt\": \"Explain what a black hole is.\", \"response\": \"A black hole is a region of space where gravity is so strong that nothing, not even light, can escape.\"},\n",
    "        {\"prompt\": \"What is 2 + 2?\", \"response\": \"2 + 2 equals 4.\"},\n",
    "        {\"prompt\": \"Translate to Spanish: I love programming.\", \"response\": \"Me encanta programar.\"},\n",
    "        {\"prompt\": \"What is the boiling point of water in Celsius?\", \"response\": \"The boiling point of water is 100 degrees Celsius.\"},\n",
    "        {\"prompt\": \"Define photosynthesis.\", \"response\": \"Photosynthesis is the process by which green plants use sunlight to synthesize food from carbon dioxide and water.\"},\n",
    "        {\"prompt\": \"What is the square root of 16?\", \"response\": \"The square root of 16 is 4.\"},\n",
    "        {\"prompt\": \"Translate to German: Good morning.\", \"response\": \"Guten Morgen.\"},\n",
    "        {\"prompt\": \"Who wrote 'Pride and Prejudice'?\", \"response\": \"Jane Austen wrote 'Pride and Prejudice'.\"},\n",
    "        {\"prompt\": \"Explain gravity.\", \"response\": \"Gravity is the force that attracts objects toward each other, especially the force that makes things fall to the ground.\"},\n",
    "        {\"prompt\": \"What is the capital of Germany?\", \"response\": \"The capital of Germany is Berlin.\"},\n",
    "        {\"prompt\": \"Translate to Italian: See you tomorrow.\", \"response\": \"Ci vediamo domani.\"},\n",
    "        {\"prompt\": \"Who painted the Mona Lisa?\", \"response\": \"The Mona Lisa was painted by Leonardo da Vinci.\"},\n",
    "        {\"prompt\": \"Explain what DNA is.\", \"response\": \"DNA is the molecule that carries the genetic instructions for life.\"},\n",
    "        {\"prompt\": \"What is the largest planet in our solar system?\", \"response\": \"The largest planet in our solar system is Jupiter.\"},\n",
    "        {\"prompt\": \"Translate to Japanese: Thank you very much.\", \"response\": \"どうもありがとうございます (Dōmo arigatō gozaimasu).\"},\n",
    "        {\"prompt\": \"What is the speed of light?\", \"response\": \"The speed of light is approximately 299,792 kilometers per second.\"},\n",
    "        {\"prompt\": \"Define a computer.\", \"response\": \"A computer is an electronic device that processes and stores data, capable of performing calculations and tasks.\"},\n",
    "        {\"prompt\": \"Who discovered penicillin?\", \"response\": \"Alexander Fleming discovered penicillin.\"},\n",
    "        {\"prompt\": \"Translate to French: Where is the bathroom?\", \"response\": \"Où sont les toilettes?\"},\n",
    "        {\"prompt\": \"What is 5 factorial (5!)?\", \"response\": \"5 factorial, or 5!, is 120.\"},\n",
    "        {\"prompt\": \"Translate to Spanish: Have a nice day.\", \"response\": \"Que tengas un buen día.\"},\n",
    "        {\"prompt\": \"Explain the water cycle.\", \"response\": \"The water cycle is the continuous movement of water on Earth through evaporation, condensation, and precipitation.\"},\n",
    "        {\"prompt\": \"What is the capital of Japan?\", \"response\": \"The capital of Japan is Tokyo.\"},\n",
    "        {\"prompt\": \"Define renewable energy.\", \"response\": \"Renewable energy comes from natural sources that can be replenished, like solar and wind power.\"},\n",
    "        {\"prompt\": \"Translate to German: Happy birthday.\", \"response\": \"Alles Gute zum Geburtstag.\"},\n",
    "        {\"prompt\": \"What is the main ingredient in guacamole?\", \"response\": \"The main ingredient in guacamole is avocado.\"},\n",
    "        {\"prompt\": \"Explain what an atom is.\", \"response\": \"An atom is the smallest unit of matter that retains the properties of an element.\"},\n",
    "        {\"prompt\": \"What is the freezing point of water in Fahrenheit?\", \"response\": \"The freezing point of water is 32 degrees Fahrenheit.\"},\n",
    "        {\"prompt\": \"Translate to Italian: How much does this cost?\", \"response\": \"Quanto costa questo?\"},\n",
    "        {\"prompt\": \"Who developed the theory of relativity?\", \"response\": \"Albert Einstein developed the theory of relativity.\"},\n",
    "        {\"prompt\": \"Explain photosynthesis in simple terms.\", \"response\": \"Photosynthesis is how plants make their food using sunlight, water, and air.\"},\n",
    "        {\"prompt\": \"What is the capital of Australia?\", \"response\": \"The capital of Australia is Canberra.\"},\n",
    "        {\"prompt\": \"Translate to Japanese: Good evening.\", \"response\": \"こんばんは (Konbanwa).\"},\n",
    "        {\"prompt\": \"Who is the author of '1984'?\", \"response\": \"The author of '1984' is George Orwell.\"},\n",
    "        {\"prompt\": \"Define gravity.\", \"response\": \"Gravity is the force that pulls objects toward each other.\"},\n",
    "        {\"prompt\": \"Translate to French: I am happy.\", \"response\": \"Je suis heureux/heureuse.\"},\n",
    "        {\"prompt\": \"What is the currency of the United Kingdom?\", \"response\": \"The currency of the United Kingdom is the British Pound.\"},\n",
    "        {\"prompt\": \"Explain what a neuron is.\", \"response\": \"A neuron is a nerve cell that transmits signals in the body.\"},\n",
    "        {\"prompt\": \"What is the smallest country in the world?\", \"response\": \"The smallest country in the world is Vatican City.\"},\n",
    "        {\"prompt\": \"Translate to Spanish: What time is it?\", \"response\": \"¿Qué hora es?\"},\n",
    "        {\"prompt\": \"Who painted the Sistine Chapel ceiling?\", \"response\": \"Michelangelo painted the Sistine Chapel ceiling.\"},\n",
    "        {\"prompt\": \"Define an ecosystem.\", \"response\": \"An ecosystem is a community of living organisms interacting with their environment.\"},\n",
    "        {\"prompt\": \"What is the chemical formula of water?\", \"response\": \"The chemical formula of water is H₂O.\"},\n",
    "        {\"prompt\": \"Translate to German: Where are you from?\", \"response\": \"Woher kommst du?\"},\n",
    "        {\"prompt\": \"Who discovered gravity?\", \"response\": \"Sir Isaac Newton is credited with discovering gravity.\"},\n",
    "        {\"prompt\": \"Explain the theory of evolution.\", \"response\": \"The theory of evolution states that species change over time through natural selection.\"},\n",
    "        {\"prompt\": \"What is the capital of Canada?\", \"response\": \"The capital of Canada is Ottawa.\"},\n",
    "        {\"prompt\": \"Translate to Italian: Good luck.\", \"response\": \"Buona fortuna.\"},\n",
    "        {\"prompt\": \"What is the value of pi?\", \"response\": \"The value of pi is approximately 3.14159.\"}\n",
    "    ],\n",
    "    \"test\": [\n",
    "        {\"prompt\": \"What is the capital of France?\", \"response\": \"The capital of France is Paris.\"},\n",
    "        {\"prompt\": \"Translate to Spanish: Good morning.\", \"response\": \"Buenos días.\"},\n",
    "        {\"prompt\": \"What is 10 divided by 2?\", \"response\": \"10 divided by 2 equals 5.\"},\n",
    "        {\"prompt\": \"Translate to French: Thank you.\", \"response\": \"Merci.\"},\n",
    "        {\"prompt\": \"What is the largest ocean on Earth?\", \"response\": \"The largest ocean on Earth is the Pacific Ocean.\"},\n",
    "        {\"prompt\": \"Who wrote 'Hamlet'?\", \"response\": \"William Shakespeare wrote 'Hamlet'.\"},\n",
    "        {\"prompt\": \"Translate to Japanese: Goodbye.\", \"response\": \"さようなら (Sayōnara).\"},\n",
    "        {\"prompt\": \"What is the capital of Italy?\", \"response\": \"The capital of Italy is Rome.\"},\n",
    "        {\"prompt\": \"Define velocity.\", \"response\": \"Velocity is the speed of an object in a specific direction.\"},\n",
    "        {\"prompt\": \"Translate to German: How are you?\", \"response\": \"Wie geht's?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Step 3: Prepare the dataset for training\n",
    "# Tokenize the dataset to convert text into input IDs and labels\n",
    "def tokenize_function(example):\n",
    "    input_text = f\"Instruction: {example['prompt']}\\nResponse: {example['response']}\"\n",
    "    tokenized = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"input_ids\"],  # Add labels for causal LM\n",
    "    }\n",
    "\n",
    "\n",
    "# Convert data into a Dataset object and tokenize\n",
    "from datasets import DatasetDict, Dataset\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_list(instruction_data[\"train\"]),\n",
    "    \"test\": Dataset.from_list(instruction_data[\"test\"]),\n",
    "})\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=False)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"prompt\", \"response\"])  # Clean extra columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4606af6-f5fb-44e1-bf07-fa833d93b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\raj\\AppData\\Local\\Temp\\ipykernel_1000\\1508292002.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision if supported\n",
    "    report_to='none',\n",
    "\n",
    ")\n",
    "# Step 5: Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The model to train\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets[\"test\"],    # Evaluation dataset\n",
    "    tokenizer=tokenizer,                 # Tokenizer for processing inputs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "521fdd25-5b28-4b02-9d10-870b0997128b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 06:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.110495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.166600</td>\n",
       "      <td>0.080162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>0.073244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb431d96-791b-4cf7-85fe-5a880a781968",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "Instruction: What is the value of pi?\n",
      "Response: The value of pi is approximately 1.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Test the fine-tuned model\n",
    "# Generate a response for a given instruction\n",
    "test_prompt = \"What is the value of pi?\"\n",
    "input_text = f\"Instruction: {test_prompt}\\nResponse:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate response using the fine-tuned model\n",
    "output_ids = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Response:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d7c807-399b-495c-a63d-4196118a03db",
   "metadata": {},
   "source": [
    "# GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00cdde60-0a28-4570-a9cb-46fad3296606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"imdb\") \n",
    "# dataset = load_dataset(\"glue\", \"mrpc\") \n",
    "\n",
    "# Define the model and tokenizer\n",
    "model_name = \"bert-base-uncased\" \n",
    "# model_name = \"distilgpt2\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c06efac-ee68-46d6-aeb8-846b417c56b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b3bf6902334cb9b4bc1797c245fa51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c79db850274480099f8f3c59a21692e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5649f60adf9e446d996c6c2df1af3b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "# def tokenize_function(examples):\n",
    "#     print(type(examples))\n",
    "#     tokenized = tokenizer.tokenize(examples[\"text\"], examples[\"label\"], truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=512,)\n",
    "#     # return tokenized\n",
    "#     return {\n",
    "#         \"input_ids\": tokenized[\"input_ids\"],\n",
    "#         \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "#         \"labels\": tokenized[\"input_ids\"],  # Add labels for causal LM\n",
    "#     }\n",
    "\n",
    "def tokenize_function(example):\n",
    "    input_text = f\"Instruction: {example['text']}\\nResponse: {example['label']}\"\n",
    "    tokenized = tokenizer(\n",
    "        input_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"input_ids\"],  # Add labels for causal LM\n",
    "    }\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a4bdd81-0157-403c-b15d-c9b50d7fafc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=512,\n",
    "    per_device_eval_batch_size=512,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0065df3-f992-4d8a-9da2-7b976c851c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbf1cd-1861-475c-976b-a12d67a9fcd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
