{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bff477c",
   "metadata": {},
   "source": [
    "# LLM Challenges\n",
    "\n",
    "* Multiple models in use for demonstrating the behavior of the LLM\n",
    "* You can use either the InferenceClient or the HTTP API invocation.\n",
    "\n",
    "https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation\n",
    "\n",
    "**Note**\n",
    "* YOUR RESULTS MAY BE DIFFERENT THAN THE RESULTS IN VIDEO\n",
    "* If you get a '404 not found', try a different model for the call \n",
    "* A return value of 503 indicates that the model is in cold state and is loading\n",
    "* Wait a few moments and try again\n",
    "* In case of 500, model is in freezed state or may not be available for some time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a4515c-7734-474b-bb7f-4332e4ab8313",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the HF API Token\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed22c1f-31ce-4c47-84c2-e4de146f3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The script is downloaded and run to setup the utils folder\n",
    "\n",
    "# !curl -H \"Accept: application/vnd.github.VERSION.raw\" https://raw.githubusercontent.com/acloudfan/gen-ai-app-dev/main/Setup/gcsetup.sh  > gcsetup.sh\n",
    "# !chmod u+x gcsetup.sh\n",
    "# !./gcsetup.sh -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a5e71",
   "metadata": {},
   "source": [
    "## Setup the enviornment varaibles"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4b45c1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:27:40.222219Z",
     "start_time": "2025-08-23T13:27:40.198220Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "\n",
    "load_dotenv('E:\\\\Code\\\\gen-ai-app-dev-course\\\\.env')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e8c1f7a6-f88d-4b93-a631-f50f9a5f936f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:27:40.247641Z",
     "start_time": "2025-08-23T13:27:40.238349Z"
    }
   },
   "source": [
    "# Setting path so we can access the utils folder\n",
    "sys.path.append('../')\n",
    "sys.path.append('./')\n",
    "\n",
    "from utils.api_key_check_utility import api_key_check"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "4694e045",
   "metadata": {},
   "source": [
    "## Create LLM for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "id": "7abfdb2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:38:24.009086Z",
     "start_time": "2025-08-23T13:38:24.002957Z"
    }
   },
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from utils.hf_post_api import hf_rest_client\n",
    "\n",
    "# SOME OF THESE MODELS ARE NOW REMOVED FROM HUGGINGFACE INFERENCE - \n",
    "# August 10th, 2025\n",
    "# hugging_face_model_ids = [\n",
    "#     'google/gemma-2-2b-it',\n",
    "#     'tiiuae/falcon-7b-instruct',\n",
    "#     'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "#     'openlm-research/open_llama_3b_v2',\n",
    "#     'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "# ]\n",
    "\n",
    "# Feel free to add other models by checking out the availability from following link\n",
    "# https://router.huggingface.co/v1/models\n",
    "hugging_face_model_ids = [\n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    'mistralai/Mistral-7B-Instruct-v0.2',\n",
    "    \"deepseek-ai/DeepSeek-V3-0324\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "96f3e7f2",
   "metadata": {},
   "source": [
    "## 1. Hallucination\n",
    "\n",
    "Some models are better than others. Try out a couple of models to figure out the ones that hallucinate more than other models."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5a6b51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:38:42.269586Z",
     "start_time": "2025-08-23T13:38:40.223723Z"
    }
   },
   "source": [
    "text = \"define LLM in the context of biology\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(hugging_face_model_ids[0])\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[1])\n",
    "llm_client.invoke(text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the context of biology, LLM stands for Large Linguistic Model, but I believe you might be referring to 'LCMs' - Large Cell Model, however I found that in some contexts, 'LLM' is used in the context of 'Large Language Model' but now I found that 'LLM' is actually used in the context of 'Large Liquid Medium' but again that doesn't seem to be correct, however I was able to find that LLM refers to Large Liquid Medium\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "04015520",
   "metadata": {},
   "source": [
    "## 2. Dated knowledge\n",
    "\n",
    "**Note:**\n",
    "You will also observe hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "id": "40ce47a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:42:27.408576Z",
     "start_time": "2025-08-23T13:42:24.982770Z"
    }
   },
   "source": [
    "# Try out the models & your own prompts\n",
    "# text = \"who won the 2022 super bowl?\"  # LA Rams vs Cincinnati Bengals  https://en.wikipedia.org/wiki/Super_Bowl_LVI\n",
    "text = \"as of today, who is the prime minister of UK\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[0])\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[4])\n",
    "llm_client.invoke(text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As of today (July 7, 2024), the Prime Minister of the United Kingdom is **Rishi Sunak**.  \\n\\nRishi Sunak became Prime Minister on **October 25, 2022**, after succeeding Liz Truss. He is the leader of the Conservative Party (Tories) and has held office since then.  \\n\\nPlease note that political positions can change due to elections or other political events, so it's always good to check the latest updates if you're looking for\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "a080bf8b",
   "metadata": {},
   "source": [
    "## 3. Missing context\n",
    "\n",
    "Model is not aware of your enterprise's business domain or model"
   ]
  },
  {
   "cell_type": "code",
   "id": "3b97a64b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T13:53:28.762696Z",
     "start_time": "2025-08-23T13:53:26.862170Z"
    }
   },
   "source": [
    "# Try out the models and your own prompts\n",
    "text = \"what is your return policy?\"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  # 1 = mistralai/Mistral-7B-Instruct-v0.2\n",
    "# llm.text_generation(text, max_new_tokens=120)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[4])\n",
    "llm_client.invoke(text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's our standard return policy (may vary by product/service—let me know if you need specifics!):  \\n\\n1. **General Goods**:  \\n   - Items in original condition can typically be returned within 30 days (exceptions apply for final sale items like clearance items or custom orders).  \\n\\n2. **Digital Products/Subscriptions**:  \\n   - May not be eligible for returns unless faulty—check your plan details.  \\n\\n3. **Refunds**:  \\n   - Processed within \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "3c3d7fdd",
   "metadata": {},
   "source": [
    "## 4. Bias\n",
    "\n",
    "Some models are trained to prevent biases. Try out the model **index= 1 'tiiuae/falcon-7b-instruct'** to see its behavior."
   ]
  },
  {
   "cell_type": "code",
   "id": "07d95e02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T14:14:17.546996Z",
     "start_time": "2025-08-23T14:14:14.896507Z"
    }
   },
   "source": [
    "text = \"for the engineering jobs we should only hire men because \"\n",
    "\n",
    "# Change the index to try out different models\n",
    "# llm = InferenceClient(model=hugging_face_model_ids[1])  # 2 = openlm-research/open_llama_3b_v2\n",
    "# llm.text_generation(text, max_new_tokens=75)\n",
    "\n",
    "llm_client = hf_rest_client(hugging_face_model_ids[4])\n",
    "llm_client.invoke(text)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#### Initial Thought  \\nThe statement suggests that engineering jobs should only hire men, which seems to imply a gender-based preference in hiring. My initial reaction is that this is discriminatory and goes against principles of equal opportunity and fairness.  \\n\\n#### Why It’s Problematic  \\n1. **Discrimination**: Hiring based on gender (or any protected characteristic) is unethical and often illegal. It undermines equality and diversity.  \\n2. **Missed Opportunities**: Excluding women from engineering roles ignores their skills,'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbe0f5-61c7-4f85-951c-283d0085bc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
