{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1c106b-5a32-452c-a374-d689da710d20",
   "metadata": {},
   "source": [
    "# Tokenizer class\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "\n",
    "Try out various methods:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer\n",
    "\n",
    "* call(\"text\")\n",
    "* tokenize(\"text\")\n",
    "* encode(\"text\") : converts to IDs of the tokens\n",
    "* decode(encoded_text) : converts IDs to token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb52814-cc1c-4b09-984b-f64ea9b941fb",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e406922-0738-4d1d-8afd-e38556f344d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008fc60-e1a1-419b-bd0b-bd02d39e82e7",
   "metadata": {},
   "source": [
    "### Import appropriate model & config classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1addb38c-49a3-419f-91ba-de3fff2bc114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a58a7ef-daeb-434d-8061-1dfe5ad59923",
   "metadata": {},
   "source": [
    "## 1. Create an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99d8a6f-b6f7-4f9c-9f2c-48af0c8539fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f682ba824d10461c9edfbb629cf87ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3837d201ff3b4bc4aeb8485956c1ccfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a3ea207bb24fd68c272593ba110397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\genai-course-test\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd29a576e0d34b03b83dc0e0aaac822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffaaf5c-d3e6-45de-884e-2ea89ee7d309",
   "metadata": {},
   "source": [
    "## 2. Tokenize  the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3986a0-5431-47dc-8cf6-613e145274ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'is', 'a', 'story', '##tell', '##er']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'he is a storyteller'\n",
    "\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf07b3-7d3d-4c12-a53a-89b40b519e82",
   "metadata": {},
   "source": [
    "# 3. Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c4ebff-7e30-4fb0-b373-28faad33c161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2002, 2003, 1037, 2466, 23567, 2121, 102]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc9445-ad70-435b-af69-aad49e794999",
   "metadata": {},
   "source": [
    "## 4. Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e145d9d8-8fe5-4c43-8bb8-85315255a4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] he is a storyteller [SEP]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "\n",
    "decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87872e12-c8c9-4674-a302-27c343011b5e",
   "metadata": {},
   "source": [
    "## 5. Create tensors for input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13839a10-fedc-4952-bbbc-ffa1974ba3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2002, 2003, 1037, 2466, 23567, 2121, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns Python Lists\n",
    "inputs = tokenizer(text)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba6e34c-8687-4525-aa26-fc4ce41eb6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2002,  2003,  1037,  2466, 23567,  2121,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns tensors in desired format (PyTorch=pt, TensorFlow=tf, Flax=jax, Numpy=np)\n",
    "return_tensors = 'pt' \n",
    "\n",
    "inputs = tokenizer(text, return_tensors=return_tensors)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16c9c38-6a67-4564-8bb5-fd467d9feef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c025d0-83f9-4dbd-bef3-8936747a21dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
