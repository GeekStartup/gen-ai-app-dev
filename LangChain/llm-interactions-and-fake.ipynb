{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d5cf52-b57c-4aef-a558-0aeb33981186",
   "metadata": {},
   "source": [
    "# Fake LLM\n",
    "\n",
    "https://api.python.langchain.com/en/stable/llms/langchain_community.llms.fake.FakeListLLM.html#langchain_community.llms.fake.FakeListLLM\n",
    "\n",
    "Use at development time to save on the costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b379805-4f9c-4675-9063-cd41bf84135d",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "If you are running the code in Google colab, install the packages by uncommenting/running the cell below\n",
    "\n",
    "* The API key file file will not be available\n",
    "* You will be prompted to provide the Cohere API Token\n",
    "\n",
    "Uncomment & run the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8a42e4-ff37-48f8-9ea4-32264d2fc37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-community -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c578ae-7c43-4428-9c99-afdfb3e890dd",
   "metadata": {},
   "source": [
    "## 1. Import fake LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d3dae4b-4a35-4a57-a4e5-b1dbf3d5bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.fake import FakeListLLM\n",
    "from langchain_community.llms.fake import FakeStreamingListLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c71ca0-64f1-4e0f-99c0-a261f0747fdd",
   "metadata": {},
   "source": [
    "## 2. Setup the Fake List LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98f5f5f-e860-4ff6-95ac-302b492df190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random responses sent by the LLM\n",
    "fake_responses = [\n",
    "    \"this is a fake response 1\",\n",
    "    \"this is a fake response 2\",\n",
    "]\n",
    "\n",
    "# Create the faker\n",
    "fake_list_llm = FakeListLLM(responses=fake_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ec3f9-3663-4697-9b22-7e55a5cf1e1b",
   "metadata": {},
   "source": [
    "#### Synchronous invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ce125e-6474-4e73-b369-1cbca8adb5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.78 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this is a fake response 1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Quick test to see if it is working\n",
    "fake_list_llm.invoke(\"give whatever\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d2936-5dc8-49a1-880d-5d9a07c95eae",
   "metadata": {},
   "source": [
    "#### Asynchronous ainvoke()\n",
    "\n",
    "* Uses the Python [asynchio library](https://docs.python.org/3/library/asyncio.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4b25eb5-7cff-419a-8ebc-bae56a1b28ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use await to invoke the async function\n",
    "response =  await fake_list_llm.ainvoke(\"give whatever\")\n",
    "\n",
    "# print response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf801e16-a226-4e3e-a0ba-b3973c7ec010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78cdd93f-f6a3-48fa-8f13-dcbee4cccaa8",
   "metadata": {},
   "source": [
    "## 3. Streaming\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/expression_language/streaming/\n",
    "\n",
    "* Supports synchronous & asynchronous streaming\n",
    "* Use Fake Streaming LLM to demonstrate the working\n",
    "\n",
    "https://api.python.langchain.com/en/stable/llms/langchain_community.llms.fake.FakeStreamingListLLM.html#langchain_community.llms.fake.FakeStreamingListLLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927413c-3636-4a7d-afca-39921a46e7a4",
   "metadata": {},
   "source": [
    "#### Synchronous stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ad75c0-bebc-4017-9ce2-3cdfc824273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a fake response 1"
     ]
    }
   ],
   "source": [
    "# Create a fake streaming LLM with an artificial delay of 0.5 seconds between chunks\n",
    "fake_streaming_llm = FakeStreamingListLLM(responses=fake_responses, sleep=0.5)\n",
    "\n",
    "# Sysnchronous streaming\n",
    "for chunk in fake_streaming_llm.stream(\"does not matter\"):\n",
    "    print(chunk,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0d921-8553-420d-97bb-20afb421ab04",
   "metadata": {},
   "source": [
    "#### Asynchronous astream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43da323-3634-4674-be42-6c81e26f6cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Do something while, while LLM is generating a response i.e., thread is not blocked ...\n",
      "this is a fake response 1"
     ]
    }
   ],
   "source": [
    "# Initiate the LLM streaming as async\n",
    "streaming_object = fake_streaming_llm.astream(\"does not matter\")\n",
    "\n",
    "# Do other things, while LLM is processing in parallel\n",
    "print(\"\\nDo something while, while LLM is generating a response i.e., thread is not blocked ...\")\n",
    "\n",
    "# Gather some of the streamed \n",
    "async for chunk in streaming_object:\n",
    "    print(chunk,end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3f090d-40e6-4282-bd47-02c4219e782e",
   "metadata": {},
   "source": [
    "## 4. Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a13321-3792-4633-8991-101c2df008d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"person\": \"tom\", \"org\": \"apple\"}',\n",
       " '{\"person\": \"nick\", \"org\": \"amazon\"}',\n",
       " '{\"person\": \"anil\", \"org\": \"meta\"}',\n",
       " '{\"person\": \"tom\", \"org\": \"apple\"}']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fake responses\n",
    "fake_responses = [\n",
    "    '{\"person\": \"tom\", \"org\": \"apple\"}',\n",
    "    '{\"person\": \"nick\", \"org\": \"amazon\"}',\n",
    "    '{\"person\": \"anil\", \"org\": \"meta\"}',\n",
    "]\n",
    "\n",
    "fake_list_llm = FakeListLLM(responses=fake_responses)\n",
    "\n",
    "# Test requests sent in a batch in parallel\n",
    "request_inputs = [\n",
    "    \"extract named entitities :  ......\",\n",
    "    \"extract named entitities :  ......\",\n",
    "    \"extract named entitities :  ......\",\n",
    "    \"extract named entitities :  ......\",\n",
    "]\n",
    "\n",
    "# Invoke LLM in parallel for all 3 \n",
    "fake_list_llm.batch(request_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a8e1c-ecb6-4515-9206-54384e270117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
