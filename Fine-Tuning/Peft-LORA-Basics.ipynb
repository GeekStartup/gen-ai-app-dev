{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d7e4dc-01a3-4a54-b07b-4f74fa0edfd5",
   "metadata": {},
   "source": [
    "# PEFT - LoRA Basics\n",
    "\n",
    "**bitsandbytes**\n",
    "\n",
    "Default version requires CUDA. Experimental versions available for different CPU.\n",
    "\n",
    "https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n",
    "\n",
    "**Inspired by**\n",
    "\n",
    "https://github.com/huggingface/peft/blob/main/examples/int8_training/Finetune_opt_bnb_peft.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "901fef7b-8587-4c6a-add4-344dcd38dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3547ea-d4cb-4e0f-81cd-1b5cea0361b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Load model\n",
    "# \"facebook/opt-6.7b\"\n",
    "\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Load with 8 bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) #, quantization_config=BitsAndBytesConfig(load_in_8bit=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad17565-8aee-4398-9164-09204be331fa",
   "metadata": {},
   "source": [
    "## Model layers\n",
    "\n",
    "\n",
    "In transformer architectures, the modules that can be configured with **LoRA** often depend on the model's specific implementation. While the commonly known modules are `q_proj`, `k_proj`, and `v_proj`, there are additional modules in transformers that can also benefit from LoRA adaptation.\n",
    "\n",
    "Here’s a more comprehensive list of LoRA-compatible layers typically found in transformer models:\n",
    "\n",
    "---\n",
    "\n",
    "### Common LoRA-Configurable Layers:\n",
    "1. **`q_proj`**: Query projection in the attention mechanism.\n",
    "2. **`k_proj`**: Key projection in the attention mechanism.\n",
    "3. **`v_proj`**: Value projection in the attention mechanism.\n",
    "4. **`out_proj`**: Output projection in the attention mechanism.\n",
    "5. **`fc1` / `dense_h_to_4h`**: First fully connected layer in the feedforward network (FFN).\n",
    "6. **`fc2` / `dense_4h_to_h`**: Second fully connected layer in the feedforward network (target_layers)\n",
    "\n",
    "### Explanation of Additional Layers:\n",
    "1. **`out_proj`**:\n",
    "   - After the attention mechanism calculates its weighted sum, `out_proj` projects the result back into the model's embedding space.\n",
    "\n",
    "2. **`fc1` and `fc2`**:\n",
    "   - These are the layers in the feedforward network within each transformer block.\n",
    "   - Adapting these with LoRA can improve performance in certain downstream tasks.\n",
    "\n",
    "3. **Other Custom Layers**:\n",
    "   - Some transformer variants might use different naming conventions (`dense_h_to_4h`, `dense_4h_to_h` in OpenAI's GPT-like architectures) or add additional modules that could be LoRA-configured.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Tips:\n",
    "- Check the model’s documentation or inspect its architecture to find precise names for the layers.\n",
    "- Use filtering logic (e.g., keywords like `proj`, `fc`, `dense`) to identify potential layers.\n",
    "- Test LoRA configurations iteratively to see the impact of adapting additional layers.\n",
    "\n",
    "This approach ensures you maximize the effectiveness of LoRA while minimizing unnecessary adaptations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "766ca905-ec21-45d2-810c-56890a91aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# Dumps a list of layers\n",
    "def list_all_layers():\n",
    "    # Returns an iterator over all modules in the model, along with their names\n",
    "    modules = model.named_modules()\n",
    "    for name, module in modules:\n",
    "        print(name)\n",
    "\n",
    "# Read model documentation to learn the keywords\n",
    "def list_lora_compatible_layers():\n",
    "    num_lora_compatible_layers = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if any(keyword in name for keyword in [\"proj\", \"fc\", \"dense\"]):  # Adapt this as needed\n",
    "            num_lora_compatible_layers = num_lora_compatible_layers + 1\n",
    "\n",
    "    print(\"LoRA-compatible layers:\", num_lora_compatible_layers)\n",
    "\n",
    "def count_lora_layers():\n",
    "    modules = model.named_modules()\n",
    "    num_layers = 0\n",
    "    lora_layers_k = 0\n",
    "    lora_layers_q = 0\n",
    "    lora_layers_v = 0\n",
    "    lora_layers_out = 0\n",
    "    for name, module in modules:\n",
    "        num_layers = num_layers + 1\n",
    "        if 'k_proj' in name:\n",
    "            lora_layers_k = lora_layers_k + 1\n",
    "        if 'q_proj' in name:\n",
    "            lora_layers_q = lora_layers_q + 1  \n",
    "        if 'v_proj' in name:\n",
    "            lora_layers_v = lora_layers_v + 1  \n",
    "        if 'out_proj' in name:\n",
    "            lora_layers_out = lora_layers_out + 1  \n",
    "\n",
    "    print(\"Number of layers in the model = \", num_layers)\n",
    "    print(\"LORA layers (k_proj, q_proj, v_proj, out_proj) :  \", lora_layers_k, lora_layers_q, lora_layers_v, lora_layers_out )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "878b5a7b-0282-4dd0-a55b-d72b29fe7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA-compatible layers: 626\n",
      "--------------------------------\n",
      "Number of layers in the model =  791\n",
      "LORA layers (k_proj, q_proj, v_proj, out_proj) :   24 264 264 24\n"
     ]
    }
   ],
   "source": [
    "# list_all_layers()\n",
    "\n",
    "list_lora_compatible_layers()\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "count_lora_layers()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09827e9a-8946-4c70-9f81-22394bd42503",
   "metadata": {},
   "source": [
    "## Trainable parameters \n",
    "\n",
    "Primarily determined by the value assigned to **r**\n",
    "\n",
    "* **r**: Current rank value in the iteration.\n",
    "* **lora_alpha**: A scaling factor to balance the effect of LoRA updates.\n",
    "* **target_modules**: Specifies the modules of the base model to which LoRA is applied. Here, q_proj and v_proj are likely related to query and value * projections in transformer architectures.\n",
    "* **lora_dropout:** Dropout probability for regularization of LoRA parameters.\n",
    "* **bias:** Defines how biases in the original model should be treated. \"none\" means no additional bias parameters are added.\n",
    "* **task_type**: Specifies the task type. \"CAUSAL_LM\" indicates this is a causal language modeling task (e.g., autoregressive text generation).\n",
    "\n",
    "\n",
    "### Task types\n",
    "The `task_type` parameter in LoRA (Low-Rank Adaptation) specifies the type of task for which the model is being fine-tuned. The specific values of `task_type` determine how the adaptation layers are configured and integrated into the model. Below are the most common `task_type` values supported by LoRA implementations, such as those in the [PEFT](https://github.com/huggingface/peft) library:\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### List of Supported `task_type` Values:\r\n",
    "\r\n",
    "1. **`CAUSAL_LM`** (Causal Language Modeling)\r\n",
    "   - Used for autoregressive tasks where the model predicts the next token in a sequence.\r\n",
    "   - Example models: GPT, GPT-2, GPT-3, Codex.\r\n",
    "   - Applications: Text generation, code generation.\r\n",
    "\r\n",
    "2. **`SEQ2SEQ_LM`** (Sequence-to-Sequence Language Modeling)\r\n",
    "   - Used for tasks that involve input-output sequence transformations.\r\n",
    "   - Example models: T5, BART.\r\n",
    "   - Applications: Machine translation, text summarization.\r\n",
    "\r\n",
    "3. **`TOKEN_CLASSIFICATION`**\r\n",
    "   - Used for token-level tasks where each token in the input receives a label.\r\n",
    "   - Example models: BERT, RoBERTa.\r\n",
    "   - Applications: Named Entity Recognition (NER), Part-of-Speech tagging.\r\n",
    "\r\n",
    "4. **`SEQ_CLASSIFICATION`** (Sequence Classification)\r\n",
    "   - Used for tasks where an entire sequence is classified into a single category.\r\n",
    "   - Example models: BERT, RoBERTa.\r\n",
    "   - Applications: Sentiment analysis, spam detection, document classification.\r\n",
    "\r\n",
    "5. **`MULTIPLE_CHOICE`**\r\n",
    "   - Used for tasks where the model selects the correct choice from multiple options based on a context.\r\n",
    "   - Example models: RoBERTa, DeBERTa.\r\n",
    "   - Applications: Question answering in a multiple-choice format (e.g., SWAG, RACE).\r\n",
    "\r\n",
    "6. **`QUESTION_ANSWERING`**\r\n",
    "   - Used for extractive question-answering tasks where the model predicts a span of text from the input context.\r\n",
    "   - Example models: BERT, DistilBERT.\r\n",
    "   - Applications: SQuAD-style tasks, FAQ extraction.\r\n",
    "\r\n",
    "7. **`IMAGE_CLASSIFICATION`**\r\n",
    "   - Used for image classification tasks in vision models.\r\n",
    "   - Example models: ViT, ConvNext.\r\n",
    "   - Applications: Object detection, image categorization.\r\n",
    "\r\n",
    "8. **`FEATURE_EXTRACTION`**\r\n",
    "   - Used for extracting intermediate features from a model without specific downstream task adaptation.\r\n",
    "   - Applications: Embedding generation, unsupervised feature analysis.\r\n",
    "\r\n",
    "9. **`UNSPECIFIED`**\r\n",
    "   - A general or fallback category for tasks not explicitly defined in the library.\r\n",
    "   - Applications: Custom tasks requiring specialized configurations.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### How to Find Supported `task_type` Values:\r\n",
    "If you're using a library like PEFT, consult its [documentation](https://github.com/huggingface/peft) or the source code to confirm all supported `task_type` values. You can also check the library's implementation for class definitions or enumerations that list these values.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a725396-6820-4a33-a31c-9fbd5f297123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params r={r}: {trainable_params} || all params: {all_param} || trainable: {round(100 * trainable_params / all_param,2)}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19a9c26f-51c3-41a1-9303-ef2bb0afbbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params r=1: 98304 || all params: 331294720 || trainable: 0.03%\n",
      "trainable params r=8: 786432 || all params: 331982848 || trainable: 0.24%\n",
      "trainable params r=16: 1572864 || all params: 332769280 || trainable: 0.47%\n",
      "trainable params r=32: 3145728 || all params: 334342144 || trainable: 0.94%\n",
      "trainable params r=64: 6291456 || all params: 337487872 || trainable: 1.86%\n",
      "trainable params r=128: 12582912 || all params: 343779328 || trainable: 3.66%\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "for r in (1,8,16,32,64,128):\n",
    "    config = LoraConfig(\n",
    "        r=r, \n",
    "        lora_alpha=32, \n",
    "        target_modules=[\"q_proj\",\"v_proj\"], \n",
    "        lora_dropout=0.05, \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ca5e15-9b79-47e1-b1bd-133ca101d89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
