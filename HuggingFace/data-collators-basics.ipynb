{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12480d84-5c27-4139-879d-b12f82371de4",
   "metadata": {},
   "source": [
    "# Data collators\n",
    "\n",
    "Data collators are essential in natural language processing (NLP) because they simplify the process of preparing data for model training. In NLP tasks, input sequences can vary in length, and models typically require fixed-length sequences to process them in batches. Data collators handle tasks like **padding** (to ensure sequences are the same length), **masking** (to ignore padding tokens during training), and **batching** (to group multiple examples together efficiently). Without data collators, these tasks would need to be manually managed, which can be complex and error-prone. By automating these steps, data collators streamline data preprocessing, improve memory efficiency, and ensure models focus only on relevant tokens, ultimately enhancing training performance and simplicity.\n",
    "\n",
    "[DataCollator documentation](https://huggingface.co/docs/transformers/en/main_classes/data_collator)\n",
    "\n",
    "[Padding & truncation documentation](https://huggingface.co/docs/transformers/en/pad_truncation)\n",
    "\n",
    "### Google Colab\n",
    "* Install the required packages ; uncomment the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec20cad-76fe-4a78-8a42-ee86c1aa105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a1847-7a05-41d8-b9b4-882b406aa816",
   "metadata": {},
   "source": [
    "## DefaultDataCollator\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DefaultDataCollator\n",
    "\n",
    "Very simple data collator that simply collates batches of dict-like objects and performs special handling for potential keys named:\r\n",
    "\r\n",
    "* label: handles a single value (int or float) per object\r\n",
    "* label_ids: handles a list of values per object\r\n",
    "\r\n",
    "\r\n",
    "Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs to the m\n",
    "\n",
    "**Note:**\n",
    "* Dataset must have a column name label or label_ids otherwise collator will throw an errorodel. del. ful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a83ea3e-f547-4152-a5e5-c7ddfe0807a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    {\"text\": \"Hello, how are you?\"},\n",
    "    {\"text\": \"This is the longest sequence in the batch\"},\n",
    "    {\"text\": \"Shortest one\"}\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f87f86-3e30-4943-b3e1-50b861a7eb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31816d092d3b4d2abfd36ccd9153c279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input_ids array :  20\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "# In general, the max_length is adjusted statically based on the length of the longest sentence in the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=20)\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"Size of input_ids array : \", len(tokenized_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45f67c9-5b43-439f-9e9b-456113c72e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2003,  1996,  6493,  5537,  1999,  1996, 14108,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "===================\n",
      "torch.Size([2, 20])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "# Initialize DefaultDataCollator\n",
    "# Note : The default data collator does not need a tokenizer, which means that it can throw error if tokenized sequence are not provided\n",
    "# In this case the sequence are tokenized & padded manually\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Convert the dataset into a list of examples\n",
    "examples = [tokenized_dataset[i] for i in range(len(tokenized_dataset))]\n",
    "\n",
    "# Manually create a batch (e.g., first two examples)\n",
    "# Batch size = 2\n",
    "# Size of the input_ids arrays adjusted dynamically to length of the longest sentence in the batch\n",
    "batch = data_collator(examples[:2])\n",
    "\n",
    "# Print the collated batch\n",
    "print(batch)\n",
    "print(\"===================\")\n",
    "print(batch['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8004d60-6d72-4746-8364-c262a08df60a",
   "metadata": {},
   "source": [
    "## DataCollatorWithPadding\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorWithPadding\n",
    "\n",
    "The DataCollatorWithPadding is a Hugging Face utility that dynamically pads tokenized sequences in a batch to match the longest sequence in that batch. It's particularly useful when working with variable-length inputs. `DataCollatorWithPadding` is particularly useful in scenarios where sequences of varying lengths need to be processed efficiently. It is ideal for tasks like **text classification**, **question answering**, and other NLP tasks, where the input sequences can differ in length. The collator dynamically pads the sequences to the length of the longest sequence in the batch, ensuring that the padding is minimized, which optimizes memory usage. It also handles **masking** for padded tokens, ensuring that the model only attends to actual tokens during training. This dynamic padding approach makes it suitable for a wide range of models, including those for tasks like **named entity recognition (NER)** or **sentiment analysis**, where input sequences can vary in size. By reducing unnecessary padding, it enhances **training efficiency** and **model performance**.\n",
    "\n",
    "### Batch size\n",
    "Batch size is controlled by the Trainer or other classes such as PyTorch DataLoader. For simplicity, in this example the entrire dataset is getting passed in a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c035587-8608-4c35-8add-93309f28d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================= Tokenize individual sample with padding = max_length  =======\n",
      "{'text': 'Hello, how are you?'}    len[input_ids] =  100\n",
      "{'text': 'This is the longest sequence in the batch'}    len[input_ids] =  100\n",
      "{'text': 'Shortest one'}    len[input_ids] =  100\n",
      "======================== Tokenize without padding i.e., padding=False ======\n",
      "{'text': 'Hello, how are you?'}    len[input_ids] =  8\n",
      "{'text': 'This is the longest sequence in the batch'}    len[input_ids] =  10\n",
      "{'text': 'Shortest one'}    len[input_ids] =  4\n",
      "======================== Dynamic padding with DataCollatorWithPadding ======\n",
      "tensor([[  101,  7592,  1010,  2129,  2024,  2017,  1029,   102,     0,     0],\n",
      "        [  101,  2023,  2003,  1996,  6493,  5537,  1999,  1996, 14108,   102],\n",
      "        [  101, 20047,  2028,   102,     0,     0,     0,     0,     0,     0]])\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    {\"text\": \"Hello, how are you?\"},\n",
    "    {\"text\": \"This is the longest sequence in the batch\"},\n",
    "    {\"text\": \"Shortest one\"}\n",
    "]\n",
    "\n",
    "print(\"======================= Tokenize individual sample with padding = max_length  =======\")\n",
    "# Tokenize the data with padding set to max_length - creates a list of dicts\n",
    "# Assume the longest sequence in the data is length = 100\n",
    "tokenized_data = [tokenizer(d[\"text\"], truncation=True, padding=\"max_length\", max_length=100) for d in data]\n",
    "for i, dat in enumerate(tokenized_data):\n",
    "    print(data[i],'   len[input_ids] = ', len(dat['input_ids']))\n",
    "\n",
    "print(\"======================== Tokenize without padding i.e., padding=False ======\")\n",
    "tokenized_data = [tokenizer(d[\"text\"], truncation=True) for d in data]\n",
    "\n",
    "for i, dat in enumerate(tokenized_data):\n",
    "    print(data[i],'   len[input_ids] = ', len(dat['input_ids']))\n",
    "\n",
    "print(\"======================== Dynamic padding with DataCollatorWithPadding ======\")\n",
    "\n",
    "# Use DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Prepare a batch\n",
    "batch = data_collator(tokenized_data)\n",
    "\n",
    "# Print batch keys and tensor shapes\n",
    "print(batch['input_ids'])\n",
    "\n",
    "# In a batch all input_ids are packed in a single tensor of equal length\n",
    "# [x, y]  x = Size of the batch, y = size of the ['input_ids']\n",
    "# Padding length = Size of the longest sequence in the batch\n",
    "print(batch['input_ids'].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3dd33b-a822-4610-8751-21a0bf3cc06b",
   "metadata": {},
   "source": [
    "## DataCollatorForSeq2Seq\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq\n",
    "\n",
    "`DataCollatorForSeq2Seq` is specifically designed for sequence-to-sequence tasks such as machine translation, summarization, and text generation. It is ideal for models like T5, BART, or MarianMT, where both **input sequences** (e.g., source texts) and **target sequences** (e.g., labels) are required. This collator dynamically pads both the input and target sequences to the length of the longest sequence in the batch, ensuring efficient memory usage and reducing unnecessary padding. It also creates **attention masks** for both inputs and labels, allowing the model to ignore padded tokens during training. By handling padding and masking seamlessly for both input and target sequences, `DataCollatorForSeq2Seq` simplifies batch preparation and improves **training efficiency** for sequence-to-sequence tasks. This makes it especially useful for tasks like **machine translation**, **summarization**, and **question answering**, where the model must process pairs of source and target texts.\n",
    "\n",
    "1. **Dynamic Padding**:\n",
    "\n",
    "* Automatically pads sequences to the maximum length in the batch, reducing unnecessary padding compared to padding to a fixed length.\n",
    "* Ensures both input sequences (e.g., source texts) and target sequences (e.g., labels) are padded to the same length for proper batching.\n",
    "\n",
    "2. **Supports Labels**:\n",
    "\n",
    "* Specially designed for Seq2Seq tasks where both input and target sequences are needed.\n",
    "* The collator will handle padding for both input_ids and labels (target sequences).\n",
    "\n",
    "3. **Masking for Padding**:\n",
    "\n",
    "* Creates attention_mask tensors for both inputs and labels, ensuring the model ignores padded tokens during training.\n",
    "* Masks are set to 1 for actual tokens and 0 for padding tokens, ensuring no contribution from padding tokens during loss calculation.\n",
    "\n",
    "3. **Efficient for Seq2Seq Models**:\n",
    "\n",
    "* Specifically tailored for sequence-to-sequence models like T5, BART, or MarianMT, which require both source and target sequences.\n",
    "* It makes batch preparation easier and more efficient for such models.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Labels are of different length compared to the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599932e-c754-4a3b-b91c-4051141271bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer and model (e.g., T5 for summarization)\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Source & Target text\n",
    "data = [\n",
    "    {\"source_text\": \"Translate English to French: How are you?\", \"target_text\": \"Comment ça va?\"},\n",
    "    {\"source_text\": \"Translate English to French: I love programming.\", \"target_text\": \"J'aime programmer.\"}\n",
    "]\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb80bc-1b9c-4d4b-8f0f-d205529628fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize input (source) and target (label) texts\n",
    "    model_inputs = tokenizer(examples[\"source_text\"], max_length=50, truncation=True, )\n",
    "    labels = tokenizer(examples[\"target_text\"], max_length=50, truncation=True, )\n",
    "\n",
    "    # Add labels to the model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Fix to a BUG\n",
    "# Need to remove text columns otherwise, you will get the following error: \n",
    "# ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`source_text` in this case) have excessive nesting (inputs type `list` where type `int` is expected).\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['source_text', 'target_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72ce6e-9173-4d4a-87a3-b2bc4b44d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Needed for the data collator\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Use DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Example: Prepare a batch\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.default_data_collator\n",
    "batch = data_collator([tokenized_dataset[i] for i in range(len(data))])\n",
    "\n",
    "print(batch)\n",
    "\n",
    "# Check the output\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key}: {value.shape if hasattr(value, 'shape') else value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754faf7-6305-4121-b4fd-3b526103295f",
   "metadata": {},
   "source": [
    "## DataCollatorForLanguageModeling\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling\n",
    "\n",
    "`DataCollatorForLanguageModeling` is useful in several scenarios, particularly when working with language models. It is ideal for **Masked Language Modeling (MLM)**, as seen in models like BERT, where a percentage of tokens are masked and the model must predict them. It is also applicable to **Causal Language Modeling (CLM)**, used in models like GPT, where the model predicts the next token in a sequence. Additionally, it is helpful in **Text Generation** tasks, where models generate text based on context, such as with GPT-based models. Furthermore, it ensures **Efficient Language Model Training** by applying dynamic padding, which optimizes memory usage and improves training efficiency, especially when handling variable-length sequences.\n",
    "\n",
    "1. **Supports Both MLM and CLM**:\n",
    "* Handles both masked and causal language modeling tasks, making it versatile.\n",
    "  \n",
    "2. **Dynamic Padding**:\n",
    "* Efficient padding that reduces unnecessary computation and memory usage.\n",
    "\n",
    "3. **Attention Masking**:\n",
    "* Ensures padding tokens are ignored in the attention mechanism, making the model focus on real tokens only.\n",
    "\n",
    "4. **Prepares Tensors for Training**:\n",
    "* The collator returns the inputs, attention masks, and labels in the correct format for training language models.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "* Set *mlm=False* for Causal Language Modeling (CLM) i.e., an NLP task in which the model predicts next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff903df-23b0-429e-b18a-469198e371e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer\n",
    "\n",
    "# Load a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example data\n",
    "data = [\n",
    "    \"This is a sample sentence.\",\n",
    "    \"Another example for masked language modeling.\"\n",
    "]\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_data = tokenizer(data, truncation=True, max_length=10, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Use DataCollatorForLanguageModeling\n",
    "# mlm_probability=0.15, each token has a 15% chance of being masked.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "# Prepare a batch\n",
    "batch = data_collator([tokenized_data[\"input_ids\"][i] for i in range(len(data))])\n",
    "\n",
    "# Print masked inputs\n",
    "# input_ids: Contains the tokenized sentences with some tokens replaced by [MASK] (or randomized).\n",
    "# labels: Contains the original token IDs, with masked positions preserved and all other positions set to -100 (ignored by the loss function).\n",
    "print(\"Input IDs with masking:\\n\", batch[\"input_ids\"])\n",
    "print(\"Labels:\\n\", batch['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73dfd9-3722-485b-bdd2-fb531622d3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
