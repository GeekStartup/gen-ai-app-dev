{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b719d460-3a4a-4c8d-8b7c-10275dbd8465",
   "metadata": {},
   "source": [
    "# Transformers trainer class\n",
    "https://huggingface.co/docs/transformers/en/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb9609-82b4-4d91-97ad-9b32aceb9aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d882c57-18b9-42c4-b19c-9d4aa233c6c7",
   "metadata": {},
   "source": [
    "### (Optional) Try the fine-tuned model\n",
    "\n",
    "The fully fine-tuned model is available for you to try out. Uncomment the code below and run it for testing.\n",
    "\n",
    "[acloudfan/fine-tuned-BERT-IMDB](https://huggingface.co/acloudfan/fine-tuned-BERT-IMDB/settings)\n",
    "\n",
    "* LABEL_0    Negative\n",
    "* LABEL_1    Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5f23d-0daa-4917-acc3-e2c5b7b953a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"acloudfan/fine-tuned-BERT-IMDB\")\n",
    "# data = [\"I love you\", \"I hate you\"]\n",
    "# sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a5fd2-e48d-452e-9d98-1c5b9be89667",
   "metadata": {},
   "source": [
    "## 1. Use GPU/CUDA for fine-tuning\n",
    "\n",
    "All frameworks provide support for running training and fine-tuning on GPU/CUDA devices.\n",
    "\n",
    "```\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model     = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "inputs    = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "model     = model.to(device)\n",
    "outputs   = model(**inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194066cb-2c88-4c27-8df9-18cbe2c66a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Identify the device available for training\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212283d-fcfe-47aa-8cfc-4173b90024c4",
   "metadata": {},
   "source": [
    "## 2. Define simple eval function & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9055770-62a9-4a7e-b37b-500452df2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Run test with base model\n",
    "MODEL_ID=\"google-bert/bert-base-cased\"\n",
    "\n",
    "# Create the instance of the model with 'sequence classification head' - ignore the warning as the 'sequence classification head' is not trained\n",
    "# https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSequenceClassification.from_pretrained\n",
    "# Checkout other models that can be fine-tuned using this method\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2, torch_dtype=\"auto\").to(device)\n",
    "\n",
    "# print(model.config)\n",
    "\n",
    "# Load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Define the inference function\n",
    "def predict_label_with_given_model(use_model, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Predict the label for a given text using the fine-tuned model from the trainer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text for inference.\n",
    "\n",
    "    Returns:\n",
    "        int: The predicted label.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # PyTorch tensors\n",
    "        truncation=True,      # Truncate if the text is too long\n",
    "        padding=True          # Pad to the max length in the batch\n",
    "    ).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # No gradient calculation during inference\n",
    "        outputs = use_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()  # Get the label with the highest score\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Testing the function\n",
    "def run_simple_eval(use_model):\n",
    "    # Example texts for testing\n",
    "    test_texts = [\n",
    "        \"This movie was fantastic! I really enjoyed it.\",\n",
    "        \"I didn't like this product. It broke after one use.\",\n",
    "        \"The book was okay, not great but not terrible either.\",\n",
    "        \"it is all fun\"\n",
    "    ]\n",
    "\n",
    "    # Run inference and print results\n",
    "    print(\"Label = 0 :  Negative      Label = 1 : Positive\")\n",
    "    for text in test_texts:\n",
    "        label = predict_label_with_given_model(use_model,text)\n",
    "        print(f\"Text: {text}\\nPredicted Label: {label}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Run evaluation on the base model\n",
    "run_simple_eval(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a2d79-6af2-408b-9d74-32d92003b062",
   "metadata": {},
   "source": [
    "## 3. Load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd5339-9484-4e32-bd70-1b7b46765bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(\"Number of records in 'Train' : \",len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110945c0-b650-4fd6-befa-457ed1456ea7",
   "metadata": {},
   "source": [
    "## 4. (optional) Reduce the size of the training & test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebb3aa-a87f-45b5-ac4b-b4bdc34d32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust this to change the number of train and test rows\n",
    "TOTAL_NUMBER_RECORDS = 5000\n",
    "\n",
    "# Reduce the size of the dataset\n",
    "reduced_dataset=dataset['train'].shuffle(seed=42).select(range(TOTAL_NUMBER_RECORDS))\n",
    "\n",
    "# Split the dataset into train and test\n",
    "reduced_dataset = reduced_dataset.train_test_split(train_size=0.9)\n",
    "\n",
    "reduced_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8568fd0-da0d-46be-bf4c-03fe563f2223",
   "metadata": {},
   "source": [
    "## 5. Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1bcc0c-60e4-4514-8a41-123a354a5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize function used by map\n",
    "# Move the tokenized data to device (GPU)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True).to(device)\n",
    "\n",
    "\n",
    "# Tokenize using map\n",
    "tokenized_datasets = reduced_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Uncommenting this line will lead to an error as the trainer expects dataset to have a column with the name 'label'\n",
    "# ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,token_type_ids,attention_mask\n",
    "# tokenized_datasets = tokenized_datasets.rename_column('label','type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dd98a-4530-4e1b-a996-21adb8cd34c9",
   "metadata": {},
   "source": [
    "## 6. Setup TrainingArguments & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd13d6-10c9-4174-91bd-44dddb6d6d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Output directory for the training\n",
    "output_dir = \"./temp\"\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    # Directory for logs and outputs from trainer\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    \n",
    "\n",
    "    # Hyperparameters\n",
    "    eval_strategy = \"epoch\",                # Can be set to \"steps\" but will lead to longer training times\n",
    "    per_device_train_batch_size=4,          # Default = 8. Adjust based on the VRAM/GPU. High value can lead to OOM errors\n",
    "    per_device_eval_batch_size=4,           # Default = 8. Adjust based on the VRAM/GPU. High value can lead to OOM errors\n",
    "    num_train_epochs=2,                     # Higher epoch means longer training time\n",
    "    # max_steps=100,                        # Change this for forced stopping - good for experimentation\n",
    "    learning_rate=0.00005,                  # The initial learning rate for AdamW optimizer\n",
    "    warmup_steps=0,\n",
    "    warmup_ratio=0,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",                    # Using the default optimizer\n",
    "\n",
    "    # Reporting & Logging - refer documentation for more args\n",
    "    report_to='none',                       # \"azure_ml\", \"clearml\", \"codecarbon\", \"comet_ml\", \"dagshub\", \"dvclive\", \"flyte\", \"mlflow\", \"neptune\", \"tensorboard\", and \"wandb\"\n",
    "    logging_strategy=\"steps\",\n",
    "    # logging_steps=50,                       # default = 500, Number of update steps between two logs if logging_strategy=\"steps\". Otherwise a number between 0 & 1, interprtted as a ratio of train steps\n",
    "\n",
    "    # Reports to - integration with MLOPS & ML experiments tooling\n",
    "    run_name=\"Fine-Tune-BERT-IMDB\",\n",
    "    \n",
    "\n",
    "    # Save strategy - refer documentation for more args\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    # Hub strategy -  - refer documentation for more args\n",
    "    \n",
    "    \n",
    ")\n",
    "\n",
    "# Setup traininer with minimilitic st of arguments\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['test'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe43d5-f0ae-4b92-96fd-f1ab91723f2a",
   "metadata": {},
   "source": [
    "## 7. Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cc890-f839-4d55-a1aa-10a96a185e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f80bf-5e0e-4f7e-a662-4200cb972902",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = trainer.model\n",
    "\n",
    "# Run evaluation on the base model\n",
    "run_simple_eval(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9aa0e9-30b1-408c-bfd2-b81059e14d52",
   "metadata": {},
   "source": [
    "## 8. Save the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375f226-2562-4edf-9d3d-2935ab2a445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = output_dir + \"/fine-tuned-BERT-IMDB\"\n",
    "\n",
    "trainer.save_model(model_folder)\n",
    "tokenizer.save_pretrained(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06c20d4-1429-4f82-a43d-4d83226086f2",
   "metadata": {},
   "source": [
    "## 9. Load fine-tuned model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991fc4f-0096-4e9d-abb0-d4e0593dbaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "print(\"Provide the HuggingFace API token:\")\n",
    "\n",
    "hf_token = getpass.getpass()\n",
    "\n",
    "hub_model_id = \"acloudfan/fine-tuned-BERT-IMDB\"\n",
    "\n",
    "fine_tuned_model.push_to_hub(hub_model_id, hf_token=hf_token)\n",
    "tokenizer.push_to_hub(hub_model_id, hf_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb8291-f840-4f3f-a527-3b82c7ae4cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
