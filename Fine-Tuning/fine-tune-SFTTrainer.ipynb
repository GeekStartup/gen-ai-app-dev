{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecdd9752-334e-42b4-ac61-c3f0ae3e5a65",
   "metadata": {},
   "source": [
    "# HuggingFace Supervised Fine-tuning Trainer (SFT)\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer\n",
    "\n",
    "## TinyLlamma\n",
    "https://arxiv.org/pdf/2401.02385\n",
    "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.1\n",
    "https://huggingface.co/facebook/opt-350m\n",
    "https://huggingface.co/facebook/MobileLLM-125M\n",
    "\n",
    "## Example scripts\n",
    "https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py\n",
    "\n",
    "## Inspired by\n",
    "https://colab.research.google.com/github/huggingface/smol-course/blob/main/1_instruction_tuning/notebooks/sft_finetuning_example.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4606f3-a8fd-47b2-8f09-7ba419be1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "354e58d2-84a8-43df-9cd1-52bd4ca35a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816005c1-8134-4026-9f43-b722b0e800b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the base model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "model_name = \"facebook/MobileLLM-125M\"\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"tiny-llama-ft\"\n",
    "os.environ[\"WANDB_DIR\"] = \"./temp\"\n",
    "os.environ[\"WANDB_JOB_NAME\"] = \"some-job-name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de0b81e-df31-45b3-8176-6c3807918ef6",
   "metadata": {},
   "source": [
    "## 1. Load the model to appropriate available device (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcad2466-9fd4-4fff-a976-b6d85984575e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# Check the machine in use and set the device to use for training\n",
    "# cuda = GPU, mps = Metal Performance Shaders on macOS or Apple GPU, cpu otherwise\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Print device info\n",
    "print(\"Model loaded to: \", device)\n",
    "\n",
    "\n",
    "\n",
    "# Load the pretrained model & move it to the specified device\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name\n",
    ").to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Setup for the model specific chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbab373-c83f-4902-b4d4-ffa2f10e5e47",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "**Dataset format support**\n",
    "\n",
    "https://huggingface.co/docs/trl/en/sft_trainer#dataset-format-support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfa2555-5797-4015-8084-909f7e63150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"HuggingFaceTB/smoltalk\"\n",
    "dataset_split = \"everyday-conversations\"\n",
    "\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf3b85-9c9f-4edf-9796-3ac29d573819",
   "metadata": {},
   "source": [
    "## 3. Setup the training configuration\n",
    "\n",
    "**SFTConfig**\n",
    "\n",
    "https://huggingface.co/docs/trl/v0.12.2/en/sft_trainer#trl.SFTConfig\n",
    "\n",
    "This object specifies hyperparameters and settings for the fine-tuning process. Itâ€™s tailored to supervised fine-tuning tasks, often used for adapting language models to specific tasks or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e250894a-d5ee-446b-b8d9-86f55e9c50e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = datetime.now()\n",
    "\n",
    "# Create a readable timestamp\n",
    "formatted_time = current_time.strftime(\"%b-%d-%Y-%H-%M-%S\")\n",
    "\n",
    "# Create a name for the run\n",
    "wandb_run_name = f\"FT_run_{formatted_time}\"\n",
    "\n",
    "# Adjust the model\n",
    "fine_tuned_model_name = f\"fine-tuned-chat-model\"\n",
    "\n",
    "# Model assets output folder\n",
    "model_output_folder = \"c:/temp/sft_output\"\n",
    "\n",
    "# SFTrainer configuration\n",
    "sft_config = SFTConfig(\n",
    "\n",
    "    # Output directory for model assets\n",
    "    output_dir = model_output_folder,  \n",
    "\n",
    "    # Hyperparameter : Controls maximum number of steps to be executed\n",
    "    # Maximum number of gradient update steps during training.\n",
    "    max_steps=100,  \n",
    "\n",
    "    # Common starting point for fine-tuning\n",
    "    # The initial learning rate for the optimizer.\n",
    "    learning_rate=5e-5,  \n",
    "\n",
    "    # Set according to your GPU memory capacity\n",
    "    # Number of training samples per device in each batch. Smaller values help fit large models into memory-constrained GPUs.\n",
    "    per_device_train_batch_size=4,  \n",
    "\n",
    "    # Frequency of logging training metrics\n",
    "    # Logs metrics (e.g., loss) every 10 steps during training.\n",
    "    logging_steps=10,  \n",
    "\n",
    "    # Frequency of saving model checkpoints\n",
    "    # Saves model checkpoints every 100 steps. In case of failure, loss or work will be limited to a maximum of 100 steps\n",
    "    save_steps=100,  \n",
    "\n",
    "    # Evaluate the model at regular intervals\n",
    "    eval_strategy=\"steps\",  \n",
    "\n",
    "    # Frequency of evaluation\n",
    "    # Run the model evaluation after every 50 steps\n",
    "    eval_steps=50,  \n",
    "\n",
    "    # Use MPS for mixed precision training\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  \n",
    "\n",
    "    # Set a unique name for your model - used for HuggingFace hub\n",
    "    hub_model_id=fine_tuned_model_name,  \n",
    "\n",
    "    # Reporting\n",
    "    report_to = \"wandb\",\n",
    "    run_name = wandb_run_name,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c85fbfe-7369-4907-a887-b95457866bc8",
   "metadata": {},
   "source": [
    "## SFTrainer\n",
    "\n",
    "https://huggingface.co/docs/trl/v0.12.2/en/sft_trainer#trl.SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519503b8-43a8-4e9b-a9db-ecd0f9ae050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raj\\anaconda3\\envs\\gen-ai-app-dev-course\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d47aeebf254bd8a2475d4bd2d5cc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "\n",
    "    # The language model being fine-tuned.\n",
    "    model=model,\n",
    "\n",
    "    # Passes the fine-tuning configuration defined above \n",
    "    args=sft_config,\n",
    "\n",
    "    # Training dataset\n",
    "    train_dataset=ds[\"train\"],\n",
    "\n",
    "    # Evaluation dataset\n",
    "    eval_dataset=ds[\"test\"],\n",
    "\n",
    "    # Tokenizer used\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcb839-5071-4eec-9a5e-f7fa79c52266",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Wandb - configuration\n",
    "https://docs.wandb.ai/guides/track/environment-variables/\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65199f1-ff8f-4a38-adb2-ffe6112f9022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: WARNING Path ./temp\\wandb\\ wasn't writable, using system temp directory\n",
      "wandb: Currently logged in as: acloudfan (raj-acloudfan). Use `wandb login --relogin` to force relogin\n",
      "wandb: WARNING Path ./temp\\wandb\\ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\raj\\AppData\\Local\\Temp\\wandb\\run-20241212_103151-tz0p2m2p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raj-acloudfan/tiny-llama-ft/runs/tz0p2m2p' target=\"_blank\">FT_run_Dec-12-2024-10-31-48</a></strong> to <a href='https://wandb.ai/raj-acloudfan/tiny-llama-ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raj-acloudfan/tiny-llama-ft' target=\"_blank\">https://wandb.ai/raj-acloudfan/tiny-llama-ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raj-acloudfan/tiny-llama-ft/runs/tz0p2m2p' target=\"_blank\">https://wandb.ai/raj-acloudfan/tiny-llama-ft/runs/tz0p2m2p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{fine_tuned_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba3094-ffc6-4345-b95d-c891a332df2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d175da-043d-4eae-8b55-d10f064fb692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
