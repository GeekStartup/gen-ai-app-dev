{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0af15c7-e0a5-4770-a1a9-671d23cca360",
   "metadata": {},
   "source": [
    "# Evaluate with RAGAS\n",
    "\n",
    "**Note**\n",
    "\n",
    "The code in this notebook requires the evaluation dataset to be available as a JSON file. The JSON file is an array of JSON structure\n",
    "```\n",
    "{\n",
    "    \"user_input\" : user_input,\n",
    "    \"retrieved_contexts\"  : retrieved_contexts,\n",
    "    \"reference\" : reference,\n",
    "    \"response\" : responses\n",
    "}\n",
    "```\n",
    "The JSON file is generated with the notebook: **ragas-prepare-eval-dataset**\n",
    "\n",
    "**Evaluation steps**\n",
    "\n",
    "1. Read the eval file and create the evaluation dataset *(check the last cell in prepare eval dataset notebook)*\n",
    "2. Setup RAGAS configuration and metrics\n",
    "3. Run the evaluation\n",
    "\n",
    "**Sample results**\n",
    "\n",
    "Embedding LLM = \"all-MiniLM-L6-v2\"\n",
    "RAG LLM = Cohere Command\n",
    "Judge LLM = Google Gemini Flash 1.5\n",
    "Chunk size = 900\n",
    "Chunk overlap = 150\n",
    "{'answer_relevancy': 0.9053, 'context_precision': 0.4000, 'context_recall': 0.2143, 'context_entity_recall': 0.0686, 'semantic_similarity': 0.8116, 'answer_correctness': 0.4810}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffe1420-42e4-41e3-afbc-e21aa16524fd",
   "metadata": {},
   "source": [
    "## Setup packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a736063-3225-4f84-9715-e1f6dddcb51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "from IPython.display import JSON\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "import sys\n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Load the file that contains the API keys\n",
    "load_dotenv('C:\\\\Users\\\\raj\\\\.jupyter\\\\.env')\n",
    "\n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils.create_llm import create_gpt_llm, create_cohere_llm, create_ollama_llm, create_hugging_face_llm, create_google_llm, create_ai21_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab30042-aff2-44c6-8a66-8ed137a44b3b",
   "metadata": {},
   "source": [
    "## 1. Create the eval dataset by reading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f175c5b-f492-4d3d-8263-f4598bb67f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to your JSON file\n",
    "eval_filepath = \"amzn-10k/amz-10k-2024-eval-dataset-cohere-chunk900-overlap150-k5.json\"\n",
    "\n",
    "# Open the JSON file and load its contents into a variable\n",
    "with open(eval_filepath, 'r', encoding='utf-8') as file:\n",
    "    eval_dataset_json = json.load(file)\n",
    "\n",
    "eval_dataset_hf = Dataset.from_dict(eval_dataset_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f500a19c-c21b-4df7-a6c9-963e4cfa50b9",
   "metadata": {},
   "source": [
    "## 2. Setup RAGAS config & metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b746942b-dace-42f6-8ae1-deee0629a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate, RunConfig\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    context_entity_recall,\n",
    "    answer_similarity,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "        answer_relevancy,        # ['user_input', 'response']\n",
    "        context_precision,       # ['user_input', 'retrieved_contexts', 'reference'] \n",
    "        context_recall,          # ['user_input', 'retrieved_contexts']\n",
    "        context_entity_recall,   # ['user_input', 'retrieved_contexts']\n",
    "        answer_similarity,       # ['response']\n",
    "        answer_correctness       # ['user_input', 'response']\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac9a866-cb13-4d24-a2bc-9ceab3612d51",
   "metadata": {},
   "source": [
    "## 3. Run evaluation\n",
    "\n",
    "https://docs.ragas.io/en/stable/references/evaluate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0232dc83-5db9-4ac1-91e7-df8cde8de5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15971632980d4375a0820ecb68906854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[5]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[17]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[23]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[29]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Exception raised in Job[32]: OutputParserException(Failed to parse StringIO from completion {\"classifications\": [{\"statement\": \"Amazon has a centralized cybersecurity team led by a chief security officer, conducts regular risk assessments, and implements controls to prevent, detect, and mitigate data loss or security incidents.\", \"reason\": \"The statement is a summary of the given context.\", \"attributed\": 1}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'classifications': [{'st...xt.', 'attributed': 1}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[35]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Exception raised in Job[38]: OutputParserException(Failed to parse StringIO from completion {\"classifications\": [{\"statement\": \"Amazon's major operating expenses include cost of sales ($326.288 billion), fulfillment costs ($98.505 billion), and technology and infrastructure costs ($88.544 billion) in 2024.\", \"reason\": \"The sentence about the exact numbers is not mentioned in the given context.\", \"attributed\": 0}, {\"statement\": \"Amazon's major operating expenses include cost of sales, fulfillment costs, and technology and infrastructure costs.\", \"reason\": \"The sentence is relevant to the question and somewhat mentioned in the context.\", \"attributed\": 0.5}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'classifications': [{'st....', 'attributed': 0.5}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[41]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[47]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Exception raised in Job[50]: OutputParserException(Failed to parse StringIO from completion {\"classifications\": [{\"statement\": \"Amazon faces risks in international operations, including regulatory challenges, currency fluctuations, geopolitical events, and competition from local companies with established brands.\", \"reason\": \"The sentence mentions factors like regulatory challenges, currency fluctuations, geopolitical events and competition from local companies with established brands, all of which are mentioned in the context as risks associated with international operations.\", \"attributed\": 1}]}. Got: 1 validation error for StringIO\n",
      "text\n",
      "  Field required [type=missing, input_value={'classifications': [{'st...ns.', 'attributed': 1}]}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE )\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[53]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "Prompt correctness_classifier failed to parse output: The output parser failed to parse the output including retries.\n",
      "Exception raised in Job[59]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a Ragas LLM wrapper\n",
    "# llm_for_evaluation = create_google_llm()\n",
    "llm_for_evaluation = create_cohere_llm()\n",
    "# llm_for_evaluation = create_gpt_llm(args={\"temperature\": 0, \"max_tokens\": 2000})\n",
    "\n",
    "# Create the RAGAS LLM wrapper\n",
    "llm_for_evaluation_wrapper = LangchainLLMWrapper(llm_for_evaluation)\n",
    "\n",
    "# Create a Ragas Embeddings wrapper\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings_model_wrapper = LangchainEmbeddingsWrapper(embedding_function)\n",
    "\n",
    "# Convert dataset to Ragas eval dataset\n",
    "eval_dataset_ragas = EvaluationDataset.from_hf_dataset(eval_dataset_hf)\n",
    "\n",
    "\n",
    "\n",
    "# Validate\n",
    "\n",
    "# eval_dataset_ragas.validate_samples()\n",
    "# eval_dataset_ragas.to_csv()\n",
    "\n",
    "# eval_dataset_ragas\n",
    "\n",
    "result = evaluate(\n",
    "        dataset=eval_dataset_ragas,\n",
    "        metrics=metrics,\n",
    "        llm=llm_for_evaluation_wrapper,\n",
    "        embeddings=embeddings_model_wrapper,\n",
    "        run_config=RunConfig(max_workers=2)\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8189ec-3596-45b7-af93-76208ff31a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.9102, 'context_precision': 0.4000, 'context_recall': 0.2143, 'context_entity_recall': 0.0686, 'semantic_similarity': 0.8116, 'answer_correctness': 0.4810}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51f065e-e6e6-4b5b-845d-6a13b8c9eca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
